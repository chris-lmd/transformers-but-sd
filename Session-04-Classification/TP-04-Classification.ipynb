{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 04 - Classification de Texte : Détection de Fake News\n",
    "\n",
    "**Module** : Réseaux de Neurones Approfondissement  \n",
    "**Durée** : 2h  \n",
    "**Objectif** : Entraîner un Transformer pour détecter les fake news\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs pédagogiques\n",
    "\n",
    "À la fin de ce TP, vous serez capable de :\n",
    "1. Préparer un dataset de texte pour un Transformer\n",
    "2. Entraîner votre Transformer sur une tâche de classification\n",
    "3. Évaluer et interpréter les résultats\n",
    "4. Utiliser un pipeline de traduction pour l'inférence en français"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers datasets matplotlib numpy scikit-learn tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Notre Transformer (des TPs précédents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Composants du Transformer ===\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        B, S, _ = x.shape\n",
    "        Q = self.W_q(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = (attn @ V).transpose(1, 2).contiguous().view(B, S, self.embed_dim)\n",
    "        return self.W_o(out), attn\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        ff_dim = ff_dim or 4 * embed_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, :x.size(1)])\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ff = FeedForward(embed_dim, ff_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out, _ = self.attn(self.norm1(x), mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, num_classes,\n",
    "                 max_len=512, dropout=0.1, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_len, dropout)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, num_heads, dropout=dropout) \n",
    "                                     for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "        self.embed_dim = embed_dim\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x) * math.sqrt(self.embed_dim)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        # Moyenne sur tous les tokens (alternative à [CLS])\n",
    "        x = x.mean(dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "print(\"Transformer chargé !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Chargement du Dataset\n",
    "\n",
    "Nous utilisons un dataset de fake news en anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Charger un dataset de fake news\n",
    "# On utilise un sous-ensemble pour l'entraînement rapide\n",
    "print(\"Chargement du dataset...\")\n",
    "\n",
    "# Dataset LIAR (fact-checking)\n",
    "dataset = load_dataset(\"liar\", trust_remote_code=True)\n",
    "\n",
    "print(f\"\\nStructure du dataset:\")\n",
    "print(dataset)\n",
    "\n",
    "print(f\"\\nExemple:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le dataset LIAR a 6 labels, simplifions en 2 (fake vs real)\n",
    "# Labels originaux: pants-fire, false, barely-true, half-true, mostly-true, true\n",
    "# Fake: pants-fire, false, barely-true (0, 1, 2)\n",
    "# Real: half-true, mostly-true, true (3, 4, 5)\n",
    "\n",
    "def simplify_label(example):\n",
    "    # 0, 1, 2 -> 0 (Fake)\n",
    "    # 3, 4, 5 -> 1 (Real)\n",
    "    example['binary_label'] = 0 if example['label'] < 3 else 1\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(simplify_label)\n",
    "\n",
    "# Vérifier la distribution\n",
    "train_labels = [ex['binary_label'] for ex in dataset['train']]\n",
    "print(f\"Distribution train: Fake={train_labels.count(0)}, Real={train_labels.count(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Tokenization\n",
    "\n",
    "On crée un tokenizer simple basé sur les mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Créer un vocabulaire\n",
    "# ============================================\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, texts, max_vocab_size=10000, min_freq=2):\n",
    "        \"\"\"\n",
    "        Tokenizer simple basé sur les mots.\n",
    "        \"\"\"\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        \n",
    "        # Compter les mots\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            word_counts.update(words)\n",
    "        \n",
    "        # Créer le vocabulaire\n",
    "        # TODO: Créer word2idx et idx2word\n",
    "        # 1. Commencer par les tokens spéciaux\n",
    "        # 2. Ajouter les mots les plus fréquents (>= min_freq)\n",
    "        # 3. Limiter à max_vocab_size\n",
    "        \n",
    "        self.word2idx = {self.pad_token: 0, self.unk_token: 1}\n",
    "        \n",
    "        # Ajouter les mots fréquents\n",
    "        for word, count in word_counts.most_common(max_vocab_size - 2):\n",
    "            if count >= min_freq:\n",
    "                self.word2idx[word] = len(self.word2idx)\n",
    "        \n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        \n",
    "        print(f\"Vocabulaire créé: {self.vocab_size} mots\")\n",
    "    \n",
    "    def encode(self, text, max_len=128):\n",
    "        \"\"\"Convertit un texte en indices.\"\"\"\n",
    "        words = text.lower().split()[:max_len]\n",
    "        indices = [self.word2idx.get(w, self.word2idx[self.unk_token]) for w in words]\n",
    "        \n",
    "        # Padding\n",
    "        if len(indices) < max_len:\n",
    "            indices += [self.word2idx[self.pad_token]] * (max_len - len(indices))\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convertit des indices en texte.\"\"\"\n",
    "        words = [self.idx2word.get(idx, self.unk_token) for idx in indices]\n",
    "        return ' '.join(w for w in words if w != self.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le tokenizer\n",
    "train_texts = [ex['statement'] for ex in dataset['train']]\n",
    "tokenizer = SimpleTokenizer(train_texts, max_vocab_size=8000, min_freq=2)\n",
    "\n",
    "# Test\n",
    "test_text = \"The president said the economy is doing great.\"\n",
    "encoded = tokenizer.encode(test_text, max_len=20)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Dataset PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['statement']\n",
    "        label = item['binary_label']\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(text, self.max_len)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les datasets\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = FakeNewsDataset(dataset['train'], tokenizer, MAX_LEN)\n",
    "val_dataset = FakeNewsDataset(dataset['validation'], tokenizer, MAX_LEN)\n",
    "test_dataset = FakeNewsDataset(dataset['test'], tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le modèle\n",
    "model = TransformerClassifier(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_dim=128,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    num_classes=2,\n",
    "    max_len=MAX_LEN,\n",
    "    dropout=0.1,\n",
    "    pad_idx=0\n",
    ").to(device)\n",
    "\n",
    "# Compter les paramètres\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Paramètres: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 2 : Boucle d'entraînement\n",
    "# ============================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # TODO: Implémenter\n",
    "        # 1. Zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward\n",
    "        outputs = model(input_ids)\n",
    "        \n",
    "        # 3. Loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 4. Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Stats\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EPOCHS = 5\n",
    "LR = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Historique\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(\"Début de l'entraînement...\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history['train_acc'], label='Train')\n",
    "axes[1].plot(history['val_acc'], label='Val')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Évaluation sur le Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation finale\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport détaillé\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label']\n",
    "        \n",
    "        outputs = model(input_ids)\n",
    "        preds = outputs.argmax(dim=-1).cpu()\n",
    "        \n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels.extend(labels.tolist())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['Fake', 'Real']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Fake', 'Real'],\n",
    "            yticklabels=['Fake', 'Real'])\n",
    "plt.xlabel('Prédiction')\n",
    "plt.ylabel('Vérité')\n",
    "plt.title('Matrice de Confusion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Pipeline avec Traduction (FR → EN)\n",
    "\n",
    "Pour utiliser notre modèle sur du texte français, on ajoute une étape de traduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Charger le traducteur FR -> EN\n",
    "print(\"Chargement du traducteur...\")\n",
    "translator = pipeline(\"translation_fr_to_en\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "print(\"Traducteur chargé !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 3 : Pipeline complet FR -> Classification\n",
    "# ============================================\n",
    "\n",
    "class FakeNewsDetectorFR:\n",
    "    def __init__(self, model, tokenizer, translator, device, max_len=64):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.translator = translator\n",
    "        self.device = device\n",
    "        self.max_len = max_len\n",
    "        self.labels = ['Fake', 'Real']\n",
    "    \n",
    "    def predict(self, text_fr):\n",
    "        \"\"\"\n",
    "        Prédit si un texte français est une fake news.\n",
    "        \n",
    "        Args:\n",
    "            text_fr: Texte en français\n",
    "        \n",
    "        Returns:\n",
    "            dict avec 'label', 'confidence', 'translation'\n",
    "        \"\"\"\n",
    "        # TODO: Implémenter\n",
    "        \n",
    "        # 1. Traduire FR -> EN\n",
    "        translation = self.translator(text_fr, max_length=200)[0]['translation_text']\n",
    "        \n",
    "        # 2. Tokenizer\n",
    "        input_ids = self.tokenizer.encode(translation, self.max_len)\n",
    "        input_ids = torch.tensor([input_ids], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        # 3. Prédiction\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "            probs = F.softmax(outputs, dim=-1)\n",
    "            pred = outputs.argmax(dim=-1).item()\n",
    "            confidence = probs[0, pred].item()\n",
    "        \n",
    "        return {\n",
    "            'label': self.labels[pred],\n",
    "            'confidence': confidence,\n",
    "            'translation': translation,\n",
    "            'probabilities': {'Fake': probs[0, 0].item(), 'Real': probs[0, 1].item()}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le détecteur\n",
    "detector = FakeNewsDetectorFR(model, tokenizer, translator, device)\n",
    "\n",
    "# Tests en français\n",
    "textes_test = [\n",
    "    \"Le président a annoncé une baisse des impôts pour les classes moyennes.\",\n",
    "    \"Des extraterrestres ont été découverts dans une base secrète du gouvernement.\",\n",
    "    \"L'économie a connu une croissance de 2% au dernier trimestre.\",\n",
    "    \"Un homme a survécu 3 mois sans manger grâce à la méditation.\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "for texte in textes_test:\n",
    "    result = detector.predict(texte)\n",
    "    print(f\"\\nTexte FR: {texte}\")\n",
    "    print(f\"Traduction: {result['translation']}\")\n",
    "    print(f\"Prédiction: {result['label']} (confiance: {result['confidence']:.2%})\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Récapitulatif\n",
    "\n",
    "### Ce que nous avons fait\n",
    "\n",
    "1. **Préparation des données** : Tokenization, Dataset, DataLoader\n",
    "2. **Entraînement** : Boucle d'entraînement avec validation\n",
    "3. **Évaluation** : Métriques, matrice de confusion\n",
    "4. **Pipeline FR→EN** : Traduction + Classification\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Tokenizer simple (word-level) → Perd les mots rares\n",
    "- Petit modèle → Capacité limitée\n",
    "- Dataset anglais → Biais culturels possibles\n",
    "\n",
    "### Améliorations possibles\n",
    "\n",
    "- Utiliser un tokenizer BPE (comme BERT)\n",
    "- Augmenter la taille du modèle\n",
    "- Fine-tuner un modèle pré-entraîné (BERT, RoBERTa)\n",
    "- Utiliser un dataset multilingue\n",
    "\n",
    "### Prochaine session\n",
    "\n",
    "**Projet Final** : Vous choisirez entre :\n",
    "- Approfondir ce détecteur de fake news\n",
    "- Explorer CLIP pour la recherche d'images par texte"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
