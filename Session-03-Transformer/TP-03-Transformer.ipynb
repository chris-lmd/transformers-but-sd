{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 03 - Architecture Transformer Complète\n",
    "\n",
    "**Module** : Réseaux de Neurones Approfondissement  \n",
    "**Durée** : 2h  \n",
    "**Objectif** : Assembler un Transformer Encoder complet\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs pédagogiques\n",
    "\n",
    "À la fin de ce TP, vous serez capable de :\n",
    "1. Implémenter le Positional Encoding\n",
    "2. Comprendre les connexions résiduelles et Layer Normalization\n",
    "3. Assembler un bloc Transformer complet\n",
    "4. Empiler plusieurs blocs pour créer un encodeur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Récapitulatif : Nos briques de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention (du TP précédent)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def concat_heads(self, x):\n",
    "        batch_size, _, seq_len, _ = x.shape\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        return x.view(batch_size, seq_len, self.embed_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        Q = self.split_heads(self.W_q(x))\n",
    "        K = self.split_heads(self.W_k(x))\n",
    "        V = self.split_heads(self.W_v(x))\n",
    "        \n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = attn_weights @ V\n",
    "        \n",
    "        concat_output = self.concat_heads(attn_output)\n",
    "        output = self.W_o(concat_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "print(\"MultiHeadAttention chargé !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Positional Encoding\n",
    "\n",
    "### Le problème\n",
    "\n",
    "L'attention est **permutation-invariante** : elle ne sait pas que \"Le chat mange\" ≠ \"mange chat Le\".\n",
    "\n",
    "On doit injecter l'information de **position** dans les embeddings.\n",
    "\n",
    "### La solution : Sinusoïdes\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n",
    "\n",
    "Où :\n",
    "- `pos` : position dans la séquence (0, 1, 2, ...)\n",
    "- `i` : dimension (0, 1, 2, ..., embed_dim/2)\n",
    "- `d` : embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Implémenter Positional Encoding\n",
    "# ============================================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding avec sinusoïdes.\n",
    "    \n",
    "    Args:\n",
    "        embed_dim: Dimension des embeddings\n",
    "        max_len: Longueur maximale des séquences\n",
    "        dropout: Taux de dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Créer la matrice de positional encoding\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        \n",
    "        # Positions : [0, 1, 2, ..., max_len-1]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        \n",
    "        # Diviseur : 10000^(2i/d)\n",
    "        # Astuce : utiliser exp(log) pour la stabilité numérique\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim)\n",
    "        )\n",
    "        \n",
    "        # TODO: Remplir pe avec sin et cos\n",
    "        # pe[:, 0::2] = sin(position * div_term)  # indices pairs\n",
    "        # pe[:, 1::2] = cos(position * div_term)  # indices impairs\n",
    "        \n",
    "        pe[:, 0::2] = None  # À compléter\n",
    "        pe[:, 1::2] = None  # À compléter\n",
    "        \n",
    "        # Ajouter dimension batch\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, embed_dim)\n",
    "        \n",
    "        # Enregistrer comme buffer (pas un paramètre entraînable)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Embeddings, shape (batch, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        # Ajouter positional encoding (tronqué à la longueur de la séquence)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test et visualisation\n",
    "pe = PositionalEncoding(embed_dim=64, max_len=100, dropout=0.0)\n",
    "\n",
    "# Visualiser le positional encoding\n",
    "pe_matrix = pe.pe[0, :50, :].numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap\n",
    "im = axes[0].imshow(pe_matrix.T, cmap='RdBu', aspect='auto')\n",
    "axes[0].set_xlabel(\"Position\")\n",
    "axes[0].set_ylabel(\"Dimension\")\n",
    "axes[0].set_title(\"Positional Encoding (Heatmap)\")\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Courbes pour quelques dimensions\n",
    "for d in [0, 1, 10, 20, 30]:\n",
    "    axes[1].plot(pe_matrix[:, d], label=f'dim {d}')\n",
    "axes[1].set_xlabel(\"Position\")\n",
    "axes[1].set_ylabel(\"Valeur\")\n",
    "axes[1].set_title(\"Positional Encoding (Courbes)\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Les basses dimensions (0, 1) oscillent rapidement, les hautes dimensions (30) oscillent lentement. Cela permet au modèle de distinguer les positions proches ET lointaines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Feed-Forward Network\n",
    "\n",
    "Après l'attention, chaque position passe par un réseau feed-forward identique.\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "La dimension cachée est typiquement 4× embed_dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 2 : Feed-Forward Network\n",
    "# ============================================\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-Forward Network (2 couches linéaires avec ReLU).\n",
    "    \n",
    "    Args:\n",
    "        embed_dim: Dimension d'entrée/sortie\n",
    "        ff_dim: Dimension cachée (typiquement 4 * embed_dim)\n",
    "        dropout: Taux de dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, ff_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        if ff_dim is None:\n",
    "            ff_dim = 4 * embed_dim\n",
    "        \n",
    "        # TODO: Créer le réseau\n",
    "        # linear1: embed_dim -> ff_dim\n",
    "        # activation: ReLU (ou GELU)\n",
    "        # dropout\n",
    "        # linear2: ff_dim -> embed_dim\n",
    "        \n",
    "        self.linear1 = None  # À compléter\n",
    "        self.linear2 = None  # À compléter\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()  # ou nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implémenter le forward\n",
    "        # x -> linear1 -> activation -> dropout -> linear2\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "ff = FeedForward(embed_dim=32, ff_dim=128)\n",
    "x = torch.randn(2, 6, 32)\n",
    "out = ff(x)\n",
    "print(f\"FFN output shape: {out.shape}\")  # (2, 6, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Connexions Résiduelles + Layer Normalization\n",
    "\n",
    "### Connexions résiduelles\n",
    "\n",
    "$$\\text{output} = x + \\text{SubLayer}(x)$$\n",
    "\n",
    "Permet aux gradients de mieux circuler (comme dans ResNet).\n",
    "\n",
    "### Layer Normalization\n",
    "\n",
    "Normalise sur la dimension des features (pas sur le batch comme BatchNorm).\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta$$\n",
    "\n",
    "### Architecture d'un bloc\n",
    "\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "  ├──────────────────┐\n",
    "  ↓                  │ (residual)\n",
    "LayerNorm            │\n",
    "  ↓                  │\n",
    "Multi-Head Attention │\n",
    "  ↓                  │\n",
    "Dropout              │\n",
    "  ↓                  │\n",
    "  + ←────────────────┘\n",
    "  ↓\n",
    "  ├──────────────────┐\n",
    "  ↓                  │ (residual)\n",
    "LayerNorm            │\n",
    "  ↓                  │\n",
    "Feed-Forward         │\n",
    "  ↓                  │\n",
    "Dropout              │\n",
    "  ↓                  │\n",
    "  + ←────────────────┘\n",
    "  ↓\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démonstration Layer Normalization\n",
    "x = torch.randn(2, 4, 8)  # (batch, seq, features)\n",
    "\n",
    "# Layer Norm normalise sur la dernière dimension (features)\n",
    "ln = nn.LayerNorm(8)\n",
    "x_norm = ln(x)\n",
    "\n",
    "print(f\"Avant LayerNorm:\")\n",
    "print(f\"  Moyenne par position: {x[0, 0].mean():.4f}\")\n",
    "print(f\"  Std par position: {x[0, 0].std():.4f}\")\n",
    "\n",
    "print(f\"\\nAprès LayerNorm:\")\n",
    "print(f\"  Moyenne par position: {x_norm[0, 0].mean():.4f}\")\n",
    "print(f\"  Std par position: {x_norm[0, 0].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Bloc Transformer Encoder\n",
    "\n",
    "### Exercice 3 : Assembler le bloc complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Un bloc de Transformer Encoder.\n",
    "    \n",
    "    Args:\n",
    "        embed_dim: Dimension des embeddings\n",
    "        num_heads: Nombre de têtes d'attention\n",
    "        ff_dim: Dimension du feed-forward (défaut: 4 * embed_dim)\n",
    "        dropout: Taux de dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        if ff_dim is None:\n",
    "            ff_dim = 4 * embed_dim\n",
    "        \n",
    "        # TODO: Créer les composants\n",
    "        # 1. Multi-Head Attention\n",
    "        self.attention = None  # MultiHeadAttention(embed_dim, num_heads)\n",
    "        \n",
    "        # 2. Feed-Forward\n",
    "        self.feed_forward = None  # FeedForward(embed_dim, ff_dim, dropout)\n",
    "        \n",
    "        # 3. Layer Normalizations (2)\n",
    "        self.norm1 = None  # nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = None  # nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # 4. Dropout (2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input, shape (batch, seq_len, embed_dim)\n",
    "            mask: Masque optionnel\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        # TODO: Implémenter le forward\n",
    "        # Note: On utilise \"Pre-LN\" (LayerNorm avant l'opération)\n",
    "        \n",
    "        # Bloc 1: Attention avec résiduel\n",
    "        # residual = x\n",
    "        # x = self.norm1(x)\n",
    "        # x, _ = self.attention(x, mask)\n",
    "        # x = self.dropout1(x)\n",
    "        # x = residual + x\n",
    "        \n",
    "        # Bloc 2: Feed-Forward avec résiduel\n",
    "        # residual = x\n",
    "        # x = self.norm2(x)\n",
    "        # x = self.feed_forward(x)\n",
    "        # x = self.dropout2(x)\n",
    "        # x = residual + x\n",
    "        \n",
    "        return None  # À compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du bloc\n",
    "block = TransformerEncoderBlock(embed_dim=32, num_heads=4, dropout=0.1)\n",
    "\n",
    "x = torch.randn(2, 6, 32)\n",
    "out = block(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")  # Doit être identique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Transformer Encoder Complet\n",
    "\n",
    "On empile N blocs et on ajoute l'embedding + positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 4 : Transformer Encoder complet\n",
    "# ============================================\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder complet.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Taille du vocabulaire\n",
    "        embed_dim: Dimension des embeddings\n",
    "        num_heads: Nombre de têtes d'attention\n",
    "        num_layers: Nombre de blocs Transformer\n",
    "        ff_dim: Dimension du feed-forward\n",
    "        max_len: Longueur max des séquences\n",
    "        dropout: Taux de dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers,\n",
    "                 ff_dim=None, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Créer les composants\n",
    "        \n",
    "        # 1. Token Embedding\n",
    "        self.token_embedding = None  # nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # 2. Positional Encoding\n",
    "        self.positional_encoding = None  # PositionalEncoding(embed_dim, max_len, dropout)\n",
    "        \n",
    "        # 3. Liste de N blocs Transformer\n",
    "        self.layers = nn.ModuleList([\n",
    "            # TransformerEncoderBlock(embed_dim, num_heads, ff_dim, dropout)\n",
    "            # for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # 4. Layer Norm finale\n",
    "        self.final_norm = None  # nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Token indices, shape (batch, seq_len)\n",
    "            mask: Masque optionnel\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        # TODO: Implémenter\n",
    "        # 1. Token embedding (+ scaling par sqrt(embed_dim))\n",
    "        # x = self.token_embedding(x) * math.sqrt(self.embed_dim)\n",
    "        \n",
    "        # 2. Ajouter positional encoding\n",
    "        # x = self.positional_encoding(x)\n",
    "        \n",
    "        # 3. Passer par tous les blocs\n",
    "        # for layer in self.layers:\n",
    "        #     x = layer(x, mask)\n",
    "        \n",
    "        # 4. Normalisation finale\n",
    "        # x = self.final_norm(x)\n",
    "        \n",
    "        return None  # À compléter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "encoder = TransformerEncoder(\n",
    "    vocab_size=1000,\n",
    "    embed_dim=64,\n",
    "    num_heads=4,\n",
    "    num_layers=3,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Input : indices de tokens\n",
    "token_ids = torch.randint(0, 1000, (2, 10))  # batch=2, seq=10\n",
    "\n",
    "output = encoder(token_ids)\n",
    "\n",
    "print(f\"Token IDs shape: {token_ids.shape}\")\n",
    "print(f\"Encoder output shape: {output.shape}\")  # (2, 10, 64)\n",
    "\n",
    "# Compter les paramètres\n",
    "num_params = sum(p.numel() for p in encoder.parameters())\n",
    "print(f\"\\nNombre de paramètres: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Application : Classification de séquences\n",
    "\n",
    "Ajoutons une tête de classification pour faire de la classification de texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer pour la classification de texte.\n",
    "    Utilise le token [CLS] (premier token) pour la classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers,\n",
    "                 num_classes, ff_dim=None, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = TransformerEncoder(\n",
    "            vocab_size, embed_dim, num_heads, num_layers,\n",
    "            ff_dim, max_len, dropout\n",
    "        )\n",
    "        \n",
    "        # Tête de classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        encoded = self.encoder(x)  # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # Prendre le premier token (équivalent [CLS])\n",
    "        cls_output = encoded[:, 0, :]  # (batch, embed_dim)\n",
    "        \n",
    "        # Classifier\n",
    "        logits = self.classifier(cls_output)  # (batch, num_classes)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du classifieur\n",
    "classifier = TransformerClassifier(\n",
    "    vocab_size=1000,\n",
    "    embed_dim=64,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    num_classes=3,  # ex: positif, négatif, neutre\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "token_ids = torch.randint(0, 1000, (4, 20))  # 4 séquences de 20 tokens\n",
    "logits = classifier(token_ids)\n",
    "\n",
    "print(f\"Input shape: {token_ids.shape}\")\n",
    "print(f\"Logits shape: {logits.shape}\")  # (4, 3)\n",
    "print(f\"\\nPrédictions (après softmax): {F.softmax(logits, dim=-1)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Récapitulatif\n",
    "\n",
    "### Architecture Transformer Encoder\n",
    "\n",
    "```\n",
    "Tokens (indices)\n",
    "      ↓\n",
    "Token Embedding (× √d)\n",
    "      ↓\n",
    "Positional Encoding (+)\n",
    "      ↓\n",
    "┌─────────────────────┐\n",
    "│  Transformer Block  │ × N\n",
    "│  ├─ LayerNorm       │\n",
    "│  ├─ Multi-Head Attn │\n",
    "│  ├─ Residual        │\n",
    "│  ├─ LayerNorm       │\n",
    "│  ├─ Feed-Forward    │\n",
    "│  └─ Residual        │\n",
    "└─────────────────────┘\n",
    "      ↓\n",
    "Final LayerNorm\n",
    "      ↓\n",
    "Output (contextualized embeddings)\n",
    "```\n",
    "\n",
    "### Composants clés\n",
    "\n",
    "| Composant | Rôle |\n",
    "|-----------|------|\n",
    "| Positional Encoding | Injecter l'information de position |\n",
    "| Multi-Head Attention | Capturer les relations entre tokens |\n",
    "| Feed-Forward | Transformation non-linéaire |\n",
    "| Residuals | Faciliter le flux des gradients |\n",
    "| Layer Norm | Stabiliser l'entraînement |\n",
    "\n",
    "### Prochaine session\n",
    "\n",
    "Nous allons **entraîner** ce Transformer sur une tâche de classification de texte (Fake News) !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Pour aller plus loin (optionnel)\n",
    "\n",
    "### Visualisation des embeddings positionnels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Installation sklearn pour cette section optionnelle\n!pip install scikit-learn -q\n\n# Les positions proches ont des encodages similaires\npe = PositionalEncoding(embed_dim=64, max_len=100, dropout=0.0)\npe_matrix = pe.pe[0].numpy()\n\n# Calculer la similarité cosinus entre positions\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsimilarity = cosine_similarity(pe_matrix[:30])\n\nplt.figure(figsize=(8, 6))\nplt.imshow(similarity, cmap='RdBu')\nplt.colorbar(label='Similarité cosinus')\nplt.xlabel('Position')\nplt.ylabel('Position')\nplt.title('Similarité entre positions (Positional Encoding)')\nplt.show()\n\nprint(\"Observation: Les positions proches ont des encodages plus similaires.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}