{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet C - Mini-GPT : G√©n√©ration de Texte\n",
    "\n",
    "**Module** : R√©seaux de Neurones Approfondissement  \n",
    "**Dur√©e** : 2h  \n",
    "**Objectif** : Construire et entra√Æner un petit mod√®le g√©n√©ratif type GPT\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs du projet\n",
    "\n",
    "Dans ce projet, vous allez :\n",
    "1. Comprendre la diff√©rence entre Encodeur (BERT) et D√©codeur (GPT)\n",
    "2. Impl√©menter le masque causal pour la g√©n√©ration\n",
    "3. Entra√Æner un mini-GPT sur un corpus fran√ßais\n",
    "4. G√©n√©rer du texte de mani√®re autoregressive"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "> **üìå Note p√©dagogique** : Cette session introduit le **d√©codeur** (GPT) avec son **masque causal**. \n> C'est une architecture diff√©rente de l'encodeur (BERT) vu en Sessions 1-4.\n>\n> **Cross-attention** (mentionn√© en Session 2) : Dans les architectures **encodeur-d√©codeur** compl√®tes (traduction, T5), \n> le d√©codeur utilise du cross-attention pour \"interroger\" l'encodeur. Ici, on se concentre sur le **d√©codeur seul** (GPT), \n> qui n'utilise que la **self-attention causale**.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib numpy tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Encodeur vs D√©codeur : Quelle diff√©rence ?\n",
    "\n",
    "### BERT (Encodeur) - Ce qu'on a fait jusqu'ici\n",
    "\n",
    "```\n",
    "Entr√©e:  \"Le chat [MASK] sur le canap√©\"\n",
    "                    ‚Üì\n",
    "         Chaque mot voit TOUS les autres\n",
    "                    ‚Üì\n",
    "Sortie:  Pr√©dire [MASK] = \"dort\"\n",
    "```\n",
    "\n",
    "- **Bidirectionnel** : chaque token voit le pass√© ET le futur\n",
    "- **Usage** : Classification, NER, Question-Answering extractif\n",
    "\n",
    "### GPT (D√©codeur) - Ce qu'on fait maintenant\n",
    "\n",
    "```\n",
    "Entr√©e:  \"Le chat dort sur le\"\n",
    "                    ‚Üì\n",
    "         Chaque mot voit SEULEMENT les pr√©c√©dents\n",
    "                    ‚Üì\n",
    "Sortie:  Pr√©dire le mot suivant = \"canap√©\"\n",
    "```\n",
    "\n",
    "- **Unidirectionnel (causal)** : chaque token ne voit que le pass√©\n",
    "- **Usage** : G√©n√©ration de texte, chatbots, compl√©tion\n",
    "\n",
    "### Le masque causal\n",
    "\n",
    "Pour emp√™cher un token de \"tricher\" en regardant le futur, on utilise un **masque causal** :\n",
    "\n",
    "```\n",
    "              Le   chat  dort  sur   le\n",
    "      Le    [  1     0     0     0    0  ]   ‚Üê \"Le\" ne voit que lui-m√™me\n",
    "     chat   [  1     1     0     0    0  ]   ‚Üê \"chat\" voit \"Le\" et lui-m√™me\n",
    "     dort   [  1     1     1     0    0  ]   ‚Üê \"dort\" voit les 3 premiers\n",
    "      sur   [  1     1     1     1    0  ]   ‚Üê etc.\n",
    "       le   [  1     1     1     1    1  ]   ‚Üê dernier voit tout le pass√©\n",
    "```\n",
    "\n",
    "Les 0 deviennent `-‚àû` avant le softmax ‚Üí attention = 0 sur ces positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Impl√©mentation du Masque Causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Cr√©er le masque causal\n",
    "# ============================================\n",
    "\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Cr√©e un masque causal (triangulaire inf√©rieur).\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Longueur de la s√©quence\n",
    "    \n",
    "    Returns:\n",
    "        mask: Tensor bool√©en (seq_len, seq_len)\n",
    "              True = position √† masquer (ne pas regarder)\n",
    "              False = position visible\n",
    "    \"\"\"\n",
    "    # TODO: Cr√©er une matrice triangulaire sup√©rieure de True\n",
    "    # Indice: torch.triu(torch.ones(...), diagonal=1)\n",
    "    # diagonal=1 pour que la diagonale soit visible (un token se voit lui-m√™me)\n",
    "    \n",
    "    mask = None  # √Ä compl√©ter\n",
    "    \n",
    "    return mask.bool()\n",
    "\n",
    "# Test\n",
    "mask = create_causal_mask(5)\n",
    "print(\"Masque causal (True = masqu√©) :\")\n",
    "print(mask.int())  # Affiche 0 et 1 pour plus de lisibilit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(~mask, cmap='Blues')  # ~ inverse pour afficher \"visible\" en bleu\n",
    "plt.xticks(range(5), ['pos 0', 'pos 1', 'pos 2', 'pos 3', 'pos 4'])\n",
    "plt.yticks(range(5), ['pos 0', 'pos 1', 'pos 2', 'pos 3', 'pos 4'])\n",
    "plt.xlabel(\"Positions regard√©es (Keys)\")\n",
    "plt.ylabel(\"Positions qui regardent (Queries)\")\n",
    "plt.title(\"Masque Causal (bleu = visible)\")\n",
    "plt.colorbar(label=\"Visible\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Architecture Mini-GPT\n",
    "\n",
    "On reprend notre Transformer et on ajoute le masque causal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Self-Attention avec masque causal (style GPT).\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Pr√©-calculer le masque causal (enregistr√© comme buffer)\n",
    "        mask = torch.triu(torch.ones(max_len, max_len), diagonal=1).bool()\n",
    "        self.register_buffer('mask', mask)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, S, _ = x.shape\n",
    "        \n",
    "        # Projections Q, K, V\n",
    "        Q = self.W_q(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Appliquer le masque causal\n",
    "        scores = scores.masked_fill(self.mask[:S, :S], float('-inf'))\n",
    "        \n",
    "        # Softmax et dropout\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Output\n",
    "        out = (attn @ V).transpose(1, 2).contiguous().view(B, S, self.embed_dim)\n",
    "        return self.W_o(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    \"\"\"Un bloc de GPT (attention causale + FFN).\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(embed_dim, num_heads, max_len, dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attn(self.norm1(x)))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 2 : Compl√©ter le Mini-GPT\n",
    "# ============================================\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Mini-GPT pour la g√©n√©ration de texte caract√®re par caract√®re.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Taille du vocabulaire (nombre de caract√®res uniques)\n",
    "        embed_dim: Dimension des embeddings\n",
    "        num_heads: Nombre de t√™tes d'attention\n",
    "        num_layers: Nombre de blocs GPT\n",
    "        max_len: Longueur maximale des s√©quences\n",
    "        dropout: Taux de dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_layers=4, \n",
    "                 max_len=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Position embedding (apprise, pas sinuso√Ødale)\n",
    "        self.position_embedding = nn.Embedding(max_len, embed_dim)\n",
    "        \n",
    "        # Blocs GPT\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GPTBlock(embed_dim, num_heads, max_len, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer norm finale\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # T√™te de pr√©diction (projette vers le vocabulaire)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Indices de tokens, shape (batch, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Scores pour chaque token du vocabulaire, shape (batch, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        B, S = x.shape\n",
    "        \n",
    "        # TODO: Impl√©menter le forward\n",
    "        \n",
    "        # 1. Token embeddings\n",
    "        tok_emb = None  # self.token_embedding(x)\n",
    "        \n",
    "        # 2. Position embeddings\n",
    "        # Cr√©er les positions [0, 1, 2, ..., S-1]\n",
    "        positions = None  # torch.arange(S, device=x.device)\n",
    "        pos_emb = None  # self.position_embedding(positions)\n",
    "        \n",
    "        # 3. Additionner et dropout\n",
    "        x = None  # self.dropout(tok_emb + pos_emb)\n",
    "        \n",
    "        # 4. Passer par tous les blocs\n",
    "        # for block in self.blocks:\n",
    "        #     x = block(x)\n",
    "        \n",
    "        # 5. Layer norm finale\n",
    "        x = None  # self.norm(x)\n",
    "        \n",
    "        # 6. Projection vers le vocabulaire\n",
    "        logits = None  # self.head(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, context, max_new_tokens, temperature=1.0):\n",
    "        \"\"\"\n",
    "        G√©n√®re du texte de mani√®re autoregressive.\n",
    "        \n",
    "        Args:\n",
    "            context: Tensor de shape (1, seq_len) avec le contexte initial\n",
    "            max_new_tokens: Nombre de tokens √† g√©n√©rer\n",
    "            temperature: Contr√¥le la \"cr√©ativit√©\" (1.0 = normal, <1 = conservateur, >1 = cr√©atif)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor avec le contexte + tokens g√©n√©r√©s\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Tronquer si n√©cessaire\n",
    "            context_truncated = context[:, -self.max_len:]\n",
    "            \n",
    "            # Forward\n",
    "            logits = self(context_truncated)\n",
    "            \n",
    "            # Prendre les logits du dernier token\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # √âchantillonner\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Ajouter au contexte\n",
    "            context = torch.cat([context, next_token], dim=1)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test rapide\n",
    "model_test = MiniGPT(vocab_size=100, embed_dim=64, num_heads=4, num_layers=2)\n",
    "x_test = torch.randint(0, 100, (2, 32))\n",
    "out_test = model_test(x_test)\n",
    "print(f\"Input: {x_test.shape}\")\n",
    "print(f\"Output: {out_test.shape}\")  # Attendu: (2, 32, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Pr√©paration des Donn√©es\n",
    "\n",
    "On va entra√Æner notre Mini-GPT sur un corpus de texte fran√ßais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√©l√©charger un texte fran√ßais (Les Mis√©rables - Victor Hugo)\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/135/pg135.txt\"\n",
    "print(\"T√©l√©chargement du corpus...\")\n",
    "\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    text = response.read().decode('utf-8')\n",
    "\n",
    "# Nettoyer (garder une portion pour l'entra√Ænement rapide)\n",
    "# Le texte commence apr√®s les en-t√™tes Gutenberg\n",
    "start_marker = \"PREMI√àRE PARTIE\"\n",
    "start_idx = text.find(start_marker)\n",
    "if start_idx != -1:\n",
    "    text = text[start_idx:]\n",
    "\n",
    "# Limiter la taille pour Colab (environ 500Ko)\n",
    "text = text[:500000]\n",
    "\n",
    "print(f\"Taille du corpus: {len(text):,} caract√®res\")\n",
    "print(f\"\\nExtrait:\\n{text[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le vocabulaire (niveau caract√®re)\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Vocabulaire: {vocab_size} caract√®res uniques\")\n",
    "print(f\"Caract√®res: {''.join(chars[:50])}...\")\n",
    "\n",
    "# Mappings\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Fonctions d'encodage/d√©codage\n",
    "def encode(s):\n",
    "    return [char2idx[c] for c in s]\n",
    "\n",
    "def decode(indices):\n",
    "    return ''.join([idx2char[i] for i in indices])\n",
    "\n",
    "# Test\n",
    "test_str = \"Bonjour!\"\n",
    "encoded = encode(test_str)\n",
    "decoded = decode(encoded)\n",
    "print(f\"\\nTest: '{test_str}' -> {encoded} -> '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder tout le texte\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "\n",
    "# Split train/val (90/10)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Train: {len(train_data):,} tokens\")\n",
    "print(f\"Val: {len(val_data):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # x = contexte, y = contexte d√©cal√© de 1 (ce qu'on veut pr√©dire)\n",
    "        x = self.data[idx:idx + self.block_size]\n",
    "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "# Configuration\n",
    "BLOCK_SIZE = 128  # Longueur des s√©quences\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = TextDataset(train_data, BLOCK_SIZE)\n",
    "val_dataset = TextDataset(val_data, BLOCK_SIZE)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Batches train: {len(train_loader)}\")\n",
    "print(f\"Batches val: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier un batch\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(f\"x shape: {x_batch.shape}\")  # (batch, block_size)\n",
    "print(f\"y shape: {y_batch.shape}\")  # (batch, block_size)\n",
    "\n",
    "# Exemple\n",
    "print(f\"\\nExemple (input):  '{decode(x_batch[0].tolist())[:50]}...'\")\n",
    "print(f\"Exemple (target): '{decode(y_batch[0].tolist())[:50]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le mod√®le\n",
    "model = MiniGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=128,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    "    max_len=BLOCK_SIZE,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Compter les param√®tres\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Param√®tres: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 3 : Boucle d'entra√Ænement\n",
    "# ============================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # TODO: Impl√©menter\n",
    "        # 1. Zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward\n",
    "        logits = model(x)  # (batch, seq_len, vocab_size)\n",
    "        \n",
    "        # 3. Loss (cross-entropy)\n",
    "        # Attention: reshape pour cross_entropy\n",
    "        # logits: (B, S, V) -> (B*S, V)\n",
    "        # y: (B, S) -> (B*S,)\n",
    "        B, S, V = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(B*S, V), y.view(B*S))\n",
    "        \n",
    "        # 4. Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Gradient clipping (stabilit√©)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # 6. Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        B, S, V = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(B*S, V), y.view(B*S))\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EPOCHS = 5\n",
    "LR = 3e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "# Historique\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "print(\"D√©but de l'entra√Ænement...\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss = evaluate(model, val_loader, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # G√©n√©rer un √©chantillon\n",
    "    context = torch.tensor([encode(\"Jean Valjean \")], dtype=torch.long, device=device)\n",
    "    generated = model.generate(context, max_new_tokens=100, temperature=0.8)\n",
    "    print(f\"  G√©n√©ration: {decode(generated[0].tolist())[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(history['train_loss'], label='Train')\n",
    "plt.plot(history['val_loss'], label='Val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Courbe d\\'apprentissage')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. G√©n√©ration de Texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 4 : Exp√©rimenter avec la g√©n√©ration\n",
    "# ============================================\n",
    "\n",
    "def generate_text(prompt, max_tokens=200, temperature=1.0):\n",
    "    \"\"\"\n",
    "    G√©n√®re du texte √† partir d'un prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Texte de d√©part\n",
    "        max_tokens: Nombre de caract√®res √† g√©n√©rer\n",
    "        temperature: Cr√©ativit√© (0.5=conservateur, 1.0=normal, 1.5=cr√©atif)\n",
    "    \"\"\"\n",
    "    context = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "    generated = model.generate(context, max_new_tokens=max_tokens, temperature=temperature)\n",
    "    return decode(generated[0].tolist())\n",
    "\n",
    "# Tests avec diff√©rentes temp√©ratures\n",
    "prompt = \"La nuit √©tait \"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    print(f\"\\n--- Temp√©rature = {temp} ---\")\n",
    "    print(generate_text(prompt, max_tokens=150, temperature=temp))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©ration interactive\n",
    "prompts = [\n",
    "    \"Il marchait dans la rue \",\n",
    "    \"L'√©v√™que dit √† \",\n",
    "    \"Paris est une ville \",\n",
    "    \"Le soleil se levait sur \",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(generate_text(prompt, max_tokens=200, temperature=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Analyse et Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les probabilit√©s de pr√©diction\n",
    "def visualize_predictions(text):\n",
    "    \"\"\"Visualise les probabilit√©s de pr√©diction pour chaque caract√®re.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    x = torch.tensor([encode(text)], dtype=torch.long, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Pour chaque position, afficher le caract√®re pr√©dit\n",
    "    print(f\"Texte: '{text}'\\n\")\n",
    "    print(\"Position | R√©el | Pr√©dit | Confiance\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for i in range(len(text) - 1):\n",
    "        actual = text[i + 1]\n",
    "        pred_idx = probs[0, i].argmax().item()\n",
    "        pred_char = idx2char[pred_idx]\n",
    "        confidence = probs[0, i, char2idx[actual]].item()\n",
    "        \n",
    "        match = \"‚úì\" if pred_char == actual else \"‚úó\"\n",
    "        actual_display = repr(actual)[1:-1]  # Afficher les caract√®res sp√©ciaux\n",
    "        pred_display = repr(pred_char)[1:-1]\n",
    "        \n",
    "        print(f\"   {i:3d}   | '{actual_display:2s}' | '{pred_display:2s}'  | {confidence:.2%} {match}\")\n",
    "\n",
    "visualize_predictions(\"Jean Valjean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. R√©capitulatif\n",
    "\n",
    "### Ce que vous avez appris\n",
    "\n",
    "1. **Diff√©rence Encodeur/D√©codeur** : Le masque causal emp√™che de \"voir le futur\"\n",
    "2. **Architecture GPT** : Attention causale + g√©n√©ration autoregressive\n",
    "3. **Entra√Ænement** : Pr√©dire le caract√®re suivant (next token prediction)\n",
    "4. **Temp√©rature** : Contr√¥le le compromis entre coh√©rence et cr√©ativit√©\n",
    "\n",
    "### Comparaison avec les vrais GPT\n",
    "\n",
    "| | Notre Mini-GPT | GPT-2 Small | GPT-3 |\n",
    "|--|----------------|-------------|-------|\n",
    "| Param√®tres | ~1M | 117M | 175B |\n",
    "| Donn√©es | ~500Ko | 40Go | ~500Go |\n",
    "| Vocabulaire | Caract√®res | BPE (~50k) | BPE (~50k) |\n",
    "| Contexte | 128 | 1024 | 2048 |\n",
    "\n",
    "### Limitations de notre mod√®le\n",
    "\n",
    "- **Vocabulaire caract√®re** : Inefficace, perd la notion de \"mot\"\n",
    "- **Petit corpus** : Surapprentissage probable\n",
    "- **Contexte court** : Ne peut pas capturer les d√©pendances longues\n",
    "\n",
    "### Pour aller plus loin\n",
    "\n",
    "- Utiliser un tokenizer BPE (comme SentencePiece)\n",
    "- Entra√Æner sur plus de donn√©es\n",
    "- Ajouter des techniques modernes (RoPE, KV-cache, Flash Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espace pour vos exp√©rimentations\n",
    "\n",
    "# Essayez diff√©rents prompts !\n",
    "mon_prompt = \"Cosette regardait \"\n",
    "print(generate_text(mon_prompt, max_tokens=300, temperature=0.8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}