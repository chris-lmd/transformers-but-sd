{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 02 - Le Mécanisme d'Attention\n",
    "\n",
    "**Module** : Réseaux de Neurones Approfondissement  \n",
    "**Durée** : 2h  \n",
    "**Objectif** : Comprendre et implémenter le mécanisme d'attention, brique fondamentale des Transformers\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs pédagogiques\n",
    "\n",
    "À la fin de ce TP, vous serez capable de :\n",
    "1. Expliquer intuitivement ce qu'est l'attention\n",
    "2. Comprendre les concepts de Query, Key, Value\n",
    "3. Implémenter le Scaled Dot-Product Attention\n",
    "4. Visualiser et interpréter les poids d'attention\n",
    "\n",
    "---\n",
    "\n",
    "## Prérequis\n",
    "\n",
    "Ce TP suppose que vous avez complété le **TP 01 - Fondamentaux NLP** où vous avez découvert :\n",
    "- La tokenization (comment transformer du texte en nombres)\n",
    "- Les embeddings (représentations vectorielles des mots)\n",
    "- Word2Vec et la similarité sémantique\n",
    "- Un premier aperçu de l'attention\n",
    "\n",
    "Ici, nous allons **approfondir le mécanisme d'attention** et l'implémenter from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports\n",
    "\n",
    "Exécutez cette cellule pour installer les dépendances nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances (Google Colab)\n",
    "!pip install torch matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction : Pourquoi l'attention ?\n",
    "\n",
    "> **Note pédagogique** : Dans les sessions 2 à 4, on se concentre sur le **fonctionnement** de l'architecture (inférence/forward pass). L'**entraînement** (backpropagation, optimisation) sera abordé dans les projets.\n",
    "\n",
    "### 1.1 Les architectures séquentielles (RNN / LSTM)\n",
    "\n",
    "Les **réseaux récurrents (RNN)** traitent les séquences **mot par mot** :\n",
    "\n",
    "```\n",
    "        ┌───┐    ┌───┐    ┌───┐    ┌───┐    ┌───┐\n",
    "  x₁ ──▶│ h ├───▶│ h ├───▶│ h ├───▶│ h ├───▶│ h ├──▶ sortie\n",
    "  Le    └───┘    └───┘    └───┘    └───┘    └───┘\n",
    "           │        │        │        │        │\n",
    "          x₂       x₃       x₄       x₅       x₆\n",
    "         chat     dort      sur      le     canapé\n",
    "```\n",
    "\n",
    "**Problème** : L'information passe de cellule en cellule. Pour relier \"Le chat\" à \"canapé\", il faut traverser toute la chaîne → l'info se dégrade (gradient évanescent).\n",
    "\n",
    "Les **LSTM** ajoutent des \"portes\" pour mieux contrôler la mémoire :\n",
    "\n",
    "```\n",
    "                    ┌─────────────────────────┐\n",
    "                    │      CELLULE LSTM       │\n",
    "        ┌───────────┬───────────┬─────────────┤\n",
    "        │  Porte    │  Porte    │    Porte    │\n",
    "        │  Oubli    │  Entrée   │   Sortie    │\n",
    "        └───────────┴───────────┴─────────────┘\n",
    "              │           │            │\n",
    "          Effacer?    Ajouter?    Utiliser?\n",
    "```\n",
    "\n",
    "**Amélioration** : Les LSTM retiennent mieux les infos longue distance.\n",
    "**Mais** : Toujours séquentiel (lent) et limité sur les très longues séquences.\n",
    "\n",
    "### 1.2 L'architecture Transformer\n",
    "\n",
    "Le **Transformer** (2017) abandonne la récurrence. Chaque mot peut regarder **tous les autres directement** :\n",
    "\n",
    "```\n",
    "Entrée: \"Le chat dort sur le canapé\" (6 tokens)\n",
    "         │    │    │    │    │    │\n",
    "         ▼    ▼    ▼    ▼    ▼    ▼\n",
    "┌─────────────────────────────────────────────┐\n",
    "│          EMBEDDINGS (6 vecteurs)            │\n",
    "└─────────────────────────────────────────────┘\n",
    "         │    │    │    │    │    │\n",
    "         ▼    ▼    ▼    ▼    ▼    ▼\n",
    "┌─────────────────────────────────────────────┐\n",
    "│             SELF-ATTENTION                  │\n",
    "│   Chaque vecteur regarde les 5 autres       │\n",
    "│   → Enrichit chaque mot avec le CONTEXTE    │\n",
    "└─────────────────────────────────────────────┘\n",
    "         │    │    │    │    │    │\n",
    "         ▼    ▼    ▼    ▼    ▼    ▼\n",
    "       (6 vecteurs enrichis)\n",
    "         │    │    │    │    │    │\n",
    "         ▼    ▼    ▼    ▼    ▼    ▼\n",
    "┌─────────────────────────────────────────────┐\n",
    "│        FEED-FORWARD (par position)          │\n",
    "│   Exploite le contexte enrichi              │\n",
    "│   (comme un réseau de neurones classique)   │\n",
    "└─────────────────────────────────────────────┘\n",
    "         │    │    │    │    │    │\n",
    "         ▼    ▼    ▼    ▼    ▼    ▼\n",
    "      Sortie: 6 vecteurs transformés\n",
    "```\n",
    "\n",
    "**Points clés** :\n",
    "- **Entrée = Sortie** : Si tu entres 6 mots → tu obtiens 6 vecteurs enrichis\n",
    "- **Taille variable** : Tu peux entrer 5, 50, ou 500 mots (jusqu'à une limite : 512 pour BERT, 128K pour GPT-4)\n",
    "- **Self-Attention** : Donne du contexte à chaque mot\n",
    "- **Feed-Forward** : Exploite ce contexte (transformation non-linéaire)\n",
    "\n",
    "**Que sort le Transformer ?**\n",
    "\n",
    "Le Transformer produit des **vecteurs enrichis** (représentations). Une couche de sortie (ajoutée selon la tâche) les transforme en résultat :\n",
    "- **Classification** → probabilité par classe (ex: 70% positif, 30% négatif)\n",
    "- **Génération** → probabilité du prochain mot\n",
    "- **Traduction** → phrase dans l'autre langue\n",
    "\n",
    "### Comment les mots entrent dans le Transformer ?\n",
    "\n",
    "Chaque mot passe par **deux étapes** avant d'entrer :\n",
    "\n",
    "```\n",
    "Mot \"chat\" (position 1)\n",
    "        │\n",
    "        ▼\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│ Token Embedding (fixe pour chaque token)        │\n",
    "│ \"chat\" → [0.8, 0.1, 0.3, ...]                   │\n",
    "└─────────────────────────────────────────────────┘\n",
    "        │\n",
    "        + (addition)\n",
    "        │\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│ Positional Encoding (fixe pour chaque position) │\n",
    "│ position 1 → [0.0, 0.1, 0.0, ...]               │\n",
    "└─────────────────────────────────────────────────┘\n",
    "        │\n",
    "        ▼\n",
    "Vecteur d'entrée = [0.8, 0.2, 0.3, ...]\n",
    "```\n",
    "\n",
    "**Deux composants distincts, tous deux figés après entraînement :**\n",
    "\n",
    "| Composant | Taille | Rôle |\n",
    "|-----------|--------|------|\n",
    "| Token embeddings | ~50k × dim | \"Qui suis-je ?\" (sens du mot) |\n",
    "| Positional encodings | max_len × dim | \"Où suis-je ?\" (position dans la phrase) |\n",
    "\n",
    "**Pourquoi c'est important ?** Sans le positional encoding, le modèle ne distinguerait pas :\n",
    "- *\"Le chat mange la souris\"*\n",
    "- *\"La souris mange le chat\"*\n",
    "\n",
    "(Mêmes tokens, ordre différent → sens opposé !)\n",
    "\n",
    "> **Note** : On implémentera le Positional Encoding en détail dans le **TP 04 - Architecture Transformer**.\n",
    "\n",
    "### 1.3 Empilement des blocs\n",
    "\n",
    "Ces blocs (Attention + FFN) sont **empilés** : la sortie de l'un devient l'entrée du suivant.\n",
    "\n",
    "```\n",
    "Entrée (6 vecteurs)\n",
    "         │\n",
    "         ▼\n",
    "┌─────────────────┐\n",
    "│   Attention 1   │\n",
    "│        ↓        │  Bloc 1\n",
    "│     FFN 1       │\n",
    "└────────┬────────┘\n",
    "         │\n",
    "         ▼\n",
    "┌─────────────────┐\n",
    "│   Attention 2   │\n",
    "│        ↓        │  Bloc 2\n",
    "│     FFN 2       │\n",
    "└────────┬────────┘\n",
    "         │\n",
    "        ...\n",
    "         │\n",
    "         ▼\n",
    "┌─────────────────┐\n",
    "│   Attention N   │\n",
    "│        ↓        │  Bloc N (ex: N=12 pour BERT)\n",
    "│     FFN N       │\n",
    "└────────┬────────┘\n",
    "         │\n",
    "         ▼\n",
    "Sortie (6 vecteurs très enrichis)\n",
    "```\n",
    "\n",
    "Chaque passage enrichit les représentations. Après N blocs, chaque mot \"comprend\" toute la phrase.\n",
    "\n",
    "### 1.4 Ce qu'on va construire\n",
    "\n",
    "```\n",
    "    TRANSFORMER\n",
    "    ┌────────────────────────────┐\n",
    "    │  Embedding + Positional    │\n",
    "    ├────────────────────────────┤\n",
    "    │ ┌────────────────────────┐ │\n",
    "    │ │   SELF-ATTENTION  ◀────┼─┼─── Sessions 2-3\n",
    "    │ └────────────────────────┘ │\n",
    "    │ ┌────────────────────────┐ │\n",
    "    │ │     FEED-FORWARD       │ │\n",
    "    │ └────────────────────────┘ │\n",
    "    │         × N blocs          │\n",
    "    ├────────────────────────────┤\n",
    "    │     Couche de sortie       │\n",
    "    └────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Plan du cours** :\n",
    "- **Session 1** : Fondamentaux NLP (tokenization, embeddings)\n",
    "- **Session 2** : Mécanisme d'attention (ce TP)\n",
    "- **Session 3** : Multi-Head Attention\n",
    "- **Session 4** : Assembler le Transformer complet\n",
    "- **Sessions 5-6** : Projets\n",
    "\n",
    "### 1.5 L'idée clé de l'attention\n",
    "\n",
    "L'attention répond à la question : **\"Pour comprendre ce mot, quels autres mots dois-je regarder ?\"**\n",
    "\n",
    "**Exemple** : *\"Le chat qui dormait sur le canapé a sauté\"*\n",
    "- Pour comprendre **\"a sauté\"** → regarder **\"chat\"** (le sujet, pas \"canapé\")\n",
    "\n",
    "### Analogie : La bibliothèque\n",
    "\n",
    "- **Query (Q)** : Votre question (\"Je cherche un livre sur les chats\")\n",
    "- **Key (K)** : Les mots-clés de chaque livre\n",
    "- **Value (V)** : Le contenu des livres\n",
    "\n",
    "L'attention compare votre **question** aux **mots-clés**, puis retourne un mélange pondéré des **contenus** les plus pertinents.\n",
    "\n",
    "---\n",
    "\n",
    "### Pour approfondir RNN/LSTM (optionnel)\n",
    "\n",
    "**Vidéos en français** :\n",
    "- [Machine Learnia - Les RNN expliqués](https://www.youtube.com/watch?v=EL439RMv3Xc) (~20 min)\n",
    "- [Science4All - Comprendre les LSTM](https://www.youtube.com/watch?v=WCUNPb-5EYI) (~15 min)\n",
    "\n",
    "**Articles en français** :\n",
    "- [Pensée Artificielle - Introduction aux RNN](https://www.penseeartificielle.fr/comprendre-reseaux-neurones-recurrents-rnn/)\n",
    "- [DataScientest - LSTM expliqué simplement](https://datascientest.com/lstm-tout-savoir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Visualisation intuitive\n",
    "\n",
    "Avant de coder, visualisons ce que fait l'attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple simple : attention dans une phrase\n",
    "phrase = [\"Le\", \"chat\", \"mange\", \"la\", \"souris\"]\n",
    "\n",
    "# Matrice d'attention simulée (quels mots regardent quels mots ?)\n",
    "# Chaque ligne = un mot qui \"regarde\" les autres\n",
    "attention_simulee = torch.tensor([\n",
    "    [0.8, 0.1, 0.05, 0.03, 0.02],  # \"Le\" regarde surtout lui-même\n",
    "    [0.1, 0.7, 0.1, 0.05, 0.05],   # \"chat\" regarde surtout lui-même\n",
    "    [0.05, 0.4, 0.4, 0.05, 0.1],   # \"mange\" regarde \"chat\" et lui-même\n",
    "    [0.02, 0.03, 0.05, 0.8, 0.1],  # \"la\" regarde surtout lui-même\n",
    "    [0.02, 0.1, 0.2, 0.08, 0.6],   # \"souris\" regarde \"mange\" et elle-même\n",
    "])\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention_simulee, cmap='Blues')\n",
    "plt.xticks(range(5), phrase)\n",
    "plt.yticks(range(5), phrase)\n",
    "plt.xlabel(\"Mots regardés (Keys)\")\n",
    "plt.ylabel(\"Mots qui regardent (Queries)\")\n",
    "plt.title(\"Qui regarde qui ? (Matrice d'attention)\")\n",
    "plt.colorbar(label=\"Poids d'attention\")\n",
    "\n",
    "# Afficher les valeurs\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        plt.text(j, i, f'{attention_simulee[i,j]:.2f}', \n",
    "                ha='center', va='center',\n",
    "                color='white' if attention_simulee[i,j] > 0.5 else 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Dans cette matrice, quel mot le verbe \"mange\" regarde-t-il le plus ? Pourquoi est-ce logique ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Scaled Dot-Product Attention\n",
    "\n",
    "Le **Scaled Dot-Product Attention** est l'opération qui calcule la **matrice d'attention** (les poids \"qui regarde qui\") et produit les vecteurs enrichis en sortie.\n",
    "\n",
    "**Rappel des 3 vecteurs :**\n",
    "\n",
    "| Vecteur | Rôle | Sert à... |\n",
    "|---------|------|----------|\n",
    "| **Q** (Query) | Ce que je cherche | Calculer les poids (avec K) |\n",
    "| **K** (Key) | Mon identité / étiquette | Calculer les poids (avec Q) |\n",
    "| **V** (Value) | Mon contenu / l'info que je transmets | Être récupéré selon les poids |\n",
    "\n",
    "**Concrètement** : La matrice d'attention dit \"à quel point chaque mot m'intéresse\" (calculée avec Q et K). Ensuite on récupère l'**information** (V) de ces mots, pondérée par ces poids.\n",
    "\n",
    "**Exemple** : Pour \"dort\" dans [\"Le\", \"chat\", \"dort\"], si les poids sont [0.26, 0.42, 0.32] :\n",
    "- On récupère 26% du **contenu** (V) de \"Le\"\n",
    "- On récupère 42% du **contenu** (V) de \"chat\"\n",
    "- On récupère 32% du **contenu** (V) de \"dort\"\n",
    "\n",
    "**Attention au vocabulaire** :\n",
    "- `softmax(QK^T/√d_k)` = **matrice d'attention** (les poids)\n",
    "- `Attention(Q,K,V)` = matrice d'attention × V = **sortie** (vecteurs enrichis)\n",
    "\n",
    "### La formule\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Où :\n",
    "- $Q$ (Query) : Ce que je cherche - shape `(seq_len, d_k)`\n",
    "- $K$ (Key) : Les étiquettes de ce qui est disponible - shape `(seq_len, d_k)`\n",
    "- $V$ (Value) : Le contenu disponible - shape `(seq_len, d_v)`\n",
    "- $d_k$ : Dimension des clés (pour normaliser)\n",
    "\n",
    "> **Note** : Q, K, V sont obtenus à partir des embeddings via des matrices de poids apprenables. Cela permet à chaque mot d'avoir une représentation adaptée à son rôle (chercher, s'identifier, transmettre).\n",
    "\n",
    "### Exemple concret\n",
    "\n",
    "Prenons la phrase **[\"Le\", \"chat\", \"dort\"]** avec des embeddings de dimension 4.\n",
    "\n",
    "Supposons qu'après transformation, on obtienne :\n",
    "\n",
    "```\n",
    "         Q (Queries)         K (Keys)           V (Values)\n",
    "Le    → [0.1, 0.2, 0.1, 0.0]  [0.9, 0.1, 0.0, 0.2]  [1.0, 0.0, 0.0, 0.0]\n",
    "chat  → [0.2, 0.8, 0.1, 0.3]  [0.2, 0.9, 0.2, 0.1]  [0.0, 1.0, 0.0, 0.0]\n",
    "dort  → [0.3, 0.7, 0.2, 0.1]  [0.1, 0.3, 0.8, 0.1]  [0.0, 0.0, 1.0, 0.0]\n",
    "```\n",
    "\n",
    "**Calculons l'attention pour \"dort\"** (quelle info récupère-t-il des autres mots ?) :\n",
    "\n",
    "**Étape 1 - Scores (Q·K^T)** : On compare la Query de \"dort\" aux Keys de tous les mots\n",
    "```\n",
    "Q_dort · K_Le   = 0.3×0.9 + 0.7×0.1 + 0.2×0.0 + 0.1×0.2 = 0.36\n",
    "Q_dort · K_chat = 0.3×0.2 + 0.7×0.9 + 0.2×0.2 + 0.1×0.1 = 0.74  ← score élevé !\n",
    "Q_dort · K_dort = 0.3×0.1 + 0.7×0.3 + 0.2×0.8 + 0.1×0.1 = 0.41\n",
    "\n",
    "Scores = [0.36, 0.74, 0.41]\n",
    "```\n",
    "\n",
    "**Étape 2 - Scaling (÷√d_k)** : On divise par √4 = 2\n",
    "```\n",
    "Scaled = [0.18, 0.37, 0.205]\n",
    "```\n",
    "\n",
    "**Étape 3 - Softmax** : On transforme en probabilités\n",
    "```\n",
    "Poids = [0.26, 0.42, 0.32]  (somme = 1)\n",
    "```\n",
    "\n",
    "**Étape 4 - Output (poids × V)** : Moyenne pondérée des Values\n",
    "```\n",
    "Output_dort = 0.26 × V_Le + 0.42 × V_chat + 0.32 × V_dort\n",
    "            = 0.26 × [1,0,0,0] + 0.42 × [0,1,0,0] + 0.32 × [0,0,1,0]\n",
    "            = [0.26, 0.42, 0.32, 0.0]\n",
    "```\n",
    "\n",
    "**Interprétation** : La nouvelle représentation de \"dort\" contient **42% d'info de \"chat\"** (le sujet), **32% de lui-même** (le verbe), et **26% de \"Le\"** (le déterminant). Le modèle a appris que pour comprendre un verbe, il faut surtout regarder son sujet.\n",
    "\n",
    "### Décomposition étape par étape\n",
    "\n",
    "1. **Scores** : $QK^T$ - Mesure la similarité entre queries et keys\n",
    "2. **Scaling** : Division par $\\sqrt{d_k}$ - Évite des valeurs trop grandes\n",
    "3. **Softmax** : Transforme en probabilités (somme = 1)\n",
    "4. **Output** : Multiplication par $V$ - Moyenne pondérée des values\n",
    "\n",
    "### Pourquoi softmax ? Pourquoi normaliser ?\n",
    "\n",
    "**Le softmax** transforme des scores quelconques en **probabilités** :\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "```\n",
    "Scores bruts :  [0.36, 0.74, 0.41]  (peuvent être négatifs, grands, etc.)\n",
    "                        ↓ softmax\n",
    "Probabilités :  [0.26, 0.42, 0.32]  (entre 0 et 1, somme = 1)\n",
    "```\n",
    "\n",
    "**Propriétés utiles** :\n",
    "- Toutes les valeurs sont positives et somment à 1 → interprétables comme \"pourcentage d'attention\"\n",
    "- Amplifie les différences : le score le plus élevé \"gagne\" plus de poids\n",
    "\n",
    "**La normalisation (÷√d_k)** évite un problème quand la dimension est grande :\n",
    "\n",
    "```\n",
    "Sans normalisation (d_k = 512) :\n",
    "  Scores Q·K → valeurs entre -50 et +50\n",
    "  Softmax → [0.0001, 0.9998, 0.0001]  ← trop \"peaked\" !\n",
    "  \n",
    "Avec normalisation (÷√512 ≈ 22.6) :\n",
    "  Scores → valeurs entre -2 et +2\n",
    "  Softmax → [0.20, 0.45, 0.35]  ← distribution plus douce\n",
    "```\n",
    "\n",
    "Une distribution trop \"peaked\" pose problème : gradients très faibles → apprentissage difficile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1 : Calcul manuel des scores\n",
    "\n",
    "Commençons par calculer les scores d'attention manuellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple simple avec 3 mots et dimension 4\n",
    "seq_len = 3\n",
    "d_k = 4\n",
    "\n",
    "# Créons des Query, Key, Value aléatoires\n",
    "Q = torch.randn(seq_len, d_k)\n",
    "K = torch.randn(seq_len, d_k)\n",
    "V = torch.randn(seq_len, d_k)\n",
    "\n",
    "print(\"Q (Queries):\")\n",
    "print(Q)\n",
    "print(f\"\\nShape Q: {Q.shape}\")\n",
    "print(f\"Shape K: {K.shape}\")\n",
    "print(f\"Shape V: {V.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Calculez les scores d'attention\n",
    "# ============================================\n",
    "\n",
    "# Étape 1 : Calculer QK^T (produit matriciel)\n",
    "# La transposée de K se note K.T\n",
    "\n",
    "scores = None  # TODO: Calculer QK^T\n",
    "\n",
    "print(\"Scores (QK^T):\")\n",
    "print(scores)\n",
    "print(f\"Shape: {scores.shape}\")  # Devrait être (3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 2 : Appliquez le scaling\n",
    "# ============================================\n",
    "\n",
    "# Diviser par la racine de la dimension des vecteurs pour éviter des valeurs trop grandes\n",
    "\n",
    "import math\n",
    "\n",
    "scaled_scores = None  # TODO: scores / sqrt(d_k)\n",
    "\n",
    "print(\"Scaled scores:\")\n",
    "print(scaled_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 3 : Appliquez le softmax\n",
    "# ============================================\n",
    "\n",
    "# Le softmax transforme les scores en probabilités\n",
    "# Chaque ligne doit sommer à 1\n",
    "# Indice : F.softmax(tensor, dim=i) applique softmax sur la dimension i\n",
    "\n",
    "attention_weights = None  # TODO: Appliquer softmax sur scaled_scores\n",
    "\n",
    "print(\"Poids d'attention (après softmax):\")\n",
    "print(attention_weights)\n",
    "print(f\"\\nVérification - Somme par ligne: {attention_weights.sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 4 : Calculez la sortie finale\n",
    "# ============================================\n",
    "\n",
    "# Multiplier les poids d'attention par V\n",
    "# C'est une moyenne pondérée des values\n",
    "\n",
    "output = None  # TODO: attention_weights @ V\n",
    "\n",
    "print(\"Output:\")\n",
    "print(output)\n",
    "print(f\"Shape: {output.shape}\")  # Devrait être (3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implémentation complète\n",
    "\n",
    "### Exercice 5 : Fonction d'attention\n",
    "\n",
    "Maintenant, regroupez tout dans une fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Calcule le Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries, shape (seq_len, d_k) ou (batch, seq_len, d_k)\n",
    "        K: Keys, shape (seq_len, d_k) ou (batch, seq_len, d_k)\n",
    "        V: Values, shape (seq_len, d_v) ou (batch, seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: Résultat de l'attention, shape (seq_len, d_v)\n",
    "        attention_weights: Poids d'attention, shape (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # TODO: Récupérer d_k (dernière dimension de K)\n",
    "    d_k=None\n",
    "    # TODO: Implémenter les 4 étapes\n",
    "    # 1. Calculer les scores : QK^T\n",
    "    scores = None\n",
    "    \n",
    "    # 2. Scaling : diviser par sqrt(d_k)\n",
    "    scaled_scores = None\n",
    "    \n",
    "    # 3. Softmax pour obtenir les poids\n",
    "    attention_weights = None\n",
    "    \n",
    "    # 4. Moyenne pondérée : weights @ V\n",
    "    output = None\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre fonction\n",
    "Q_test = torch.randn(4, 8)  # 4 tokens, dimension 8\n",
    "K_test = torch.randn(4, 8)\n",
    "V_test = torch.randn(4, 8)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q_test, K_test, V_test)\n",
    "\n",
    "print(f\"Structure de sortie: {output.shape}\")  # Attendu: (4, 8)\n",
    "print(f\"Structure des poids: {weights.shape}\")  # Attendu: (4, 4)\n",
    "print(f\"Somme des poids par ligne: {weights.sum(dim=1)}\")  # Attendu: [1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Pourquoi diviser par sqrt(d_k) ?\n",
    "\n",
    "C'est une question importante ! Voyons l'effet du scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison avec et sans scaling\n",
    "d_k_grand = 512  # Dimension typique dans un Transformer\n",
    "\n",
    "Q_grand = torch.randn(10, d_k_grand)\n",
    "K_grand = torch.randn(10, d_k_grand)\n",
    "\n",
    "# Scores sans scaling\n",
    "scores_sans_scaling = Q_grand @ K_grand.T\n",
    "attention_sans_scaling = F.softmax(scores_sans_scaling, dim=-1)\n",
    "\n",
    "# Scores avec scaling\n",
    "scores_avec_scaling = (Q_grand @ K_grand.T) / math.sqrt(d_k_grand)\n",
    "attention_avec_scaling = F.softmax(scores_avec_scaling, dim=-1)\n",
    "\n",
    " # Fonction pour calculer l'entropie (avec epsilon pour éviter log(0))\n",
    "def entropy(p, eps=1e-9):\n",
    "  p_safe = p.clamp(min=eps)\n",
    "  return -(p * p_safe.log()).sum(dim=-1).mean()\n",
    "\n",
    "\n",
    "print(\"=== SANS SCALING ===\")\n",
    "print(f\"Scores - min: {scores_sans_scaling.min():.2f}, max: {scores_sans_scaling.max():.2f}\")\n",
    "print(f\"Attention max par ligne: {attention_sans_scaling.max(dim=-1).values}\")\n",
    "print(f\"Entropie moyenne: {entropy(attention_sans_scaling):.4f}\")\n",
    "\n",
    "print(\"\\n=== AVEC SCALING ===\")\n",
    "print(f\"Scores - min: {scores_avec_scaling.min():.2f}, max: {scores_avec_scaling.max():.2f}\")\n",
    "print(f\"Attention max par ligne: {attention_avec_scaling.max(dim=-1).values}\")\n",
    "print(f\"Entropie moyenne: {entropy(attention_avec_scaling):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Sans scaling, le softmax devient très \"peaked\" (une valeur proche de 1, les autres proches de 0). Le scaling permet une distribution plus douce et des gradients plus stables.\n",
    "\n",
    "**Comment lire l'entropie ?**\n",
    "- **Entropie haute** (~2.3 pour 10 tokens) → attention répartie sur plusieurs mots\n",
    "- **Entropie basse** (~0) → attention concentrée sur un seul mot\n",
    "\n",
    "**Nuance importante** : Une attention concentrée n'est pas toujours mauvaise ! Par exemple, dans *\"Le chat dort, il ronfle\"*, le mot \"il\" DOIT regarder \"chat\" à 95%.\n",
    "\n",
    "Le problème c'est quand l'attention est peaked **par défaut** (artefact numérique du softmax saturé) plutôt que **par apprentissage**. Le scaling permet au modèle de **choisir** entre attention concentrée ou distribuée selon ce qui est pertinent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Module nn.Module : Self-Attention\n",
    "\n",
    "### Self-Attention vs Cross-Attention\n",
    "\n",
    "Jusqu'ici, on a manipulé Q, K, V comme des tenseurs indépendants. Mais d'où viennent-ils ?\n",
    "\n",
    "**Self-Attention** (ce qu'on fait ici) :\n",
    "- Q, K, V sont tous calculés à partir du **même** input `x`\n",
    "- Chaque mot de la phrase regarde les autres mots **de la même phrase**\n",
    "- C'est le cas dans BERT, GPT, et la plupart des Transformers\n",
    "\n",
    "```\n",
    "x (embeddings) ──┬──► W_q ──► Q\n",
    "                 ├──► W_k ──► K    (même source x)\n",
    "                 └──► W_v ──► V\n",
    "```\n",
    "\n",
    "**Cross-Attention** (on verra dans les projets) :\n",
    "- Q vient d'une source, K et V d'une **autre** source\n",
    "- Exemple : en traduction, le décodeur (français) \"interroge\" l'encodeur (anglais)\n",
    "- Utilisé dans les architectures encodeur-décodeur\n",
    "\n",
    "```\n",
    "x_decoder ──► W_q ──► Q\n",
    "x_encoder ──┬──► W_k ──► K    (sources différentes)\n",
    "            └──► W_v ──► V\n",
    "```\n",
    "\n",
    "> **Dans ce TP**, on implémente la **self-attention** : la séquence \"s'attentionne elle-même\".\n",
    "\n",
    "### D'où viennent Q, K, V ?\n",
    "\n",
    "Dans les exercices 1-5, on a utilisé des tenseurs aléatoires (`torch.randn`) pour comprendre le mécanisme d'attention. Mais en pratique, **Q, K, V sont calculés à partir des embeddings de la phrase**.\n",
    "\n",
    "**Le point clé** : Un même mot a besoin de **3 représentations différentes** selon son rôle :\n",
    "\n",
    "| Rôle | Représentation | Question posée |\n",
    "|------|----------------|----------------|\n",
    "| **Query** | `Q = x @ W_q` | \"Qu'est-ce que je cherche ?\" |\n",
    "| **Key** | `K = x @ W_k` | \"Comment les autres me voient ?\" |\n",
    "| **Value** | `V = x @ W_v` | \"Quelle info je transmets ?\" |\n",
    "\n",
    "**Exemple concret** :\n",
    "\n",
    "```\n",
    "Phrase : \"Le chat dort\"\n",
    "\n",
    "x = embeddings de la phrase (3 mots × embed_dim)\n",
    "\n",
    "Q = x @ W_q  →  chaque mot \"formule sa question\"\n",
    "K = x @ W_k  →  chaque mot \"affiche son identité\"\n",
    "V = x @ W_v  →  chaque mot \"prépare son contenu à transmettre\"\n",
    "```\n",
    "\n",
    "**Pourquoi 3 matrices différentes ?**\n",
    "\n",
    "Si on faisait simplement `Q = K = V = x`, le modèle serait limité. Les matrices W_q, W_k, W_v sont **apprises** pendant l'entraînement : le modèle découvre quelles \"facettes\" de chaque mot sont utiles pour chaque rôle.\n",
    "\n",
    "> **C'est ce qu'on implémente dans l'exercice 6** : une classe qui projette `x` vers Q, K, V, puis applique l'attention.\n",
    "\n",
    "### Exercice 6 : Classe SelfAttention en PyTorch\n",
    "\n",
    "Créons une classe PyTorch réutilisable qui :\n",
    "1. Projette l'input `x` vers Q, K, V avec des matrices apprenables\n",
    "2. Applique la fonction `scaled_dot_product_attention` de l'exercice 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Module de Self-Attention.\n",
    "    \n",
    "    Projette l'input x vers Q, K, V puis applique l'attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: Dimension des embeddings d'entrée\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # TODO: Créer 3 couches linéaires pour projeter vers Q, K, V\n",
    "        # Chaque couche : embed_dim -> embed_dim (utiliser nn.Linear)\n",
    "        self.W_q = None\n",
    "        self.W_k = None\n",
    "        self.W_v = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Embeddings, shape (batch, seq_len, embed_dim)\n",
    "\n",
    "        Returns:\n",
    "            output: Résultat de l'attention\n",
    "            attention_weights: Poids d'attention\n",
    "        \"\"\"\n",
    "        # TODO: Projeter x vers Q, K, V en utilisant les couches linéaires\n",
    "        Q = None\n",
    "        K = None\n",
    "        V = None\n",
    "\n",
    "        # TODO: Réutiliser la fonction scaled_dot_product_attention de l'exercice 5\n",
    "        # (elle fonctionne aussi avec des tenseurs 3D grâce à .transpose(-2, -1))\n",
    "        output, attention_weights = None, None\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du module\n",
    "embed_dim = 32\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "attention_layer = SelfAttention(embed_dim)\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "output, weights = attention_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")  # Attendu: (2, 5, 32)\n",
    "print(f\"Weights shape: {weights.shape}\")  # Attendu: (2, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Visualiser l'attention d'un vrai modèle\n",
    "\n",
    "Maintenant qu'on a compris et implémenté le mécanisme, regardons ce que ça donne sur un modèle **réellement entraîné**.\n",
    "\n",
    "### Tokens spéciaux : [CLS] et [SEP]\n",
    "\n",
    "Les modèles BERT ajoutent des tokens spéciaux :\n",
    "\n",
    "| Token | Rôle |\n",
    "|-------|------|\n",
    "| **[CLS]** | Début de phrase. Son vecteur représente toute la phrase. |\n",
    "| **[SEP]** | Fin de phrase / séparateur. |\n",
    "\n",
    "Exemple : `\"The cat sat\"` → `[CLS] The cat sat [SEP]`\n",
    "\n",
    "> **Note** : [CLS] reçoit souvent beaucoup d'attention, c'est normal !\n",
    "\n",
    "### Aperçu : Multi-Head\n",
    "\n",
    "DistilBERT utilise **12 têtes d'attention par couche**. Chaque tête capture des relations différentes (syntaxe, coréférence, proximité...).\n",
    "\n",
    "> **On étudiera le Multi-Head en détail dans le TP 03.** Ici, on visualise une tête qui capture bien la coréférence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de la librairie transformers\n",
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Charger un petit modèle pré-entraîné\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "model.eval()\n",
    "\n",
    "# Phrase de test (en anglais pour ce modèle)\n",
    "phrase = \"The cat sat on the mat because it was tired\"\n",
    "\n",
    "# Tokenizer la phrase\n",
    "inputs = tokenizer(phrase, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Forward pass (sans calculer les gradients)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extraire les attentions\n",
    "attentions = outputs.attentions\n",
    "\n",
    "print(f\"Phrase: {phrase}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Nombre de couches: {len(attentions)}\")\n",
    "print(f\"Nombre de têtes par couche: {attentions[0].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'attention d'une tête spécifique\n",
    "# Couche 5, Tête 2 : capture bien la coréférence \"it\" → \"cat\"\n",
    "layer = 4   # Couche 5 (index 0-5)\n",
    "head = 1    # Tête 2 (index 0-11)\n",
    "\n",
    "attention_matrix = attentions[layer][0, head].numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_matrix, cmap='Blues')\n",
    "plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "plt.yticks(range(len(tokens)), tokens)\n",
    "plt.xlabel(\"Tokens regardés (Keys)\")\n",
    "plt.ylabel(\"Tokens qui regardent (Queries)\")\n",
    "plt.title(f\"Attention réelle - Couche {layer+1}, Tête {head+1}\")\n",
    "plt.colorbar(label=\"Poids d'attention\")\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens)):\n",
    "        val = attention_matrix[i, j]\n",
    "        plt.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                color='white' if val > 0.3 else 'black', fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Que regarde le pronom \"it\" ?\n",
    "it_index = tokens.index(\"it\")\n",
    "\n",
    "print(f\"Attention de 'it' (Couche {layer+1}, Tête {head+1}) :\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for token, weight in zip(tokens, attention_matrix[it_index]):\n",
    "    bar = \"█\" * int(weight * 30)\n",
    "    highlight = \" ← antécédent !\" if token == \"cat\" else \"\"\n",
    "    print(f\"  {token:10} {weight:.2f} {bar}{highlight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations :**\n",
    "\n",
    "1. Le pronom \"it\" regarde principalement \"cat\" → le modèle a appris la **coréférence** !\n",
    "\n",
    "2. Pourquoi pas \"mat\" ? Sémantiquement, \"it was tired\" fait référence à un être vivant (le chat), pas au tapis.\n",
    "\n",
    "**Question :**\n",
    "\n",
    "Essayez avec `\"The trophy didn't fit in the suitcase because it was too big\"`. \n",
    "\n",
    "Qui est \"it\" ? Le trophée (trop gros pour rentrer) ou la valise (trop petite) ? C'est un cas **ambigu** !\n",
    "\n",
    "> **Note sur l'interprétabilité** : Les poids d'attention donnent une **intuition** sur ce que le modèle \"regarde\", mais l'interprétation formelle du raisonnement des Transformers reste un **problème ouvert en recherche**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Récapitulatif\n",
    "\n",
    "### Ce que nous avons appris\n",
    "\n",
    "1. **L'attention** permet à chaque élément de \"regarder\" tous les autres\n",
    "2. **Q, K, V** : Query (ce que je cherche), Key (les étiquettes), Value (le contenu)\n",
    "3. **Formule** : $\\text{softmax}(QK^T / \\sqrt{d_k}) \\cdot V$\n",
    "4. **Scaling** : Essentiel pour la stabilité des gradients\n",
    "\n",
    "### Points clés\n",
    "\n",
    "| Concept | Rôle |\n",
    "|---------|------|\n",
    "| Dot product $QK^T$ | Mesure la similarité |\n",
    "| Softmax | Transforme en probabilités |\n",
    "| Scaling $\\sqrt{d_k}$ | Stabilise les gradients |\n",
    "| Self-attention | Q, K, V viennent de la même source |\n",
    "\n",
    "### Prochaine session\n",
    "\n",
    "Nous verrons le **Multi-Head Attention** : plusieurs \"têtes\" d'attention qui regardent sous différents angles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Pour aller plus loin (optionnel)\n",
    "\n",
    "### Comment entraîne-t-on un Transformer ?\n",
    "\n",
    "Il existe deux grandes approches selon l'usage du modèle :\n",
    "\n",
    "### Approche 1 : Prédire le mot suivant (GPT)\n",
    "\n",
    "Pour les modèles **génératifs** (GPT, LLaMA, etc.), on entraîne le modèle à prédire le prochain mot.\n",
    "\n",
    "**Objectif** : Entraîner efficacement sur des phrases entières en un seul forward pass.\n",
    "\n",
    "```\n",
    "Phrase : \"Le chat dort sur\"\n",
    "\n",
    "Sans masque (inefficace) :\n",
    "  Forward 1 : \"Le\"           → apprend à prédire \"chat\"\n",
    "  Forward 2 : \"Le chat\"      → apprend à prédire \"dort\"\n",
    "  Forward 3 : \"Le chat dort\" → apprend à prédire \"sur\"\n",
    "  → 3 forward passes pour une phrase !\n",
    "\n",
    "Avec masque causal (efficace) :\n",
    "  Forward unique : \"Le chat dort sur\"\n",
    "    Position 1 (voit \"Le\")           → apprend à prédire \"chat\"\n",
    "    Position 2 (voit \"Le chat\")      → apprend à prédire \"dort\"\n",
    "    Position 3 (voit \"Le chat dort\") → apprend à prédire \"sur\"\n",
    "  → 1 seul forward pass, tout en parallèle !\n",
    "```\n",
    "\n",
    "**Le masque causal** permet à chaque position de ne voir que les mots précédents :\n",
    "\n",
    "```\n",
    "              Le   chat  dort  sur\n",
    "      Le    [  ✓     ✗     ✗    ✗  ]\n",
    "     chat   [  ✓     ✓     ✗    ✗  ]\n",
    "     dort   [  ✓     ✓     ✓    ✗  ]\n",
    "      sur   [  ✓     ✓     ✓    ✓  ]\n",
    "```\n",
    "\n",
    "**Implémentation** : On met `-∞` aux positions masquées → `softmax(-∞) = 0`\n",
    "\n",
    "### Approche 2 : Remplir les trous (BERT)\n",
    "\n",
    "Pour les modèles de **compréhension** (BERT, RoBERTa, etc.) :\n",
    "\n",
    "```\n",
    "Entrée :    \"Le [MASK] dort sur le [MASK]\"\n",
    "Objectif :   Prédire \"chat\" et \"canapé\"\n",
    "```\n",
    "\n",
    "Le modèle peut voir tout le contexte (gauche ET droite) pour deviner les mots masqués → pas besoin de masque causal.\n",
    "\n",
    "### Comparaison\n",
    "\n",
    "| | GPT (génératif) | BERT (compréhension) |\n",
    "|--|-----------------|---------------------|\n",
    "| **Entraînement** | Prédire le mot suivant | Prédire les mots masqués |\n",
    "| **Contexte** | Passé uniquement | Tout (bidirectionnel) |\n",
    "| **Masque causal** | Oui | Non |\n",
    "| **Usage** | Génération de texte | Classification, QA, NER |\n",
    "\n",
    "### Exercice bonus : Implémenter le masque causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un masque causal (triangulaire inférieur)\n",
    "seq_len = 4\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "\n",
    "print(\"Masque causal (True = position masquée) :\")\n",
    "print(causal_mask.int())\n",
    "print(\"\\nVisuellement : chaque ligne ne peut voir que les positions ≤ à elle-même\")\n",
    "\n",
    "# Fonction d'attention avec masque\n",
    "def scaled_dot_product_attention_with_mask(Q, K, V, mask=None):\n",
    "    \"\"\"Attention avec masque optionnel.\"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        # Mettre -inf aux positions masquées → softmax donnera 0\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = attention_weights @ V\n",
    "    return output, attention_weights\n",
    "\n",
    "# Test avec masque\n",
    "Q = torch.randn(seq_len, 8)\n",
    "K = torch.randn(seq_len, 8)\n",
    "V = torch.randn(seq_len, 8)\n",
    "\n",
    "output_masked, weights_masked = scaled_dot_product_attention_with_mask(Q, K, V, causal_mask)\n",
    "\n",
    "print(\"\\nPoids d'attention avec masque causal:\")\n",
    "print(weights_masked.round(decimals=2))\n",
    "print(\"\\nObservation: chaque ligne ne peut voir que les positions précédentes (et elle-même)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
