{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 02 - Le Mécanisme d'Attention (V2)\n",
    "\n",
    "**Module** : Réseaux de Neurones Approfondissement  \n",
    "**Durée** : 2h  \n",
    "**Objectif** : Comprendre le Positional Encoding et les bases de l'attention\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs pédagogiques\n",
    "\n",
    "À la fin de cette session, vous serez capable de :\n",
    "1. Expliquer pourquoi le **Positional Encoding** est nécessaire\n",
    "2. Calculer le PE avec la formule sin/cos\n",
    "3. Comprendre la relation entre **similarité** et **produit scalaire**\n",
    "4. Calculer les **scores d'attention** étape par étape\n",
    "\n",
    "---\n",
    "\n",
    "**Note** : Ce TP pose les fondations. Le TP suivant implémentera l'attention complète."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances (Google Colab)\n",
    "!pip install torch matplotlib numpy transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Configuration\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Rappel du TP1 - Chargement CamemBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récuperation d'un embedding sur un modèle déjà entrainé (BERT)\n",
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "\n",
    "print(\"Chargement de CamemBERT (modèle français)...\")\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model_camembert = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# On utilise UNIQUEMENT la couche d'embeddings\n",
    "embedding_layer = model_camembert.embeddings.word_embeddings\n",
    "\n",
    "print(\"✅ CamemBERT chargé !\")\n",
    "print(f\"   Dimension des embeddings : {embedding_layer.embedding_dim}\")\n",
    "\n",
    "def get_french_embeddings(phrase, target_dim=100):\n",
    "    \"\"\"\n",
    "    Extrait les embeddings d'une phrase française.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        phrase,\n",
    "        is_split_into_words=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings_768 = embedding_layer(inputs[\"input_ids\"][0])\n",
    "\n",
    "    if not hasattr(get_french_embeddings, 'projection'):\n",
    "        torch.manual_seed(42)\n",
    "        get_french_embeddings.projection = torch.randn(768, target_dim) / 30\n",
    "\n",
    "    embeddings = embeddings_768 @ get_french_embeddings.projection\n",
    "    return embeddings, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice de rappel : similarité\n",
    "mots = [\"Paris\", \"tour\", \"Eiffel\"]\n",
    "embeddings, tokens = get_french_embeddings(mots)\n",
    "\n",
    "emb_paris = embeddings[tokens.index(\"▁Paris\")]\n",
    "emb_tour = embeddings[tokens.index(\"▁tour\")]\n",
    "emb_eiffel = embeddings[tokens.index(\"▁Eiffel\")]\n",
    "\n",
    "emb_tour_eiffel = (emb_tour + emb_eiffel) / 2\n",
    "\n",
    "sim_paris_tour_eiffel = F.cosine_similarity(\n",
    "    emb_paris.unsqueeze(0),\n",
    "    emb_tour_eiffel.unsqueeze(0)\n",
    ")\n",
    "\n",
    "sim_paris_tour = F.cosine_similarity(\n",
    "    emb_paris.unsqueeze(0),\n",
    "    emb_tour.unsqueeze(0)\n",
    ")\n",
    "\n",
    "print(f\"Similarité Paris / tour Eiffel : {sim_paris_tour_eiffel.item():.4f}\")\n",
    "print(f\"Similarité Paris / tour : {sim_paris_tour.item():.4f}\")\n",
    "print(\"\\n→ Paris est plus proche de 'tour Eiffel' que de 'tour' seul\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Le problème : l'ordre des mots\n",
    "\n",
    "Les embeddings seuls ne capturent pas la **position** des mots dans la phrase.\n",
    "\n",
    "```\n",
    "\"Le chat mange la souris\"  ≠  \"La souris mange le chat\"\n",
    "```\n",
    "\n",
    "Pourtant, les mêmes mots ont les mêmes embeddings !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Positional Encoding\n",
    "\n",
    "### La formule\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "- **pos** : position dans la séquence (0, 1, 2, ...)\n",
    "- **i** : indice de la dimension\n",
    "- **d_model** : dimension totale des embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Génère le positional encoding avec la formule sin/cos.\n",
    "    \"\"\"\n",
    "    position = torch.arange(seq_len).unsqueeze(1)\n",
    "    div_term = torch.exp(\n",
    "        torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "    )\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1 : Pourquoi le PE est nécessaire ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Exercice 1 : Pourquoi le Positional Encoding ? ===\")\n",
    "\n",
    "phrase_a = [\"Pikachu\", \"attaque\", \"Dracaufeu\"]\n",
    "phrase_b = [\"Dracaufeu\", \"attaque\", \"Pikachu\"]\n",
    "\n",
    "def get_same_embeddings(tokens):\n",
    "    \"\"\"Retourne embeddings identiques pour tokens identiques.\"\"\"\n",
    "    vocab = {\"Pikachu\": 0, \"attaque\": 1, \"Dracaufeu\": 2}\n",
    "    torch.manual_seed(42)\n",
    "    base = torch.randn(3, 100)\n",
    "    return torch.stack([base[vocab[t]] for t in tokens])\n",
    "\n",
    "emb_a = get_same_embeddings(phrase_a)\n",
    "emb_b = get_same_embeddings(phrase_b)\n",
    "\n",
    "print(f\"Phrase A : {phrase_a}\")\n",
    "print(f\"Phrase B : {phrase_b}\")\n",
    "\n",
    "# Calculer la différence\n",
    "diff_sans_pe = torch.norm(emb_a - emb_b)\n",
    "print(f\"\\nDifférence SANS PE : {diff_sans_pe:.4f}\")\n",
    "\n",
    "# Pourquoi pas identiques ?\n",
    "print(\"\\n→ La différence n'est PAS nulle car les embeddings sont dans un ordre différent.\")\n",
    "print(\"   Mais la SOMME des embeddings serait identique !\")\n",
    "print(f\"   Somme A : {emb_a.sum():.4f}\")\n",
    "print(f\"   Somme B : {emb_b.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : Calculer le PE manuellement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Exercice 2 : Calculer le PE manuellement ===\")\n",
    "\n",
    "print(\"\\nCalculons le PE pour d_model=4, seq_len=3\")\n",
    "print(\"\\nFormule :\")\n",
    "print(\"  PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\")\n",
    "print(\"  PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\\n\")\n",
    "\n",
    "# PE(0, 0) : pos=0, i=0 (dimension paire) → sin(0 / 10000^0) = sin(0) = 0\n",
    "pe_0_0 = math.sin(0 / (10000 ** (0/4)))\n",
    "print(f\"PE(0, 0) = sin(0 / 10000^0) = sin(0) = {pe_0_0:.4f}\")\n",
    "\n",
    "# PE(0, 1) : pos=0, i=0 (dimension impaire) → cos(0 / 10000^0) = cos(0) = 1\n",
    "pe_0_1 = math.cos(0 / (10000 ** (0/4)))\n",
    "print(f\"PE(0, 1) = cos(0 / 10000^0) = cos(0) = {pe_0_1:.4f}\")\n",
    "\n",
    "# PE(1, 0) : pos=1, i=0 (dimension paire) → sin(1 / 10000^0) = sin(1)\n",
    "pe_1_0 = math.sin(1 / (10000 ** (0/4)))\n",
    "print(f\"PE(1, 0) = sin(1 / 10000^0) = sin(1) = {pe_1_0:.4f}\")\n",
    "\n",
    "# Vérification avec notre fonction\n",
    "pe_verif = get_positional_encoding(2, 4)\n",
    "print(f\"\\nVérification avec get_positional_encoding :\")\n",
    "print(f\"  PE[0, 0] = {pe_verif[0, 0]:.4f}\")\n",
    "print(f\"  PE[0, 1] = {pe_verif[0, 1]:.4f}\")\n",
    "print(f\"  PE[1, 0] = {pe_verif[1, 0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3 : Propriétés du PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Exercice 3 : Vérifier les propriétés du PE ===\")\n",
    "\n",
    "seq_len_test = 10\n",
    "pe_test = get_positional_encoding(seq_len_test, 100)\n",
    "\n",
    "# Propriété 1 : Valeurs ∈ [-1, 1]\n",
    "min_val = pe_test.min().item()\n",
    "max_val = pe_test.max().item()\n",
    "print(f\"\\nMin : {min_val:.4f}\")\n",
    "print(f\"Max : {max_val:.4f}\")\n",
    "print(f\"→ Toutes les valeurs sont bien dans [-1, 1] ✓\")\n",
    "\n",
    "# Propriété 2 : Différence entre positions consécutives\n",
    "diff_pos_0_1 = torch.norm(pe_test[0] - pe_test[1]).item()\n",
    "diff_pos_4_5 = torch.norm(pe_test[4] - pe_test[5]).item()\n",
    "print(f\"\\nDistance pos 0 → 1 : {diff_pos_0_1:.4f}\")\n",
    "print(f\"Distance pos 4 → 5 : {diff_pos_4_5:.4f}\")\n",
    "print(f\"→ Les distances sont similaires (positions relatives)\")\n",
    "\n",
    "# Propriété 3 : Dimensions paires et impaires\n",
    "dim_0_all_pos = pe_test[:, 0]  # sin de toutes les positions\n",
    "dim_1_all_pos = pe_test[:, 1]  # cos de toutes les positions\n",
    "print(f\"\\nDimension 0 (sin) pour toutes les positions :\")\n",
    "print(f\"  {[f'{x:.3f}' for x in dim_0_all_pos.tolist()[:5]]}...\")\n",
    "print(f\"Dimension 1 (cos) pour toutes les positions :\")\n",
    "print(f\"  {[f'{x:.3f}' for x in dim_1_all_pos.tolist()[:5]]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4 : Impact du PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Exercice 4 : Démonstration de l'impact du PE ===\")\n",
    "\n",
    "phrase_a = [\"Le\", \"chat\", \"mange\", \"la\", \"souris\"]\n",
    "phrase_b = [\"La\", \"souris\", \"mange\", \"le\", \"chat\"]\n",
    "\n",
    "# Vocabulaire simplifié\n",
    "vocab = {\"Le\": 0, \"chat\": 1, \"mange\": 2, \"la\": 3, \"souris\": 4, \"La\": 3, \"le\": 0}\n",
    "\n",
    "def get_embeddings_simple(tokens, vocab):\n",
    "    torch.manual_seed(42)\n",
    "    base = torch.randn(5, 100)\n",
    "    return torch.stack([base[vocab[t]] for t in tokens])\n",
    "\n",
    "emb_a = get_embeddings_simple(phrase_a, vocab)\n",
    "emb_b = get_embeddings_simple(phrase_b, vocab)\n",
    "\n",
    "# Positional encoding\n",
    "pe = get_positional_encoding(5, 100)\n",
    "\n",
    "# Embeddings + PE\n",
    "emb_a_pe = emb_a + pe\n",
    "emb_b_pe = emb_b + pe\n",
    "\n",
    "# Distances SANS PE\n",
    "print(\"\\nDistances SANS Positional Encoding :\")\n",
    "for i, (wa, wb) in enumerate(zip(phrase_a, phrase_b)):\n",
    "    dist = torch.norm(emb_a[i] - emb_b[i]).item()\n",
    "    print(f\"  Position {i}: '{wa}' vs '{wb}' → {dist:.4f}\")\n",
    "\n",
    "print(\"\\nDistances AVEC Positional Encoding :\")\n",
    "for i, (wa, wb) in enumerate(zip(phrase_a, phrase_b)):\n",
    "    dist = torch.norm(emb_a_pe[i] - emb_b_pe[i]).item()\n",
    "    print(f\"  Position {i}: '{wa}' vs '{wb}' → {dist:.4f}\")\n",
    "\n",
    "print(\"\\n→ Avec PE, même les mots identiques à la même position diffèrent\")\n",
    "print(\"  car le contexte global (ordre des mots) est différent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation du Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation du PE\n",
    "pe_visu = get_positional_encoding(50, 64)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pe_visu.T, cmap='RdBu', aspect='auto')\n",
    "plt.xlabel('Position dans la séquence')\n",
    "plt.ylabel('Dimension')\n",
    "plt.title('Positional Encoding (sin/cos)')\n",
    "plt.colorbar(label='Valeur')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation :\")\n",
    "print(\"- Les basses fréquences (dimensions hautes) varient lentement\")\n",
    "print(\"- Les hautes fréquences (dimensions basses) varient rapidement\")\n",
    "print(\"- Chaque position a un pattern unique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Similarité et Produit Scalaire\n",
    "\n",
    "Avant d'aborder l'attention, comprenons le lien entre **similarité cosinus** et **produit scalaire**.\n",
    "\n",
    "### Formules\n",
    "\n",
    "- **Produit scalaire** : $\\vec{a} \\cdot \\vec{b} = \\sum_i a_i b_i$\n",
    "\n",
    "- **Similarité cosinus** : $\\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{||\\vec{a}|| \\cdot ||\\vec{b}||}$\n",
    "\n",
    "Le produit scalaire mesure la \"compatibilité\" entre deux vecteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison similarité cosinus vs produit scalaire\n",
    "print(\"=\" * 60)\n",
    "print(\"SIMILARITÉ COSINUS vs PRODUIT SCALAIRE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "v1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "v2 = torch.tensor([2.0, 3.0, 4.0])\n",
    "\n",
    "dot_product = torch.dot(v1, v2)\n",
    "cos_sim = F.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0))\n",
    "cos_sim_manual = dot_product / (torch.norm(v1) * torch.norm(v2))\n",
    "\n",
    "print(f\"\\nVecteur 1 : {v1}\")\n",
    "print(f\"Vecteur 2 : {v2}\")\n",
    "print(f\"\\nProduit scalaire : {dot_product:.4f}\")\n",
    "print(f\"Similarité cosinus : {cos_sim.item():.4f}\")\n",
    "print(f\"Similarité cosinus (manuel) : {cos_sim_manual:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Le produit scalaire Q·K mesure la 'compatibilité' entre tokens\")\n",
    "print(\"   Le softmax transforme ces scores en probabilités d'attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Introduction à l'Attention\n",
    "\n",
    "### La formule de l'attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "| Composant | Rôle |\n",
    "|-----------|------|\n",
    "| **Q** (Query) | Ce que je cherche |\n",
    "| **K** (Key) | Ce que j'offre pour être trouvé |\n",
    "| **V** (Value) | L'information que je transmets |\n",
    "\n",
    "### Analogie : Bibliothèque\n",
    "\n",
    "- **Query** = Ta question (\"Je cherche un livre sur Python\")\n",
    "- **Key** = Les étiquettes des livres (\"Python\", \"Java\", \"Cuisine\"...)\n",
    "- **Value** = Le contenu des livres\n",
    "\n",
    "L'attention calcule **quels livres sont pertinents** pour ta question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.1 : Calcul des scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Exercice 5.1 : Calcul des scores ===\")\n",
    "\n",
    "# Configuration\n",
    "seq_len = 3  # 3 tokens\n",
    "d_k = 4      # dimension des vecteurs\n",
    "\n",
    "# Créer Q, K, V aléatoires\n",
    "torch.manual_seed(42)\n",
    "Q = torch.randn(seq_len, d_k)\n",
    "K = torch.randn(seq_len, d_k)\n",
    "V = torch.randn(seq_len, d_k)\n",
    "\n",
    "print(\"Q (Queries) - Ce que chaque token cherche :\")\n",
    "print(Q)\n",
    "print(f\"\\nK (Keys) - Comment chaque token se présente :\")\n",
    "print(K)\n",
    "\n",
    "# Calculer les scores : QK^T\n",
    "# Chaque ligne = un token qui \"interroge\" tous les autres\n",
    "scores = Q @ K.T\n",
    "\n",
    "print(f\"\\nScores (QK^T) - Compatibilité entre tokens :\")\n",
    "print(scores)\n",
    "print(f\"\\nShape: {scores.shape}  (3 tokens × 3 tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.2 : Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Exercice 5.2 : Scaling ===\")\n",
    "\n",
    "# Pourquoi diviser par sqrt(d_k) ?\n",
    "# → Éviter que les scores soient trop grands (gradients instables)\n",
    "\n",
    "scaled_scores = scores / math.sqrt(d_k)\n",
    "\n",
    "print(f\"Scores originaux :\")\n",
    "print(scores)\n",
    "print(f\"\\nScores après scaling (÷√{d_k} = ÷{math.sqrt(d_k):.2f}) :\")\n",
    "print(scaled_scores)\n",
    "\n",
    "print(f\"\\n→ Les valeurs sont réduites, le softmax sera plus 'doux'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.3 : Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Exercice 5.3 : Softmax ===\")\n",
    "\n",
    "# Softmax transforme les scores en probabilités\n",
    "# Chaque ligne somme à 1\n",
    "\n",
    "attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "\n",
    "print(\"Poids d'attention (après softmax) :\")\n",
    "print(attention_weights)\n",
    "\n",
    "print(f\"\\nVérification - Somme par ligne :\")\n",
    "print(attention_weights.sum(dim=1))\n",
    "print(\"\\n→ Chaque ligne = distribution de probabilités sur les tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.4 : Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Exercice 5.4 : Output ===\")\n",
    "\n",
    "# L'output = moyenne pondérée des Values par les poids d'attention\n",
    "output = attention_weights @ V\n",
    "\n",
    "print(\"V (Values) - L'information de chaque token :\")\n",
    "print(V)\n",
    "\n",
    "print(f\"\\nOutput (weights @ V) :\")\n",
    "print(output)\n",
    "print(f\"\\nShape: {output.shape}  (même que V)\")\n",
    "\n",
    "print(\"\\n→ Chaque token a maintenant une représentation enrichie\")\n",
    "print(\"   qui intègre l'information des tokens 'pertinents'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation de l'attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "tokens_demo = [\"Le\", \"chat\", \"dort\"]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention_weights.detach().numpy(), cmap='Blues')\n",
    "plt.xticks(range(3), tokens_demo)\n",
    "plt.yticks(range(3), tokens_demo)\n",
    "plt.xlabel(\"Tokens regardés (Keys)\")\n",
    "plt.ylabel(\"Tokens qui regardent (Queries)\")\n",
    "plt.title(\"Matrice d'attention\")\n",
    "plt.colorbar(label=\"Poids\")\n",
    "\n",
    "# Ajouter les valeurs\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        val = attention_weights[i, j].item()\n",
    "        plt.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                color='white' if val > 0.5 else 'black', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Récapitulatif\n",
    "\n",
    "### Ce que nous avons appris\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Positional Encoding** | Encode la position avec sin/cos, valeurs ∈ [-1, 1] |\n",
    "| **Q, K, V** | Query = question, Key = étiquette, Value = contenu |\n",
    "| **Scores** | QK^T = compatibilité entre tokens |\n",
    "| **Scaling** | Diviser par √d_k pour stabiliser |\n",
    "| **Softmax** | Transformer en probabilités |\n",
    "| **Output** | Moyenne pondérée des Values |\n",
    "\n",
    "### Formule complète\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### Prochaine session\n",
    "\n",
    "On va :\n",
    "1. Implémenter la **fonction d'attention complète**\n",
    "2. Créer une **classe SelfAttention** en PyTorch\n",
    "3. Visualiser l'attention sur un **vrai modèle** (DistilBERT)\n",
    "4. Découvrir le **Multi-Head Attention**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
