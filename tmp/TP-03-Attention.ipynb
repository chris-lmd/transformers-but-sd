{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3 - Maîtriser l'Attention\n",
    "\n",
    "**Module** : Réseaux de Neurones Approfondissement  \n",
    "**Durée** : 2h  \n",
    "**Objectif** : Implémenter l'attention de A à Z et découvrir le Multi-Head\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs pédagogiques\n",
    "\n",
    "À la fin de cette session, vous serez capable de :\n",
    "1. Implémenter la fonction `scaled_dot_product_attention()` complète\n",
    "2. Créer une classe `SelfAttention` réutilisable en PyTorch\n",
    "3. Visualiser et interpréter l'attention d'un vrai modèle (CamemBERT)\n",
    "4. Comprendre pourquoi on utilise **plusieurs têtes** d'attention\n",
    "\n",
    "---\n",
    "\n",
    "## Rappel : Où en sommes-nous ?\n",
    "\n",
    "Dans la session précédente, vous avez :\n",
    "- Compris le **Positional Encoding** (encoder l'ordre des mots)\n",
    "- Vu le lien entre **similarité** et **produit scalaire**\n",
    "- Découvert les concepts de **Query, Key, Value**\n",
    "   - Q : Ce que je cherche\n",
    "   - K : Les étiquettes\n",
    "   - V : Le contenu\n",
    "- Calculé les scores d'attention étape par étape\n",
    "\n",
    "Aujourd'hui, on passe à l'**implémentation complète** !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Installation des dépendances (Google Colab)\n",
    "!pip install torch matplotlib numpy transformers bertviz -q"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:07:11.812365Z",
     "start_time": "2026-01-26T19:07:10.939478Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Rappel : La formule de l'attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "| Étape | Opération | Rôle |\n",
    "|-------|-----------|------|\n",
    "| 1 | $QK^T$ | Calculer les scores de similarité |\n",
    "| 2 | $\\div \\sqrt{d_k}$ | Stabiliser les gradients |\n",
    "| 3 | softmax | Transformer en probabilités |\n",
    "| 4 | $\\times V$ | Moyenne pondérée des values |\n",
    "\n",
    "Dans la session précédente, vous avez fait ces étapes **séparément**. Maintenant, on les **combine** dans une fonction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Exercice 1 : Fonction d'attention complète\n",
    "\n",
    "Regroupez les 4 étapes dans une seule fonction réutilisable."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:27.567588Z",
     "start_time": "2026-01-26T19:06:27.552024Z"
    }
   },
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Calcule le Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries, shape (..., seq_len, d_k)\n",
    "        K: Keys, shape (..., seq_len, d_k)\n",
    "        V: Values, shape (..., seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: Résultat de l'attention, même shape que V\n",
    "        attention_weights: Poids d'attention, shape (..., seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # CORRECTION 1: Récupérer d_k (dernière dimension de K)\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # CORRECTION 2: Calculer les scores QK^T\n",
    "    scores = Q @ K.transpose(-2, -1)\n",
    "    \n",
    "    # CORRECTION 3: Appliquer le scaling (diviser par sqrt(d_k))\n",
    "    scaled_scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # CORRECTION 4: Appliquer softmax sur la dernière dimension\n",
    "    attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "    \n",
    "    # CORRECTION 5: Calculer la sortie (weights @ V)\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:27.877566Z",
     "start_time": "2026-01-26T19:06:27.775923Z"
    }
   },
   "source": [
    "# Test de votre fonction\n",
    "Q_test = torch.randn(4, 8)  # 4 tokens, dimension 8\n",
    "K_test = torch.randn(4, 8)\n",
    "V_test = torch.randn(4, 8)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q_test, K_test, V_test)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")    # Attendu: (4, 8)\n",
    "print(f\"Weights shape: {weights.shape}\")  # Attendu: (4, 4)\n",
    "print(f\"Somme des poids par ligne: {weights.sum(dim=-1)}\")  # Attendu: [1, 1, 1, 1]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 8])\n",
      "Weights shape: torch.Size([4, 4])\n",
      "Somme des poids par ligne: tensor([1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vérification : Effet du scaling\n",
    "\n",
    "Pourquoi diviser par $\\sqrt{d_k}$ ? Voyons l'effet."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:28.136213Z",
     "start_time": "2026-01-26T19:06:28.085878Z"
    }
   },
   "source": "# Comparaison avec et sans scaling\nd_k_grand = 512  # Dimension typique dans un Transformer\n\nQ_grand = torch.randn(10, d_k_grand)\nK_grand = torch.randn(10, d_k_grand)\n\n# Sans scaling\nscores_sans = Q_grand @ K_grand.T\npoids_sans = F.softmax(scores_sans, dim=-1)\n\n# Avec scaling\nscores_avec = (Q_grand @ K_grand.T) / math.sqrt(d_k_grand)\npoids_avec = F.softmax(scores_avec, dim=-1)\n\nprint(\"=== SANS SCALING ===\")\nprint(f\"Scores - min: {scores_sans.min():.1f}, max: {scores_sans.max():.1f}\")\nprint(f\"Poids d'attention max par ligne: {poids_sans.max(dim=-1).values[:5]}\")\n\nprint(\"\\n=== AVEC SCALING ===\")\nprint(f\"Scores - min: {scores_avec.min():.1f}, max: {scores_avec.max():.1f}\")\nprint(f\"Poids d'attention max par ligne: {poids_avec.max(dim=-1).values[:5]}\")\n\nprint(\"\\nAvec scaling, les poids sont mieux répartis (pas de valeur proche de 1)\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SANS SCALING ===\n",
      "Scores - min: -59.2, max: 58.5\n",
      "Poids d'attention max par ligne: tensor([1.0000, 0.9968, 1.0000, 1.0000, 1.0000])\n",
      "\n",
      "=== AVEC SCALING ===\n",
      "Scores - min: -2.6, max: 2.6\n",
      "Poids d'attention max par ligne: tensor([0.5353, 0.2391, 0.3317, 0.2887, 0.4791])\n",
      "\n",
      "Avec scaling, les poids sont mieux répartis (pas de valeur proche de 1)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Exercice 2 : Classe SelfAttention\n",
    "\n",
    "### D'où viennent Q, K, V ?\n",
    "\n",
    "Jusqu'ici, on a utilisé des tenseurs aléatoires. En pratique, **Q, K, V sont calculés à partir des embeddings** via des matrices apprenables.\n",
    "\n",
    "```\n",
    "x (embeddings) ──┬──► W_q ──► Q   (ce que je cherche)\n",
    "                 ├──► W_k ──► K   (comment je me présente)\n",
    "                 └──► W_v ──► V   (l'info que je transmets)\n",
    "```\n",
    "\n",
    "### Pourquoi 3 matrices différentes ?\n",
    "\n",
    "Si on utilisait directement `x @ x.T`, on calculerait les **similarités sémantiques** entre mots. \"Pikachu\" serait attentif à \"Raichu\", \"électrique\"...\n",
    "\n",
    "Avec des projections différentes (W_q, W_k, W_v), le modèle peut capturer d'autres types de relations : **syntaxiques** (sujet-verbe), **contextuelles**, etc.\n",
    "\n",
    "Les matrices sont **apprises** pendant l'entraînement : elles s'ajustent pour que la formule `softmax(Q @ K.T) @ V` produise des résultats utiles pour la tâche.\n",
    "\n",
    "> **Pour approfondir** : voir la note *\"Comprendre le Mécanisme d'Attention\"* qui détaille le rôle de Q, K, V et comment les matrices apprennent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:28.532966Z",
     "start_time": "2026-01-26T19:06:28.469268Z"
    }
   },
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Module de Self-Attention.\n",
    "    \n",
    "    Projette l'input x vers Q, K, V puis applique l'attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: Dimension des embeddings d'entrée\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # CORRECTION 1: Créer 3 couches linéaires pour projeter vers Q, K, V\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Embeddings, shape (batch, seq_len, embed_dim)\n",
    "        \n",
    "        Returns:\n",
    "            output: Résultat de l'attention\n",
    "            attention_weights: Poids d'attention\n",
    "        \"\"\"\n",
    "        # CORRECTION 2: Projeter x vers Q, K, V\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # CORRECTION 3: Appliquer la fonction scaled_dot_product_attention\n",
    "        output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        return output, attention_weights"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:28.814866Z",
     "start_time": "2026-01-26T19:06:28.659847Z"
    }
   },
   "source": [
    "# Test du module\n",
    "embed_dim = 32\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "attention_layer = SelfAttention(embed_dim)\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "output, weights = attention_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")   # Attendu: (2, 5, 32)\n",
    "print(f\"Weights shape: {weights.shape}\") # Attendu: (2, 5, 5)\n",
    "\n",
    "# Compter les paramètres\n",
    "n_params = sum(p.numel() for p in attention_layer.parameters())\n",
    "print(f\"\\nNombre de paramètres: {n_params:,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 5, 32])\n",
      "Output shape: torch.Size([2, 5, 32])\n",
      "Weights shape: torch.Size([2, 5, 5])\n",
      "\n",
      "Nombre de paramètres: 3,168\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. Visualisation sur CamemBERT\n\nMaintenant qu'on a compris et implémenté l'attention, regardons ce que ça donne sur un modèle **réellement entraîné**.\n\nOn utilise **CamemBERT**, le modèle français qu'on a découvert au TP précédent.\n\n### Tokens spéciaux CamemBERT\n\n| Token | Rôle |\n|-------|------|\n| **\\<s\\>** | Début de phrase (équivalent [CLS]) |\n| **\\</s\\>** | Fin de phrase (équivalent [SEP]) |\n\n### Choix de la couche et de la tête\n\nCamemBERT a **12 couches** et **12 têtes par couche** = 144 matrices d'attention différentes !\n\nToutes ne sont pas intéressantes à visualiser. On va explorer différentes têtes pour voir lesquelles capturent la **coréférence** (le lien entre un pronom et son antécédent).\n\n### Plan de cette section\n\n1. **Phrase standard** : observer la coréférence sur du vocabulaire courant\n2. **Phrase Pokémon** : observer les limites avec du vocabulaire hors-domaine"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:30.222865Z",
     "start_time": "2026-01-26T19:06:29.104459Z"
    }
   },
   "source": [
    "from transformers import CamembertModel, CamembertTokenizer\n",
    "\n",
    "# Charger CamemBERT\n",
    "print(\"Chargement de CamemBERT...\")\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertModel.from_pretrained(\"camembert-base\", output_attentions=True)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Modèle chargé !\")\n",
    "print(f\"  - Couches : {model.config.num_hidden_layers}\")\n",
    "print(f\"  - Têtes par couche : {model.config.num_attention_heads}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement de CamemBERT...\n",
      "Modèle chargé !\n",
      "  - Couches : 12\n",
      "  - Têtes par couche : 12\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:30.984175Z",
     "start_time": "2026-01-26T19:06:30.241611Z"
    }
   },
   "source": "# 1. PHRASE STANDARD - Coréférence claire\nphrase_standard = \"Le chat dort sur le canapé car il est fatigué\"\n\ninputs = tokenizer(phrase_standard, return_tensors=\"pt\")\ntokens_standard = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nattentions_standard = outputs.attentions\n\nprint(\"=== PHRASE STANDARD ===\")\nprint(f\"Phrase: {phrase_standard}\")\nprint(f\"Tokens: {tokens_standard}\")\nprint(\"\\nObservation : chaque mot = 1 token (vocabulaire courant)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.1 Visualisation de la coréférence (phrase standard)\n\nAvec une phrase standard, le pronom \"il\" devrait pointer vers \"chat\"."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_attention(attention_matrix, tokens, title=\"Attention\"):\n",
    "    \"\"\"Affiche une matrice d'attention avec matplotlib.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(attention_matrix, cmap='Blues')\n",
    "    plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(tokens)), tokens)\n",
    "    plt.xlabel(\"Tokens regardés (Keys)\")\n",
    "    plt.ylabel(\"Tokens qui regardent (Queries)\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label=\"Poids d'attention\")\n",
    "    \n",
    "    # Ajouter les valeurs\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            val = attention_matrix[i, j]\n",
    "            color = 'white' if val > 0.3 else 'black'\n",
    "            plt.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                    color=color, fontsize=7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:32.469959Z",
     "start_time": "2026-01-26T19:06:32.256233Z"
    }
   },
   "source": "# Visualiser l'attention sur la phrase standard\nlayer = 7  # Couche 8 (0-indexed)\nhead = 9   # Tête 10 (0-indexed)\n\nattention_matrix = attentions_standard[layer][0, head].numpy()\nplot_attention(attention_matrix, tokens_standard, f\"Phrase standard - Couche {layer+1}, Tête {head+1}\")",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attentions_standard' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m layer = \u001b[32m7\u001b[39m  \u001b[38;5;66;03m# Couche 8 (0-indexed)\u001b[39;00m\n\u001b[32m      3\u001b[39m head = \u001b[32m9\u001b[39m   \u001b[38;5;66;03m# Tête 10 (0-indexed)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m attention_matrix = \u001b[43mattentions_standard\u001b[49m[layer][\u001b[32m0\u001b[39m, head].numpy()\n\u001b[32m      6\u001b[39m plot_attention(attention_matrix, tokens_standard, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPhrase standard - Couche \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Tête \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'attentions_standard' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Que regarde le pronom \"il\" dans la phrase standard ?\nil_index = None\nfor i, t in enumerate(tokens_standard):\n    if t == \"▁il\":\n        il_index = i\n        break\n\nif il_index:\n    print(f\"Attention de 'il' (Couche {layer+1}, Tête {head+1}) :\")\n    print(\"-\" * 50)\n    \n    for i, (token, weight) in enumerate(zip(tokens_standard, attention_matrix[il_index])):\n        bar = \"*\" * int(weight * 30)\n        highlight = \" <-- antécédent !\" if \"chat\" in token.lower() else \"\"\n        print(f\"  {token:15} {weight:.3f} {bar}{highlight}\")\n    \n    print(\"\\n'il' devrait regarder principalement 'chat' (coréférence)\")\nelse:\n    print(\"Token 'il' non trouvé\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Recherche automatique des meilleures têtes pour la coréférence \"il\" -> \"chat\"\nil_idx = tokens_standard.index(\"▁il\") if \"▁il\" in tokens_standard else None\nchat_idx = tokens_standard.index(\"▁chat\") if \"▁chat\" in tokens_standard else None\n\nif il_idx and chat_idx:\n    print(\"Recherche des têtes qui capturent le mieux 'il' -> 'chat'...\")\n    print(\"=\" * 60)\n    \n    # Parcourir toutes les couches et têtes\n    scores = []\n    for layer_i in range(len(attentions_standard)):\n        for head_i in range(attentions_standard[layer_i].shape[1]):\n            weight = attentions_standard[layer_i][0, head_i, il_idx, chat_idx].item()\n            scores.append((weight, layer_i, head_i))\n    \n    # Trier par poids décroissant\n    scores.sort(reverse=True)\n    \n    print(f\"\\nTop 5 des têtes où 'il' regarde le plus 'chat' :\")\n    print(\"-\" * 60)\n    for weight, layer_i, head_i in scores[:5]:\n        print(f\"  Couche {layer_i+1:2d}, Tête {head_i+1:2d} : poids = {weight:.3f}\")\n    \n    print(f\"\\nPour visualiser la meilleure : layer={scores[0][1]}, head={scores[0][2]}\")\nelse:\n    print(\"Tokens 'il' ou 'chat' non trouvés\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:32.609427Z",
     "start_time": "2026-01-26T19:06:32.503138Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_standard' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Recherche automatique des meilleures têtes pour la coréférence \"il\" -> \"chat\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m il_idx = tokens_standard.index(\u001b[33m\"\u001b[39m\u001b[33m▁il\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m▁il\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokens_standard\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      3\u001b[39m chat_idx = tokens_standard.index(\u001b[33m\"\u001b[39m\u001b[33m▁chat\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m▁chat\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokens_standard \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m il_idx \u001b[38;5;129;01mand\u001b[39;00m chat_idx:\n",
      "\u001b[31mNameError\u001b[39m: name 'tokens_standard' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 Limites avec le vocabulaire Pokémon\n\nEssayons maintenant avec notre corpus Pokémon. Les noms comme \"Pikachu\" ou \"Dracaufeu\" sont **hors du vocabulaire** de CamemBERT (entraîné sur du français standard).\n\n**Conséquence** : ces mots sont découpés en sous-tokens, ce qui rend la coréférence plus difficile à capturer."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:33.506053Z",
     "start_time": "2026-01-26T19:06:33.356197Z"
    }
   },
   "source": "# 2. PHRASE POKÉMON - Vocabulaire hors-domaine\nphrase_pokemon = \"Pikachu a utilisé Tonnerre sur Dracaufeu car il était très efficace\"\n\ninputs_pokemon = tokenizer(phrase_pokemon, return_tensors=\"pt\")\ntokens_pokemon = tokenizer.convert_ids_to_tokens(inputs_pokemon[\"input_ids\"][0])\n\nwith torch.no_grad():\n    outputs_pokemon = model(**inputs_pokemon)\n\nattentions_pokemon = outputs_pokemon.attentions\n\nprint(\"=== PHRASE POKÉMON ===\")\nprint(f\"Phrase: {phrase_pokemon}\")\nprint(f\"Tokens: {tokens_pokemon}\")\nprint(f\"\\nObservation : 'Pikachu' et 'Dracaufeu' sont découpés en sous-tokens !\")\nprint(\"Le modèle n'a jamais vu ces mots pendant son entraînement.\")",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 2. PHRASE POKÉMON - Vocabulaire hors-domaine\u001b[39;00m\n\u001b[32m      2\u001b[39m phrase_pokemon = \u001b[33m\"\u001b[39m\u001b[33mPikachu a utilisé Tonnerre sur Dracaufeu car il était très efficace\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m inputs_pokemon = \u001b[43mtokenizer\u001b[49m(phrase_pokemon, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m tokens_pokemon = tokenizer.convert_ids_to_tokens(inputs_pokemon[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m])\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:33.799938Z",
     "start_time": "2026-01-26T19:06:33.669454Z"
    }
   },
   "source": "# Visualiser l'attention sur la phrase Pokémon\nattention_matrix_pokemon = attentions_pokemon[layer][0, head].numpy()\nplot_attention(attention_matrix_pokemon, tokens_pokemon, f\"Phrase Pokémon - Couche {layer+1}, Tête {head+1}\")",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attentions_pokemon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Visualiser l'attention sur la phrase Pokémon\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m attention_matrix_pokemon = \u001b[43mattentions_pokemon\u001b[49m[layer][\u001b[32m0\u001b[39m, head].numpy()\n\u001b[32m      3\u001b[39m plot_attention(attention_matrix_pokemon, tokens_pokemon, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPhrase Pokémon - Couche \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Tête \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'attentions_pokemon' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:41.009828Z",
     "start_time": "2026-01-26T19:06:40.955760Z"
    }
   },
   "source": "# Que regarde \"il\" dans la phrase Pokémon ?\nil_index_pokemon = None\nfor i, t in enumerate(tokens_pokemon):\n    if \"il\" in t.lower():\n        il_index_pokemon = i\n        break\n\nif il_index_pokemon:\n    print(f\"Attention de 'il' (Couche {layer+1}, Tête {head+1}) :\")\n    print(\"-\" * 50)\n    \n    for i, (token, weight) in enumerate(zip(tokens_pokemon, attention_matrix_pokemon[il_index_pokemon])):\n        bar = \"*\" * int(weight * 30)\n        print(f\"  {token:15} {weight:.3f} {bar}\")\n    \n    print(\"\\nLa coréférence est moins claire : 'il' peut référer à Pikachu, Tonnerre, ou Dracaufeu.\")\n    print(\"De plus, ces entités sont fragmentées en sous-tokens.\")\nelse:\n    print(\"Token 'il' non trouvé\")",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_pokemon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Que regarde \"il\" dans la phrase Pokémon ?\u001b[39;00m\n\u001b[32m      2\u001b[39m il_index_pokemon = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtokens_pokemon\u001b[49m):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mil\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m t.lower():\n\u001b[32m      5\u001b[39m         il_index_pokemon = i\n",
      "\u001b[31mNameError\u001b[39m: name 'tokens_pokemon' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "# Comparaison : utiliser les MÊMES têtes (trouvées sur phrase standard) sur la phrase Pokémon\n# Hypothèse : si une tête capture la coréférence, elle devrait le faire quelle que soit la phrase\n\nprint(\"Comparaison des meilleures têtes de coréférence sur les deux phrases\")\nprint(\"=\" * 70)\nprint(\"(Têtes trouvées sur phrase standard, appliquées sur phrase Pokémon)\\n\")\n\nil_idx_poke = tokens_pokemon.index(\"▁il\") if \"▁il\" in tokens_pokemon else None\n\nif il_idx_poke and 'scores' in dir():\n    for weight_std, layer_i, head_i in scores[:3]:  # Top 3 têtes\n        print(f\"Couche {layer_i+1}, Tête {head_i+1} (poids 'il'→'chat' sur standard = {weight_std:.3f}) :\")\n        \n        # Sur phrase Pokémon : où regarde \"il\" ?\n        weights_poke = attentions_pokemon[layer_i][0, head_i, il_idx_poke].numpy()\n        top_indices = weights_poke.argsort()[-5:][::-1]\n        \n        print(f\"    → Sur phrase Pokémon, 'il' regarde :\")\n        for idx in top_indices:\n            print(f\"        {tokens_pokemon[idx]:15} : {weights_poke[idx]:.3f}\")\n        print()\n    \n    print(\"-\" * 70)\n    print(\"Observation : les mêmes têtes qui capturent 'il'→'chat' sur la phrase\")\n    print(\"standard ne trouvent pas nécessairement la bonne coréférence sur la\")\n    print(\"phrase Pokémon (vocabulaire fragmenté + ambiguïté sémantique).\")\nelse:\n    print(\"Exécutez d'abord les cellules précédentes (scores et tokens_pokemon)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Comment améliorer ?\n",
    "\n",
    "Le modèle a du mal avec le vocabulaire Pokémon car :\n",
    "1. **Tokenization fragmentée** : \"Pikachu\" = plusieurs sous-tokens\n",
    "2. **Embeddings non spécialisés** : le modèle n'a jamais appris ces concepts\n",
    "\n",
    "**Solutions possibles** :\n",
    "- **Fine-tuning** : ré-entraîner sur un corpus Pokémon pour adapter les embeddings\n",
    "- **Enrichir le tokenizer** : ajouter \"Pikachu\", \"Dracaufeu\" comme tokens uniques avec `tokenizer.add_tokens()`, puis fine-tuner\n",
    "\n",
    "Ces techniques seront abordées plus tard.\n",
    "\n",
    "### 4.4 Visualisation interactive avec BertViz\n",
    "\n",
    "**BertViz** est un outil spécialisé pour visualiser l'attention des Transformers.\n",
    "\n",
    "- **head_view** : voir les connexions d'attention pour une couche (carrés colorés = têtes)\n",
    "- **model_view** : vue globale de toutes les couches et têtes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:42.815941Z",
     "start_time": "2026-01-26T19:06:41.620315Z"
    }
   },
   "source": "from bertviz import head_view, model_view\n\n# BertViz sur la phrase standard (coréférence plus visible)\nprint(\"head_view sur la phrase standard\")\nprint(\"Cliquez sur une tête pour voir ses connexions d'attention\")\nprint(\"=\"*50)\n\nhead_view(attentions_standard, tokens_standard)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_view sur la phrase standard\n",
      "Cliquez sur une tête pour voir ses connexions d'attention\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'attentions_standard' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCliquez sur une tête pour voir ses connexions d\u001b[39m\u001b[33m'\u001b[39m\u001b[33mattention\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m head_view(\u001b[43mattentions_standard\u001b[49m, tokens_standard)\n",
      "\u001b[31mNameError\u001b[39m: name 'attentions_standard' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# BertViz sur la phrase Pokémon (pour comparer)\nprint(\"head_view sur la phrase Pokémon\")\nprint(\"Comparez avec la phrase standard : la coréférence est-elle aussi claire ?\")\nprint(\"=\"*50)\n\nhead_view(attentions_pokemon, tokens_pokemon)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Pourquoi Multi-Head Attention ?\n",
    "\n",
    "### Le problème avec une seule tête\n",
    "\n",
    "Une seule tête d'attention calcule **une** représentation des relations entre mots.\n",
    "\n",
    "Mais dans une phrase, il y a **plusieurs types de relations** :\n",
    "- Relations **syntaxiques** (sujet-verbe)\n",
    "- Relations **sémantiques** (sens, coréférence)\n",
    "- Relations de **proximité** (mots voisins)\n",
    "- etc.\n",
    "\n",
    "### La solution : plusieurs têtes\n",
    "\n",
    "Chaque tête peut apprendre à détecter un type de relation différent !\n",
    "\n",
    "**Attention** : En pratique, une tête ne correspond pas forcément à un type de relation précis. Une relation peut être portée par une combinaison de têtes, et une tête peut capturer plusieurs types de patterns. C'est ce que le modèle apprend pendant l'entraînement.\n",
    "\n",
    "**Analogie** : C'est comme avoir plusieurs experts qui analysent une phrase sous différents angles, puis combinent leurs analyses.\n",
    "\n",
    "### Exemple : comparons deux têtes de CamemBERT"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Comparaison de 2 têtes sur la phrase standard\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Tête des couches profondes\nhead_deep = attentions_standard[9][0, 5].numpy()  # Couche 10, Tête 6\n\n# Tête des premières couches (attention locale)\nhead_local = attentions_standard[1][0, 0].numpy()  # Couche 2, Tête 1\n\nfor idx, (attn, title) in enumerate([\n    (head_deep, \"Couche 10, Tête 6\\n(couches profondes)\"),\n    (head_local, \"Couche 2, Tête 1\\n(attention locale)\")\n]):\n    ax = axes[idx]\n    im = ax.imshow(attn, cmap='Blues')\n    ax.set_xticks(range(len(tokens_standard)))\n    ax.set_xticklabels(tokens_standard, rotation=45, ha='right', fontsize=9)\n    ax.set_yticks(range(len(tokens_standard)))\n    ax.set_yticklabels(tokens_standard, fontsize=9)\n    ax.set_title(title, fontsize=12)\n    plt.colorbar(im, ax=ax)\n\nplt.suptitle(\"Deux têtes capturent des patterns différents\", fontsize=13, y=1.02)\nplt.tight_layout()\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Que regarde \"il\" selon chaque tête ? (phrase standard)\nil_idx = tokens_standard.index(\"▁il\") if \"▁il\" in tokens_standard else None\n\nif il_idx:\n    print(f\"Que regarde 'il' selon chaque tête ?\")\n    print(\"=\" * 50)\n    \n    print(f\"Tête profonde (C10-T6) : \", end=\"\")\n    for i, w in enumerate(head_deep[il_idx]):\n        if w > 0.08:\n            print(f\"{tokens_standard[i]}({w:.2f}) \", end=\"\")\n    print()\n    \n    print(f\"Tête locale (C2-T1) :    \", end=\"\")\n    for i, w in enumerate(head_local[il_idx]):\n        if w > 0.08:\n            print(f\"{tokens_standard[i]}({w:.2f}) \", end=\"\")\n    print()\n    \n    print(\"\\nLes couches profondes capturent mieux la coréférence (il -> chat)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Architecture Multi-Head Attention\n",
    "\n",
    "### Schéma\n",
    "\n",
    "```\n",
    "Input (seq_len, embed_dim)\n",
    "        |\n",
    "   +----+----+--------+--------+\n",
    "   |         |        |        |\n",
    "   v         v        v        v\n",
    " Head 1   Head 2   Head 3   Head 4\n",
    "   |         |        |        |\n",
    "   +----+----+--------+--------+\n",
    "        |\n",
    "    Concat\n",
    "        |\n",
    "   Linear (W_o)\n",
    "        |\n",
    "Output (seq_len, embed_dim)\n",
    "```\n",
    "\n",
    "### Dimensions\n",
    "\n",
    "| Paramètre | Exemple | Description |\n",
    "|-----------|---------|-------------|\n",
    "| embed_dim | 512 | Dimension des embeddings |\n",
    "| num_heads | 8 | Nombre de têtes |\n",
    "| d_k | 64 | Dimension par tête = embed_dim / num_heads |\n",
    "\n",
    "Chaque tête travaille avec une **dimension réduite** ($d_k$), puis on **concatène** les résultats.\n",
    "\n",
    "### Pourquoi W_o après le concat ?\n",
    "\n",
    "Après concaténation, on a un vecteur de dimension `embed_dim` (4 têtes x 64 = 512).\n",
    "\n",
    "La matrice **W_o** (output projection) permet de :\n",
    "1. **Mélanger** les informations des différentes têtes\n",
    "2. **Apprendre** comment combiner leurs \"points de vue\"\n",
    "\n",
    "Sans W_o, les têtes resteraient indépendantes. Avec W_o, le modèle peut apprendre des combinaisons utiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercice 3 : split_heads (Multi-Head)\n",
    "\n",
    "La première étape du Multi-Head est de **séparer les têtes**.\n",
    "\n",
    "On transforme `(batch, seq_len, embed_dim)` en `(batch, num_heads, seq_len, d_k)`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:43.251788Z",
     "start_time": "2026-01-26T19:06:43.145515Z"
    }
   },
   "source": [
    "# Configuration\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "embed_dim = 32\n",
    "num_heads = 4\n",
    "d_k = embed_dim // num_heads  # 32 / 4 = 8\n",
    "\n",
    "print(f\"embed_dim: {embed_dim}\")\n",
    "print(f\"num_heads: {num_heads}\")\n",
    "print(f\"d_k (dim par tête): {d_k}\")\n",
    "\n",
    "# Input\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "print(f\"\\nInput shape: {x.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_dim: 32\n",
      "num_heads: 4\n",
      "d_k (dim par tête): 8\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33md_k (dim par tête): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Input\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m x = \u001b[43mtorch\u001b[49m.randn(batch_size, seq_len, embed_dim)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mInput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:43.431519Z",
     "start_time": "2026-01-26T19:06:43.374956Z"
    }
   },
   "source": [
    "def split_heads(x, num_heads):\n",
    "    \"\"\"\n",
    "    Sépare les têtes d'attention.\n",
    "    \n",
    "    Reshape: (batch, seq_len, embed_dim) -> (batch, num_heads, seq_len, d_k)\n",
    "    \n",
    "    Args:\n",
    "        x: Tensor de shape (batch, seq_len, embed_dim)\n",
    "        num_heads: Nombre de têtes\n",
    "    \n",
    "    Returns:\n",
    "        Tensor de shape (batch, num_heads, seq_len, d_k)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, embed_dim = x.shape\n",
    "    d_k = embed_dim // num_heads\n",
    "    \n",
    "    # CORRECTION:\n",
    "    # Étape 1: Séparer embed_dim en (num_heads, d_k)\n",
    "    x = x.view(batch_size, seq_len, num_heads, d_k)\n",
    "    # Étape 2: Réorganiser pour avoir num_heads en position 1\n",
    "    x = x.transpose(1, 2)\n",
    "    \n",
    "    return x"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:43.606442Z",
     "start_time": "2026-01-26T19:06:43.511574Z"
    }
   },
   "source": [
    "# Test\n",
    "x_test = torch.randn(2, 6, 32)  # batch=2, seq=6, embed=32\n",
    "x_heads = split_heads(x_test, num_heads=4)\n",
    "\n",
    "print(f\"Avant split: {x_test.shape}\")   # (2, 6, 32)\n",
    "print(f\"Après split: {x_heads.shape}\")  # Attendu: (2, 4, 6, 8)\n",
    "\n",
    "if x_heads is not None and x_heads.shape == (2, 4, 6, 8):\n",
    "    print(\"\\nCorrect !\")\n",
    "else:\n",
    "    print(\"\\nVérifiez votre implémentation\")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m x_test = \u001b[43mtorch\u001b[49m.randn(\u001b[32m2\u001b[39m, \u001b[32m6\u001b[39m, \u001b[32m32\u001b[39m)  \u001b[38;5;66;03m# batch=2, seq=6, embed=32\u001b[39;00m\n\u001b[32m      3\u001b[39m x_heads = split_heads(x_test, num_heads=\u001b[32m4\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAvant split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_test.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)   \u001b[38;5;66;03m# (2, 6, 32)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8. Récapitulatif\n\n### Ce que nous avons appris aujourd'hui\n\n| Concept | Ce qu'on a fait |\n|---------|----------------|\n| **Exercice 1** | Fonction `scaled_dot_product_attention()` complète |\n| **Exercice 2** | Classe `SelfAttention` avec projections W_q, W_k, W_v |\n| **Visualisation** | Explorer l'attention de CamemBERT avec BertViz |\n| **Multi-Head** | Comprendre pourquoi plusieurs têtes |\n| **Exercice 3** | Fonction `split_heads()` pour séparer les têtes |\n\n### Questions de compréhension\n\nAvant de passer au TP suivant, vérifiez que vous pouvez répondre à ces questions :\n\n1. **Scaling** : Que se passe-t-il si on ne divise pas par sqrt(d_k) ? Pourquoi ?\n\n2. **Softmax** : Pourquoi la somme des poids d'attention fait toujours 1 ?\n\n3. **Paramètres** : Combien de paramètres a une couche `SelfAttention` avec `embed_dim=512` ? (indice : 3 matrices W_q, W_k, W_v)\n\n4. **Multi-Head** : Si on a `embed_dim=512` et `num_heads=8`, quelle est la dimension d_k de chaque tête ?\n\n5. **Q != K** : Pourquoi utilise-t-on des matrices différentes pour Q et K au lieu de faire simplement `x @ x.T` ?\n\n<details>\n<summary>Réponses</summary>\n\n1. Les scores deviennent très grands -> softmax donne des poids proches de 0 ou 1 -> gradients instables\n\n2. Les poids d'attention sont obtenus en appliquant softmax aux scores. Par définition, softmax produit des valeurs positives qui somment à 1 (distribution de probabilités).\n\n3. 3 x (512 x 512 + 512) = 3 x 262 656 = **787 968** paramètres (poids + biais)\n\n4. d_k = 512 / 8 = **64**\n\n5. Pour permettre au modèle de capturer d'autres relations que la similarité sémantique (relations syntaxiques, contextuelles, etc.)\n</details>\n\n### Prochaine session\n\nOn va :\n1. Terminer l'implémentation Multi-Head (`concat_heads`, classe complète)\n2. Ajouter le **Feed-Forward Network**\n3. Comprendre le **masque causal** (GPT vs BERT)\n4. Assembler un **bloc Transformer** complet !"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Pour aller plus loin (optionnel)\n",
    "\n",
    "### 9.1 Explorer toutes les têtes d'une couche"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:44.081084Z",
     "start_time": "2026-01-26T19:06:43.962083Z"
    }
   },
   "source": "# Visualiser 4 têtes différentes de la même couche (phrase standard)\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\nheads_to_show = [0, 3, 7, 11]\nlayer = 8  # Couche 9\n\nfor idx, head in enumerate(heads_to_show):\n    ax = axes[idx // 2, idx % 2]\n    w = attentions_standard[layer][0, head].numpy()\n    \n    im = ax.imshow(w, cmap='Blues')\n    ax.set_xticks(range(len(tokens_standard)))\n    ax.set_xticklabels(tokens_standard, rotation=45, ha='right', fontsize=8)\n    ax.set_yticks(range(len(tokens_standard)))\n    ax.set_yticklabels(tokens_standard, fontsize=8)\n    ax.set_title(f\"Tête {head + 1}\", fontsize=11)\n    plt.colorbar(im, ax=ax)\n\nplt.suptitle(f\"Différentes têtes de la couche {layer+1} - CamemBERT\", fontsize=13)\nplt.tight_layout()\nplt.show()",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Visualiser 4 têtes différentes de la même couche (phrase standard)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m fig, axes = \u001b[43mplt\u001b[49m.subplots(\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m14\u001b[39m, \u001b[32m12\u001b[39m))\n\u001b[32m      4\u001b[39m heads_to_show = [\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m7\u001b[39m, \u001b[32m11\u001b[39m]\n\u001b[32m      5\u001b[39m layer = \u001b[32m8\u001b[39m  \u001b[38;5;66;03m# Couche 9\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:44.464254Z",
     "start_time": "2026-01-26T19:06:44.283013Z"
    }
   },
   "source": "# Analyse : que regarde \"il\" selon chaque tête ?\nil_idx = tokens_standard.index(\"▁il\") if \"▁il\" in tokens_standard else None\n\nif il_idx:\n    print(f\"Que regarde 'il' selon chaque tête de la couche {layer+1} ?\")\n    print(\"=\" * 60)\n    \n    for head in heads_to_show:\n        weights = attentions_standard[layer][0, head, il_idx].numpy()\n        top_idx = weights.argsort()[-3:][::-1]  # Top 3\n        print(f\"Tête {head+1:2d}: \", end=\"\")\n        for i in top_idx:\n            print(f\"{tokens_standard[i]} ({weights[i]:.2f})  \", end=\"\")\n        print()",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_standard' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Analyse : que regarde \"il\" selon chaque tête ?\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m il_idx = tokens_standard.index(\u001b[33m\"\u001b[39m\u001b[33m▁il\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m▁il\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokens_standard\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m il_idx:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQue regarde \u001b[39m\u001b[33m'\u001b[39m\u001b[33mil\u001b[39m\u001b[33m'\u001b[39m\u001b[33m selon chaque tête de la couche \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokens_standard' is not defined"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 9.2 Testez vos propres phrases\n\nUtilisez cette fonction pour analyser n'importe quelle phrase :"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:44.665896Z",
     "start_time": "2026-01-26T19:06:44.565712Z"
    }
   },
   "source": "def analyser_phrase(phrase):\n    \"\"\"Analyse l'attention de CamemBERT sur une phrase.\"\"\"\n    inputs = tokenizer(phrase, return_tensors=\"pt\")\n    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    print(f\"Phrase: {phrase}\")\n    print(f\"Tokens: {tokens}\\n\")\n    \n    return head_view(outputs.attentions, tokens)\n\n# Testez avec vos phrases !\nanalyser_phrase(\"Marie a appelé son frère car il était en retard\")",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m head_view(outputs.attentions, tokens)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Testez avec vos phrases !\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43manalyser_phrase\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMarie a appelé son frère car il était en retard\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36manalyser_phrase\u001b[39m\u001b[34m(phrase)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manalyser_phrase\u001b[39m(phrase):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Analyse l'attention de CamemBERT sur une phrase.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     inputs = \u001b[43mtokenizer\u001b[49m(phrase, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     tokens = tokenizer.convert_ids_to_tokens(inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m])\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:44.833531Z",
     "start_time": "2026-01-26T19:06:44.792878Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:06:45.046323Z",
     "start_time": "2026-01-26T19:06:44.992982Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}