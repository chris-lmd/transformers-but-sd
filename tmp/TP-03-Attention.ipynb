{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3 - Maîtriser l'Attention\n",
    "\n",
    "**Module** : Réseaux de Neurones Approfondissement  \n",
    "**Durée** : 2h  \n",
    "**Objectif** : Implémenter l'attention de A à Z et découvrir le Multi-Head\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs pédagogiques\n",
    "\n",
    "À la fin de cette session, vous serez capable de :\n",
    "1. Implémenter la fonction `scaled_dot_product_attention()` complète\n",
    "2. Créer une classe `SelfAttention` réutilisable en PyTorch\n",
    "3. Visualiser et interpréter l'attention d'un vrai modèle (CamemBERT)\n",
    "4. Comprendre pourquoi on utilise **plusieurs têtes** d'attention\n",
    "\n",
    "---\n",
    "\n",
    "## Rappel : Où en sommes-nous ?\n",
    "\n",
    "Dans la session précédente, vous avez :\n",
    "- Compris le **Positional Encoding** (encoder l'ordre des mots)\n",
    "- Vu le lien entre **similarité** et **produit scalaire**\n",
    "- Découvert les concepts de **Query, Key, Value**\n",
    "   - Q : Ce que je cherche\n",
    "   - K : Les étiquettes\n",
    "   - V : Le contenu\n",
    "- Calculé les scores d'attention étape par étape\n",
    "\n",
    "Aujourd'hui, on passe à l'**implémentation complète** !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances (Google Colab)\n",
    "!pip install torch matplotlib numpy transformers bertviz -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Rappel : La formule de l'attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "| Étape | Opération | Rôle |\n",
    "|-------|-----------|------|\n",
    "| 1 | $QK^T$ | Calculer les scores de similarité |\n",
    "| 2 | $\\div \\sqrt{d_k}$ | Stabiliser les gradients |\n",
    "| 3 | softmax | Transformer en probabilités |\n",
    "| 4 | $\\times V$ | Moyenne pondérée des values |\n",
    "\n",
    "Dans la session précédente, vous avez fait ces étapes **séparément**. Maintenant, on les **combine** dans une fonction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Exercice 1 : Fonction d'attention complète\n",
    "\n",
    "Regroupez les 4 étapes dans une seule fonction réutilisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Calcule le Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries, shape (..., seq_len, d_k)\n",
    "        K: Keys, shape (..., seq_len, d_k)\n",
    "        V: Values, shape (..., seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: Résultat de l'attention, même shape que V\n",
    "        attention_weights: Poids d'attention, shape (..., seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # CORRECTION 1: Récupérer d_k (dernière dimension de K)\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # CORRECTION 2: Calculer les scores QK^T\n",
    "    scores = Q @ K.transpose(-2, -1)\n",
    "    \n",
    "    # CORRECTION 3: Appliquer le scaling (diviser par sqrt(d_k))\n",
    "    scaled_scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # CORRECTION 4: Appliquer softmax sur la dernière dimension\n",
    "    attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "    \n",
    "    # CORRECTION 5: Calculer la sortie (weights @ V)\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre fonction\n",
    "Q_test = torch.randn(4, 8)  # 4 tokens, dimension 8\n",
    "K_test = torch.randn(4, 8)\n",
    "V_test = torch.randn(4, 8)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q_test, K_test, V_test)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")    # Attendu: (4, 8)\n",
    "print(f\"Weights shape: {weights.shape}\")  # Attendu: (4, 4)\n",
    "print(f\"Somme des poids par ligne: {weights.sum(dim=-1)}\")  # Attendu: [1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vérification : Effet du scaling\n",
    "\n",
    "Pourquoi diviser par $\\sqrt{d_k}$ ? Voyons l'effet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison avec et sans scaling\n",
    "d_k_grand = 512  # Dimension typique dans un Transformer\n",
    "\n",
    "Q_grand = torch.randn(10, d_k_grand)\n",
    "K_grand = torch.randn(10, d_k_grand)\n",
    "\n",
    "# Sans scaling\n",
    "scores_sans = Q_grand @ K_grand.T\n",
    "attention_sans = F.softmax(scores_sans, dim=-1)\n",
    "\n",
    "# Avec scaling\n",
    "scores_avec = (Q_grand @ K_grand.T) / math.sqrt(d_k_grand)\n",
    "attention_avec = F.softmax(scores_avec, dim=-1)\n",
    "\n",
    "print(\"=== SANS SCALING ===\")\n",
    "print(f\"Scores - min: {scores_sans.min():.1f}, max: {scores_sans.max():.1f}\")\n",
    "print(f\"Attention max par ligne: {attention_sans.max(dim=-1).values[:5]}\")\n",
    "\n",
    "print(\"\\n=== AVEC SCALING ===\")\n",
    "print(f\"Scores - min: {scores_avec.min():.1f}, max: {scores_avec.max():.1f}\")\n",
    "print(f\"Attention max par ligne: {attention_avec.max(dim=-1).values[:5]}\")\n",
    "\n",
    "print(\"\\nAvec scaling, l'attention est mieux répartie (pas de valeur proche de 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Exercice 2 : Classe SelfAttention\n",
    "\n",
    "### D'où viennent Q, K, V ?\n",
    "\n",
    "Jusqu'ici, on a utilisé des tenseurs aléatoires. En pratique, **Q, K, V sont calculés à partir des embeddings** via des matrices apprenables.\n",
    "\n",
    "```\n",
    "x (embeddings) ──┬──► W_q ──► Q   (ce que je cherche)\n",
    "                 ├──► W_k ──► K   (comment je me présente)\n",
    "                 └──► W_v ──► V   (l'info que je transmets)\n",
    "```\n",
    "\n",
    "### Pourquoi 3 matrices différentes ?\n",
    "\n",
    "Si on utilisait directement `x @ x.T`, on calculerait les **similarités sémantiques** entre mots. \"Pikachu\" serait attentif à \"Raichu\", \"électrique\"...\n",
    "\n",
    "Avec des projections différentes (W_q, W_k, W_v), le modèle peut capturer d'autres types de relations : **syntaxiques** (sujet-verbe), **contextuelles**, etc.\n",
    "\n",
    "Les matrices sont **apprises** pendant l'entraînement : elles s'ajustent pour que la formule `softmax(Q @ K.T) @ V` produise des résultats utiles pour la tâche.\n",
    "\n",
    "> **Pour approfondir** : voir la note *\"Comprendre le Mécanisme d'Attention\"* qui détaille le rôle de Q, K, V et comment les matrices apprennent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Module de Self-Attention.\n",
    "    \n",
    "    Projette l'input x vers Q, K, V puis applique l'attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: Dimension des embeddings d'entrée\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # CORRECTION 1: Créer 3 couches linéaires pour projeter vers Q, K, V\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Embeddings, shape (batch, seq_len, embed_dim)\n",
    "        \n",
    "        Returns:\n",
    "            output: Résultat de l'attention\n",
    "            attention_weights: Poids d'attention\n",
    "        \"\"\"\n",
    "        # CORRECTION 2: Projeter x vers Q, K, V\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # CORRECTION 3: Appliquer la fonction scaled_dot_product_attention\n",
    "        output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du module\n",
    "embed_dim = 32\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "attention_layer = SelfAttention(embed_dim)\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "output, weights = attention_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")   # Attendu: (2, 5, 32)\n",
    "print(f\"Weights shape: {weights.shape}\") # Attendu: (2, 5, 5)\n",
    "\n",
    "# Compter les paramètres\n",
    "n_params = sum(p.numel() for p in attention_layer.parameters())\n",
    "print(f\"\\nNombre de paramètres: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Visualisation sur CamemBERT\n",
    "\n",
    "Maintenant qu'on a compris et implémenté l'attention, regardons ce que ça donne sur un modèle **réellement entraîné**.\n",
    "\n",
    "On utilise **CamemBERT**, le modèle français qu'on a découvert au TP précédent, avec notre corpus Pokémon !\n",
    "\n",
    "### Tokens spéciaux CamemBERT\n",
    "\n",
    "| Token | Rôle |\n",
    "|-------|------|\n",
    "| **\\<s\\>** | Début de phrase (équivalent [CLS]) |\n",
    "| **\\</s\\>** | Fin de phrase (équivalent [SEP]) |\n",
    "\n",
    "### Choix de la couche et de la tête\n",
    "\n",
    "CamemBERT a **12 couches** et **12 têtes par couche** = 144 matrices d'attention différentes !\n",
    "\n",
    "Toutes ne sont pas intéressantes à visualiser. On va explorer différentes têtes pour voir lesquelles capturent la **coréférence** (le lien entre un pronom et son antécédent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertModel, CamembertTokenizer\n",
    "\n",
    "# Charger CamemBERT\n",
    "print(\"Chargement de CamemBERT...\")\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertModel.from_pretrained(\"camembert-base\", output_attentions=True)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Modèle chargé !\")\n",
    "print(f\"  - Couches : {model.config.num_hidden_layers}\")\n",
    "print(f\"  - Têtes par couche : {model.config.num_attention_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase Pokémon avec coréférence\n",
    "phrase = \"Pikachu a utilisé Tonnerre sur Dracaufeu car il était très efficace\"\n",
    "\n",
    "# Tokeniser\n",
    "inputs = tokenizer(phrase, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "attentions = outputs.attentions  # tuple de 12 tenseurs (1 par couche)\n",
    "\n",
    "print(f\"Phrase: {phrase}\")\n",
    "print(f\"\\nTokens ({len(tokens)}): {tokens}\")\n",
    "print(f\"\\nStructure des attentions:\")\n",
    "print(f\"  - Nombre de couches: {len(attentions)}\")\n",
    "print(f\"  - Shape par couche: {attentions[0].shape}  (batch, heads, seq, seq)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Visualisation avec Matplotlib\n",
    "\n",
    "Commençons par une visualisation classique pour comprendre la structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention_matrix, tokens, title=\"Attention\"):\n",
    "    \"\"\"Affiche une matrice d'attention avec matplotlib.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(attention_matrix, cmap='Blues')\n",
    "    plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(tokens)), tokens)\n",
    "    plt.xlabel(\"Tokens regardés (Keys)\")\n",
    "    plt.ylabel(\"Tokens qui regardent (Queries)\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label=\"Poids d'attention\")\n",
    "    \n",
    "    # Ajouter les valeurs\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            val = attention_matrix[i, j]\n",
    "            color = 'white' if val > 0.3 else 'black'\n",
    "            plt.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                    color=color, fontsize=7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser une tête - Couche 8, Tête 10 (souvent intéressante pour la coréférence)\n",
    "layer = 7  # 0-indexed\n",
    "head = 9   # 0-indexed\n",
    "\n",
    "attention_matrix = attentions[layer][0, head].numpy()\n",
    "plot_attention(attention_matrix, tokens, f\"CamemBERT - Couche {layer+1}, Tête {head+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Que regarde le pronom \"il\" ?\n",
    "# Trouver l'index de \"il\" dans les tokens\n",
    "il_index = None\n",
    "for i, t in enumerate(tokens):\n",
    "    if \"il\" in t.lower():\n",
    "        il_index = i\n",
    "        break\n",
    "\n",
    "if il_index:\n",
    "    print(f\"Attention de '{tokens[il_index]}' (Couche {layer+1}, Tête {head+1}) :\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, (token, weight) in enumerate(zip(tokens, attention_matrix[il_index])):\n",
    "        bar = \"*\" * int(weight * 30)\n",
    "        highlight = \"\"\n",
    "        if \"Pikachu\" in token or \"pikachu\" in token.lower():\n",
    "            highlight = \" <-- Pikachu ?\"\n",
    "        elif \"Tonnerre\" in token or \"tonnerre\" in token.lower():\n",
    "            highlight = \" <-- Tonnerre ?\"\n",
    "        print(f\"  {token:15} {weight:.3f} {bar}{highlight}\")\n",
    "else:\n",
    "    print(\"Token 'il' non trouvé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Visualisation interactive avec BertViz\n",
    "\n",
    "**BertViz** est un outil spécialisé pour visualiser l'attention des Transformers. Il offre des vues interactives beaucoup plus riches que matplotlib.\n",
    "\n",
    "Trois modes de visualisation :\n",
    "- **head_view** : voir les connexions d'attention pour une couche\n",
    "- **model_view** : vue globale de toutes les couches et têtes\n",
    "- **neuron_view** : voir comment Q et K contribuent aux scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view, model_view\n",
    "\n",
    "# Préparer les attentions pour BertViz\n",
    "# BertViz attend une liste/tuple de tenseurs, ce qu'on a déjà !\n",
    "\n",
    "print(\"head_view : visualisation interactive d'une couche\")\n",
    "print(\"Cliquez sur une tête pour voir ses connexions d'attention\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head View - Vue par tête (interactif)\n",
    "# Affiche toutes les têtes d'une couche avec les liens d'attention\n",
    "head_view(attentions, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model View - Vue globale du modèle\n",
    "# Permet de naviguer dans toutes les couches et têtes\n",
    "print(\"model_view : vue globale de toutes les couches\")\n",
    "print(\"Sélectionnez une couche et une tête pour explorer\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model_view(attentions, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Explorer d'autres phrases Pokémon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyser_attention_pokemon(phrase, mot_cible=None):\n",
    "    \"\"\"\n",
    "    Analyse l'attention de CamemBERT sur une phrase Pokémon.\n",
    "    \n",
    "    Args:\n",
    "        phrase: La phrase à analyser\n",
    "        mot_cible: Le mot dont on veut voir l'attention (optionnel)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(phrase, return_tensors=\"pt\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    attentions = outputs.attentions\n",
    "    \n",
    "    print(f\"Phrase: {phrase}\")\n",
    "    print(f\"Tokens: {tokens}\\n\")\n",
    "    \n",
    "    # Afficher avec BertViz\n",
    "    return head_view(attentions, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essayez différentes phrases !\n",
    "phrases_pokemon = [\n",
    "    \"Le dresseur a rappelé Salamèche car il était trop faible\",\n",
    "    \"Évoli a évolué en Aquali parce qu'il a touché une Pierre Eau\",\n",
    "    \"Sacha a attrapé un Rondoudou et il était très content\",\n",
    "]\n",
    "\n",
    "# Analysez la première phrase\n",
    "analyser_attention_pokemon(phrases_pokemon[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Pourquoi Multi-Head Attention ?\n",
    "\n",
    "### Le problème avec une seule tête\n",
    "\n",
    "Une seule tête d'attention calcule **une** représentation des relations entre mots.\n",
    "\n",
    "Mais dans une phrase, il y a **plusieurs types de relations** :\n",
    "- Relations **syntaxiques** (sujet-verbe)\n",
    "- Relations **sémantiques** (sens, coréférence)\n",
    "- Relations de **proximité** (mots voisins)\n",
    "- etc.\n",
    "\n",
    "### La solution : plusieurs têtes\n",
    "\n",
    "Chaque tête peut apprendre à détecter un type de relation différent !\n",
    "\n",
    "**Attention** : En pratique, une tête ne correspond pas forcément à un type de relation précis. Une relation peut être portée par une combinaison de têtes, et une tête peut capturer plusieurs types de patterns. C'est ce que le modèle apprend pendant l'entraînement.\n",
    "\n",
    "**Analogie** : C'est comme avoir plusieurs experts qui analysent une phrase sous différents angles, puis combinent leurs analyses.\n",
    "\n",
    "### Exemple : comparons deux têtes de CamemBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprenons notre phrase Pokémon\n",
    "phrase = \"Pikachu a utilisé Tonnerre sur Dracaufeu car il était très efficace\"\n",
    "inputs = tokenizer(phrase, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "attentions = outputs.attentions\n",
    "\n",
    "# Comparaison de 2 têtes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Tête des couches profondes (relations sémantiques)\n",
    "head_semantic = attentions[9][0, 5].numpy()  # Couche 10, Tête 6\n",
    "\n",
    "# Tête des premières couches (attention locale)\n",
    "head_local = attentions[1][0, 0].numpy()  # Couche 2, Tête 1\n",
    "\n",
    "for idx, (attn, title) in enumerate([\n",
    "    (head_semantic, \"Couche 10, Tête 6\\n(couches profondes)\"),\n",
    "    (head_local, \"Couche 2, Tête 1\\n(attention plus locale)\")\n",
    "]):\n",
    "    ax = axes[idx]\n",
    "    im = ax.imshow(attn, cmap='Blues')\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_yticklabels(tokens, fontsize=9)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle(\"Deux têtes de CamemBERT capturent des patterns différents\", fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver l'index de \"il\"\n",
    "il_index = None\n",
    "for i, t in enumerate(tokens):\n",
    "    if \"il\" in t.lower():\n",
    "        il_index = i\n",
    "        break\n",
    "\n",
    "if il_index:\n",
    "    print(f\"Que regarde '{tokens[il_index]}' selon chaque tête ?\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"Tête sémantique (C10-T6) : \", end=\"\")\n",
    "    for i, w in enumerate(head_semantic[il_index]):\n",
    "        if w > 0.08:\n",
    "            print(f\"{tokens[i]}({w:.2f}) \", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Tête locale (C2-T1) :      \", end=\"\")\n",
    "    for i, w in enumerate(head_local[il_index]):\n",
    "        if w > 0.08:\n",
    "            print(f\"{tokens[i]}({w:.2f}) \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Architecture Multi-Head Attention\n",
    "\n",
    "### Schéma\n",
    "\n",
    "```\n",
    "Input (seq_len, embed_dim)\n",
    "        |\n",
    "   +----+----+--------+--------+\n",
    "   |         |        |        |\n",
    "   v         v        v        v\n",
    " Head 1   Head 2   Head 3   Head 4\n",
    "   |         |        |        |\n",
    "   +----+----+--------+--------+\n",
    "        |\n",
    "    Concat\n",
    "        |\n",
    "   Linear (W_o)\n",
    "        |\n",
    "Output (seq_len, embed_dim)\n",
    "```\n",
    "\n",
    "### Dimensions\n",
    "\n",
    "| Paramètre | Exemple | Description |\n",
    "|-----------|---------|-------------|\n",
    "| embed_dim | 512 | Dimension des embeddings |\n",
    "| num_heads | 8 | Nombre de têtes |\n",
    "| d_k | 64 | Dimension par tête = embed_dim / num_heads |\n",
    "\n",
    "Chaque tête travaille avec une **dimension réduite** ($d_k$), puis on **concatène** les résultats.\n",
    "\n",
    "### Pourquoi W_o après le concat ?\n",
    "\n",
    "Après concaténation, on a un vecteur de dimension `embed_dim` (4 têtes x 64 = 512).\n",
    "\n",
    "La matrice **W_o** (output projection) permet de :\n",
    "1. **Mélanger** les informations des différentes têtes\n",
    "2. **Apprendre** comment combiner leurs \"points de vue\"\n",
    "\n",
    "Sans W_o, les têtes resteraient indépendantes. Avec W_o, le modèle peut apprendre des combinaisons utiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercice 3 : split_heads (Multi-Head)\n",
    "\n",
    "La première étape du Multi-Head est de **séparer les têtes**.\n",
    "\n",
    "On transforme `(batch, seq_len, embed_dim)` en `(batch, num_heads, seq_len, d_k)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "embed_dim = 32\n",
    "num_heads = 4\n",
    "d_k = embed_dim // num_heads  # 32 / 4 = 8\n",
    "\n",
    "print(f\"embed_dim: {embed_dim}\")\n",
    "print(f\"num_heads: {num_heads}\")\n",
    "print(f\"d_k (dim par tête): {d_k}\")\n",
    "\n",
    "# Input\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "print(f\"\\nInput shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, num_heads):\n",
    "    \"\"\"\n",
    "    Sépare les têtes d'attention.\n",
    "    \n",
    "    Reshape: (batch, seq_len, embed_dim) -> (batch, num_heads, seq_len, d_k)\n",
    "    \n",
    "    Args:\n",
    "        x: Tensor de shape (batch, seq_len, embed_dim)\n",
    "        num_heads: Nombre de têtes\n",
    "    \n",
    "    Returns:\n",
    "        Tensor de shape (batch, num_heads, seq_len, d_k)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, embed_dim = x.shape\n",
    "    d_k = embed_dim // num_heads\n",
    "    \n",
    "    # CORRECTION:\n",
    "    # Étape 1: Séparer embed_dim en (num_heads, d_k)\n",
    "    x = x.view(batch_size, seq_len, num_heads, d_k)\n",
    "    # Étape 2: Réorganiser pour avoir num_heads en position 1\n",
    "    x = x.transpose(1, 2)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "x_test = torch.randn(2, 6, 32)  # batch=2, seq=6, embed=32\n",
    "x_heads = split_heads(x_test, num_heads=4)\n",
    "\n",
    "print(f\"Avant split: {x_test.shape}\")   # (2, 6, 32)\n",
    "print(f\"Après split: {x_heads.shape}\")  # Attendu: (2, 4, 6, 8)\n",
    "\n",
    "if x_heads is not None and x_heads.shape == (2, 4, 6, 8):\n",
    "    print(\"\\nCorrect !\")\n",
    "else:\n",
    "    print(\"\\nVérifiez votre implémentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Récapitulatif\n",
    "\n",
    "### Ce que nous avons appris aujourd'hui\n",
    "\n",
    "| Concept | Ce qu'on a fait |\n",
    "|---------|----------------|\n",
    "| **Exercice 1** | Fonction `scaled_dot_product_attention()` complète |\n",
    "| **Exercice 2** | Classe `SelfAttention` avec projections W_q, W_k, W_v |\n",
    "| **Visualisation** | Explorer l'attention de CamemBERT avec BertViz |\n",
    "| **Multi-Head** | Comprendre pourquoi plusieurs têtes |\n",
    "| **Exercice 3** | Fonction `split_heads()` pour séparer les têtes |\n",
    "\n",
    "### Questions de compréhension\n",
    "\n",
    "Avant de passer au TP suivant, vérifiez que vous pouvez répondre à ces questions :\n",
    "\n",
    "1. **Scaling** : Que se passe-t-il si on ne divise pas par sqrt(d_k) ? Pourquoi ?\n",
    "\n",
    "2. **Softmax** : Pourquoi la somme des poids d'attention fait toujours 1 ?\n",
    "\n",
    "3. **Paramètres** : Combien de paramètres a une couche `SelfAttention` avec `embed_dim=512` ? (indice : 3 matrices W_q, W_k, W_v)\n",
    "\n",
    "4. **Multi-Head** : Si on a `embed_dim=512` et `num_heads=8`, quelle est la dimension d_k de chaque tête ?\n",
    "\n",
    "5. **Q != K** : Pourquoi utilise-t-on des matrices différentes pour Q et K au lieu de faire simplement `x @ x.T` ?\n",
    "\n",
    "<details>\n",
    "<summary>Réponses</summary>\n",
    "\n",
    "1. Les scores deviennent très grands -> softmax donne des poids proches de 0 ou 1 -> gradients instables\n",
    "\n",
    "2. C'est la propriété du softmax : il transforme des scores en probabilités qui somment à 1\n",
    "\n",
    "3. 3 x (512 x 512 + 512) = 3 x 262 656 = **787 968** paramètres (poids + biais)\n",
    "\n",
    "4. d_k = 512 / 8 = **64**\n",
    "\n",
    "5. Pour permettre au modèle de capturer d'autres relations que la similarité sémantique (relations syntaxiques, contextuelles, etc.)\n",
    "</details>\n",
    "\n",
    "### Prochaine session\n",
    "\n",
    "On va :\n",
    "1. Terminer l'implémentation Multi-Head (`concat_heads`, classe complète)\n",
    "2. Ajouter le **Feed-Forward Network**\n",
    "3. Comprendre le **masque causal** (GPT vs BERT)\n",
    "4. Assembler un **bloc Transformer** complet !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Pour aller plus loin (optionnel)\n",
    "\n",
    "### 9.1 Explorer toutes les têtes d'une couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser 4 têtes différentes de la même couche\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "heads_to_show = [0, 3, 7, 11]\n",
    "layer = 8  # Couche 9\n",
    "\n",
    "for idx, head in enumerate(heads_to_show):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    w = attentions[layer][0, head].numpy()\n",
    "    \n",
    "    im = ax.imshow(w, cmap='Blues')\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=8)\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_yticklabels(tokens, fontsize=8)\n",
    "    ax.set_title(f\"Tête {head + 1}\", fontsize=11)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle(f\"Différentes têtes de la couche {layer+1} - CamemBERT\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse : que regarde \"il\" selon chaque tête ?\n",
    "if il_index:\n",
    "    print(f\"Que regarde '{tokens[il_index]}' selon chaque tête de la couche {layer+1} ?\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for head in heads_to_show:\n",
    "        weights = attentions[layer][0, head, il_index].numpy()\n",
    "        top_idx = weights.argsort()[-3:][::-1]  # Top 3\n",
    "        print(f\"Tête {head+1:2d}: \", end=\"\")\n",
    "        for i in top_idx:\n",
    "            print(f\"{tokens[i]} ({weights[i]:.2f})  \", end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Comparer l'attention sur différentes phrases\n",
    "\n",
    "Testez avec vos propres phrases Pokémon pour voir comment l'attention change !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre phrase ici !\n",
    "ma_phrase = \"Mewtwo est le Pokémon le plus puissant car il a été créé génétiquement\"\n",
    "\n",
    "analyser_attention_pokemon(ma_phrase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
