{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Session 3 - Maîtriser l'Attention (CORRECTION)\n\n**Module** : Réseaux de Neurones Approfondissement  \n**Durée** : 2h  \n**Objectif** : Implémenter l'attention de A à Z et découvrir le Multi-Head\n\n---\n\n## Objectifs pédagogiques\n\nÀ la fin de cette session, vous serez capable de :\n1. Implémenter la fonction `scaled_dot_product_attention()` complète\n2. Créer une classe `SelfAttention` réutilisable en PyTorch\n3. Visualiser et interpréter l'attention d'un vrai modèle\n4. Comprendre pourquoi on utilise **plusieurs têtes** d'attention\n\n---\n\n## Rappel : Où en sommes-nous ?\n\nDans la session précédente, vous avez :\n- ✅ Compris le **Positional Encoding** (encoder l'ordre des mots)\n- ✅ Vu le lien entre **similarité** et **produit scalaire**\n- ✅ Découvert les concepts de **Query, Key, Value**\n- ✅ Calculé les scores d'attention étape par étape\n\nAujourd'hui, on passe à l'**implémentation complète** !"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances (Google Colab)\n",
    "!pip install torch matplotlib numpy transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Rappel : La formule de l'attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "| Étape | Opération | Rôle |\n",
    "|-------|-----------|------|\n",
    "| 1 | $QK^T$ | Calculer les scores de similarité |\n",
    "| 2 | $\\div \\sqrt{d_k}$ | Stabiliser les gradients |\n",
    "| 3 | softmax | Transformer en probabilités |\n",
    "| 4 | $\\times V$ | Moyenne pondérée des values |\n",
    "\n",
    "Dans les exercices 1-4, vous avez fait ces étapes **séparément**. Maintenant, on les **combine** dans une fonction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2. Exercice 1 : Fonction d'attention complète\n\nRegroupez les 4 étapes dans une seule fonction réutilisable."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Calcule le Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries, shape (..., seq_len, d_k)\n",
    "        K: Keys, shape (..., seq_len, d_k)\n",
    "        V: Values, shape (..., seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: Résultat de l'attention, même shape que V\n",
    "        attention_weights: Poids d'attention, shape (..., seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # CORRECTION 1: Récupérer d_k (dernière dimension de K)\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # CORRECTION 2: Calculer les scores QK^T\n",
    "    scores = Q @ K.transpose(-2, -1)\n",
    "    \n",
    "    # CORRECTION 3: Appliquer le scaling (diviser par sqrt(d_k))\n",
    "    scaled_scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # CORRECTION 4: Appliquer softmax sur la dernière dimension\n",
    "    attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "    \n",
    "    # CORRECTION 5: Calculer la sortie (weights @ V)\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre fonction\n",
    "Q_test = torch.randn(4, 8)  # 4 tokens, dimension 8\n",
    "K_test = torch.randn(4, 8)\n",
    "V_test = torch.randn(4, 8)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q_test, K_test, V_test)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")    # Attendu: (4, 8)\n",
    "print(f\"Weights shape: {weights.shape}\")  # Attendu: (4, 4)\n",
    "print(f\"Somme des poids par ligne: {weights.sum(dim=-1)}\")  # Attendu: [1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vérification : Effet du scaling\n",
    "\n",
    "Pourquoi diviser par $\\sqrt{d_k}$ ? Voyons l'effet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison avec et sans scaling\n",
    "d_k_grand = 512  # Dimension typique dans un Transformer\n",
    "\n",
    "Q_grand = torch.randn(10, d_k_grand)\n",
    "K_grand = torch.randn(10, d_k_grand)\n",
    "\n",
    "# Sans scaling\n",
    "scores_sans = Q_grand @ K_grand.T\n",
    "attention_sans = F.softmax(scores_sans, dim=-1)\n",
    "\n",
    "# Avec scaling\n",
    "scores_avec = (Q_grand @ K_grand.T) / math.sqrt(d_k_grand)\n",
    "attention_avec = F.softmax(scores_avec, dim=-1)\n",
    "\n",
    "print(\"=== SANS SCALING ===\")\n",
    "print(f\"Scores - min: {scores_sans.min():.1f}, max: {scores_sans.max():.1f}\")\n",
    "print(f\"Attention max par ligne: {attention_sans.max(dim=-1).values[:5]}\")\n",
    "\n",
    "print(\"\\n=== AVEC SCALING ===\")\n",
    "print(f\"Scores - min: {scores_avec.min():.1f}, max: {scores_avec.max():.1f}\")\n",
    "print(f\"Attention max par ligne: {attention_avec.max(dim=-1).values[:5]}\")\n",
    "\n",
    "print(\"\\n✅ Avec scaling, l'attention est mieux répartie (pas de valeur proche de 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. Exercice 2 : Classe SelfAttention\n\n### D'où viennent Q, K, V ?\n\nJusqu'ici, on a utilisé des tenseurs aléatoires. En pratique, **Q, K, V sont calculés à partir des embeddings** via des matrices apprenables.\n\n```\nx (embeddings) ──┬──► W_q ──► Q   (ce que je cherche)\n                 ├──► W_k ──► K   (mon identité)\n                 └──► W_v ──► V   (mon contenu)\n```\n\n**Pourquoi 3 matrices différentes ?**\n\nUn même mot a besoin de **3 représentations** selon son rôle :\n- **Query** : \"Qu'est-ce que je cherche ?\"\n- **Key** : \"Comment les autres me voient ?\"\n- **Value** : \"Quelle information je transmets ?\"\n\nCes matrices sont **apprises** pendant l'entraînement."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Module de Self-Attention.\n",
    "    \n",
    "    Projette l'input x vers Q, K, V puis applique l'attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: Dimension des embeddings d'entrée\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # CORRECTION 1: Créer 3 couches linéaires pour projeter vers Q, K, V\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Embeddings, shape (batch, seq_len, embed_dim)\n",
    "        \n",
    "        Returns:\n",
    "            output: Résultat de l'attention\n",
    "            attention_weights: Poids d'attention\n",
    "        \"\"\"\n",
    "        # CORRECTION 2: Projeter x vers Q, K, V\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # CORRECTION 3: Appliquer la fonction scaled_dot_product_attention\n",
    "        output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du module\n",
    "embed_dim = 32\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "attention_layer = SelfAttention(embed_dim)\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "output, weights = attention_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")   # Attendu: (2, 5, 32)\n",
    "print(f\"Weights shape: {weights.shape}\") # Attendu: (2, 5, 5)\n",
    "\n",
    "# Compter les paramètres\n",
    "n_params = sum(p.numel() for p in attention_layer.parameters())\n",
    "print(f\"\\nNombre de paramètres: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Visualisation sur un vrai modèle\n",
    "\n",
    "Maintenant qu'on a compris et implémenté l'attention, regardons ce que ça donne sur un modèle **réellement entraîné**.\n",
    "\n",
    "### Tokens spéciaux\n",
    "\n",
    "Les modèles BERT ajoutent des tokens spéciaux :\n",
    "\n",
    "| Token | Rôle |\n",
    "|-------|------|\n",
    "| **[CLS]** | Début de phrase. Son vecteur représente toute la phrase. |\n",
    "| **[SEP]** | Fin de phrase / séparateur. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Charger DistilBERT\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "model.eval()\n",
    "\n",
    "# Phrase de test (en anglais pour ce modèle)\n",
    "phrase = \"The cat sat on the mat because it was tired\"\n",
    "\n",
    "# Tokeniser\n",
    "inputs = tokenizer(phrase, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "attentions = outputs.attentions\n",
    "\n",
    "print(f\"Phrase: {phrase}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"\\nNombre de couches: {len(attentions)}\")\n",
    "print(f\"Nombre de têtes par couche: {attentions[0].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'attention d'une tête spécifique\n",
    "# Couche 5, Tête 2 : capture bien la coréférence \"it\" → \"cat\"\n",
    "layer = 4\n",
    "head = 1\n",
    "\n",
    "attention_matrix = attentions[layer][0, head].numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_matrix, cmap='Blues')\n",
    "plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "plt.yticks(range(len(tokens)), tokens)\n",
    "plt.xlabel(\"Tokens regardés (Keys)\")\n",
    "plt.ylabel(\"Tokens qui regardent (Queries)\")\n",
    "plt.title(f\"Attention réelle - Couche {layer+1}, Tête {head+1}\")\n",
    "plt.colorbar(label=\"Poids d'attention\")\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens)):\n",
    "        val = attention_matrix[i, j]\n",
    "        plt.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                color='white' if val > 0.3 else 'black', fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Que regarde le pronom \"it\" ?\n",
    "it_index = tokens.index(\"it\")\n",
    "\n",
    "print(f\"Attention de 'it' (Couche {layer+1}, Tête {head+1}) :\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for token, weight in zip(tokens, attention_matrix[it_index]):\n",
    "    bar = \"█\" * int(weight * 30)\n",
    "    highlight = \" ← antécédent !\" if token == \"cat\" else \"\"\n",
    "    print(f\"  {token:10} {weight:.2f} {bar}{highlight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Le pronom \"it\" regarde principalement \"cat\" → le modèle a appris la **coréférence** !\n",
    "\n",
    "Mais cette tête ne capture qu'**un type de relation**. Pour capturer plusieurs types de relations (syntaxe, sémantique, proximité...), on utilise **plusieurs têtes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Pourquoi Multi-Head Attention ?\n",
    "\n",
    "### Le problème avec une seule tête\n",
    "\n",
    "Une seule tête d'attention calcule **une** représentation des relations entre mots.\n",
    "\n",
    "Mais dans une phrase, il y a **plusieurs types de relations** :\n",
    "- Relations **syntaxiques** (sujet-verbe)\n",
    "- Relations **sémantiques** (sens, coréférence)\n",
    "- Relations de **proximité** (mots voisins)\n",
    "- etc.\n",
    "\n",
    "### La solution : plusieurs têtes\n",
    "\n",
    "Chaque tête peut apprendre à détecter un type de relation différent !\n",
    "\n",
    "**Analogie** : C'est comme avoir plusieurs experts qui analysent une phrase sous différents angles, puis combinent leurs analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration : différentes têtes capturent différentes relations\n",
    "phrase_demo = [\"Le\", \"chat\", \"noir\", \"mange\", \"la\", \"souris\"]\n",
    "\n",
    "# Tête 1 : Relations syntaxiques (sujet-verbe)\n",
    "attention_syntaxe = torch.tensor([\n",
    "    [0.3, 0.5, 0.1, 0.05, 0.03, 0.02],\n",
    "    [0.1, 0.3, 0.1, 0.4, 0.05, 0.05],\n",
    "    [0.1, 0.6, 0.2, 0.05, 0.03, 0.02],\n",
    "    [0.05, 0.5, 0.05, 0.2, 0.1, 0.1],\n",
    "    [0.02, 0.03, 0.02, 0.03, 0.3, 0.6],\n",
    "    [0.02, 0.1, 0.02, 0.4, 0.06, 0.4],\n",
    "])\n",
    "\n",
    "# Tête 2 : Relations de proximité\n",
    "attention_proximite = torch.tensor([\n",
    "    [0.5, 0.4, 0.08, 0.01, 0.005, 0.005],\n",
    "    [0.3, 0.4, 0.25, 0.04, 0.005, 0.005],\n",
    "    [0.1, 0.35, 0.35, 0.15, 0.03, 0.02],\n",
    "    [0.02, 0.1, 0.3, 0.35, 0.18, 0.05],\n",
    "    [0.01, 0.02, 0.05, 0.25, 0.4, 0.27],\n",
    "    [0.005, 0.01, 0.02, 0.1, 0.35, 0.515],\n",
    "])\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (attn, title) in enumerate([\n",
    "    (attention_syntaxe, \"Tête 1 : Relations syntaxiques\\n(chat → mange)\"),\n",
    "    (attention_proximite, \"Tête 2 : Proximité\\n(mots voisins)\")\n",
    "]):\n",
    "    ax = axes[idx]\n",
    "    im = ax.imshow(attn, cmap='Blues')\n",
    "    ax.set_xticks(range(6))\n",
    "    ax.set_xticklabels(phrase_demo, rotation=45)\n",
    "    ax.set_yticks(range(6))\n",
    "    ax.set_yticklabels(phrase_demo)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle(\"Chaque tête capture des relations différentes\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Architecture Multi-Head Attention\n",
    "\n",
    "### Schéma\n",
    "\n",
    "```\n",
    "Input (seq_len, embed_dim)\n",
    "        ↓\n",
    "   ┌────┴────┬────────┬────────┐\n",
    "   ↓         ↓        ↓        ↓\n",
    " Head 1   Head 2   Head 3   Head 4\n",
    "   ↓         ↓        ↓        ↓\n",
    "   └────┬────┴────────┴────────┘\n",
    "        ↓\n",
    "    Concat\n",
    "        ↓\n",
    "   Linear (W_o)\n",
    "        ↓\n",
    "Output (seq_len, embed_dim)\n",
    "```\n",
    "\n",
    "### Dimensions\n",
    "\n",
    "| Paramètre | Exemple | Description |\n",
    "|-----------|---------|-------------|\n",
    "| embed_dim | 512 | Dimension des embeddings |\n",
    "| num_heads | 8 | Nombre de têtes |\n",
    "| d_k | 64 | Dimension par tête = embed_dim / num_heads |\n",
    "\n",
    "Chaque tête travaille avec une **dimension réduite** ($d_k$), puis on **concatène** les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Exercice 3 : split_heads (Multi-Head)\n\nLa première étape du Multi-Head est de **séparer les têtes**.\n\nOn transforme `(batch, seq_len, embed_dim)` en `(batch, num_heads, seq_len, d_k)`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "embed_dim = 32\n",
    "num_heads = 4\n",
    "d_k = embed_dim // num_heads  # 32 / 4 = 8\n",
    "\n",
    "print(f\"embed_dim: {embed_dim}\")\n",
    "print(f\"num_heads: {num_heads}\")\n",
    "print(f\"d_k (dim par tête): {d_k}\")\n",
    "\n",
    "# Input\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "print(f\"\\nInput shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, num_heads):\n",
    "    \"\"\"\n",
    "    Sépare les têtes d'attention.\n",
    "    \n",
    "    Reshape: (batch, seq_len, embed_dim) -> (batch, num_heads, seq_len, d_k)\n",
    "    \n",
    "    Args:\n",
    "        x: Tensor de shape (batch, seq_len, embed_dim)\n",
    "        num_heads: Nombre de têtes\n",
    "    \n",
    "    Returns:\n",
    "        Tensor de shape (batch, num_heads, seq_len, d_k)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, embed_dim = x.shape\n",
    "    d_k = embed_dim // num_heads\n",
    "    \n",
    "    # CORRECTION:\n",
    "    # Étape 1: Séparer embed_dim en (num_heads, d_k)\n",
    "    x = x.view(batch_size, seq_len, num_heads, d_k)\n",
    "    # Étape 2: Réorganiser pour avoir num_heads en position 1\n",
    "    x = x.transpose(1, 2)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "x_test = torch.randn(2, 6, 32)  # batch=2, seq=6, embed=32\n",
    "x_heads = split_heads(x_test, num_heads=4)\n",
    "\n",
    "print(f\"Avant split: {x_test.shape}\")   # (2, 6, 32)\n",
    "print(f\"Après split: {x_heads.shape}\")  # Attendu: (2, 4, 6, 8)\n",
    "\n",
    "if x_heads is not None and x_heads.shape == (2, 4, 6, 8):\n",
    "    print(\"\\n✅ Correct !\")\n",
    "else:\n",
    "    print(\"\\n❌ Vérifiez votre implémentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Récapitulatif\n",
    "\n",
    "### Ce que nous avons appris aujourd'hui\n",
    "\n",
    "| Concept | Ce qu'on a fait |\n",
    "|---------|----------------|\n",
    "| **Exercice 5** | Fonction `scaled_dot_product_attention()` complète |\n",
    "| **Exercice 6** | Classe `SelfAttention` avec projections W_q, W_k, W_v |\n",
    "| **Visualisation** | Voir ce que l'attention capture sur DistilBERT |\n",
    "| **Multi-Head** | Comprendre pourquoi plusieurs têtes |\n",
    "| **split_heads** | Première étape de l'implémentation Multi-Head |\n",
    "\n",
    "### Prochaine session\n",
    "\n",
    "On va :\n",
    "1. Terminer l'implémentation Multi-Head (`concat_heads`, classe complète)\n",
    "2. Ajouter le **Feed-Forward Network**\n",
    "3. Comprendre le **masque causal** (GPT vs BERT)\n",
    "4. Assembler un **bloc Transformer** complet !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Pour aller plus loin (optionnel)\n",
    "\n",
    "### Visualiser plusieurs têtes de DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser 4 têtes différentes de la même couche\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "heads_to_show = [0, 1, 5, 10]\n",
    "layer = 4\n",
    "\n",
    "for idx, head in enumerate(heads_to_show):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    w = attentions[layer][0, head].numpy()\n",
    "    \n",
    "    im = ax.imshow(w, cmap='Blues')\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=8)\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_yticklabels(tokens, fontsize=8)\n",
    "    ax.set_title(f\"Tête {head + 1}\", fontsize=11)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle(f\"Différentes têtes de la couche {layer+1}\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyse : que regarde \"it\" selon chaque tête ?\n",
    "print(f\"\\nQue regarde 'it' selon chaque tête ?\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for head in heads_to_show:\n",
    "    weights = attentions[layer][0, head, it_index].numpy()\n",
    "    top_idx = weights.argsort()[-2:][::-1]  # Top 2\n",
    "    print(f\"Tête {head+1:2d}: \", end=\"\")\n",
    "    for i in top_idx:\n",
    "        print(f\"{tokens[i]} ({weights[i]:.2f})  \", end=\"\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}