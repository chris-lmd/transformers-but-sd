{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3 - Maîtriser l'Attention\n",
    "\n",
    "**Module** : Réseaux de Neurones Approfondissement  \n",
    "**Durée** : 2h  \n",
    "**Objectif** : Implémenter l'attention de A à Z et découvrir le Multi-Head\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs pédagogiques\n",
    "\n",
    "À la fin de cette session, vous serez capable de :\n",
    "1. Implémenter la fonction `scaled_dot_product_attention()` complète\n",
    "2. Créer une classe `SelfAttention` réutilisable en PyTorch\n",
    "3. Visualiser et interpréter l'attention d'un vrai modèle (CamemBERT)\n",
    "4. Comprendre pourquoi on utilise **plusieurs têtes** d'attention\n",
    "\n",
    "---\n",
    "\n",
    "## Rappel : Où en sommes-nous ?\n",
    "\n",
    "Dans la session précédente, vous avez :\n",
    "- Compris le **Positional Encoding** (encoder l'ordre des mots)\n",
    "- Vu le lien entre **similarité** et **produit scalaire**\n",
    "- Découvert les concepts de **Query, Key, Value**\n",
    "   - Q : Ce que je cherche\n",
    "   - K : Les étiquettes\n",
    "   - V : Le contenu\n",
    "- Calculé les scores d'attention étape par étape\n",
    "\n",
    "Aujourd'hui, on passe à l'**implémentation complète** !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances (Google Colab)\n",
    "!pip install torch matplotlib numpy transformers bertviz -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Rappel : La formule de l'attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "| Étape | Opération | Rôle |\n",
    "|-------|-----------|------|\n",
    "| 1 | $QK^T$ | Calculer les scores de similarité |\n",
    "| 2 | $\\div \\sqrt{d_k}$ | Stabiliser les gradients |\n",
    "| 3 | softmax | Transformer en probabilités |\n",
    "| 4 | $\\times V$ | Moyenne pondérée des values |\n",
    "\n",
    "Dans la session précédente, vous avez fait ces étapes **séparément**. Maintenant, on les **combine** dans une fonction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Exercice 1 : Fonction d'attention complète\n",
    "\n",
    "Regroupez les 4 étapes dans une seule fonction réutilisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Calcule le Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries, shape (..., seq_len, d_k)\n",
    "        K: Keys, shape (..., seq_len, d_k)\n",
    "        V: Values, shape (..., seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: Résultat de l'attention, même shape que V\n",
    "        attention_weights: Poids d'attention, shape (..., seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # CORRECTION 1: Récupérer d_k (dernière dimension de K)\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # CORRECTION 2: Calculer les scores QK^T\n",
    "    scores = Q @ K.transpose(-2, -1)\n",
    "    \n",
    "    # CORRECTION 3: Appliquer le scaling (diviser par sqrt(d_k))\n",
    "    scaled_scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # CORRECTION 4: Appliquer softmax sur la dernière dimension\n",
    "    attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "    \n",
    "    # CORRECTION 5: Calculer la sortie (weights @ V)\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre fonction\n",
    "Q_test = torch.randn(4, 8)  # 4 tokens, dimension 8\n",
    "K_test = torch.randn(4, 8)\n",
    "V_test = torch.randn(4, 8)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q_test, K_test, V_test)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")    # Attendu: (4, 8)\n",
    "print(f\"Weights shape: {weights.shape}\")  # Attendu: (4, 4)\n",
    "print(f\"Somme des poids par ligne: {weights.sum(dim=-1)}\")  # Attendu: [1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vérification : Effet du scaling\n",
    "\n",
    "Pourquoi diviser par $\\sqrt{d_k}$ ? Voyons l'effet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Comparaison avec et sans scaling\nd_k_grand = 512  # Dimension typique dans un Transformer\n\nQ_grand = torch.randn(10, d_k_grand)\nK_grand = torch.randn(10, d_k_grand)\n\n# Sans scaling\nscores_sans = Q_grand @ K_grand.T\npoids_sans = F.softmax(scores_sans, dim=-1)\n\n# Avec scaling\nscores_avec = (Q_grand @ K_grand.T) / math.sqrt(d_k_grand)\npoids_avec = F.softmax(scores_avec, dim=-1)\n\nprint(\"=== SANS SCALING ===\")\nprint(f\"Scores - min: {scores_sans.min():.1f}, max: {scores_sans.max():.1f}\")\nprint(f\"Poids d'attention max par ligne: {poids_sans.max(dim=-1).values[:5]}\")\n\nprint(\"\\n=== AVEC SCALING ===\")\nprint(f\"Scores - min: {scores_avec.min():.1f}, max: {scores_avec.max():.1f}\")\nprint(f\"Poids d'attention max par ligne: {poids_avec.max(dim=-1).values[:5]}\")\n\nprint(\"\\nAvec scaling, les poids sont mieux répartis (pas de valeur proche de 1)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Exercice 2 : Classe SelfAttention\n",
    "\n",
    "### D'où viennent Q, K, V ?\n",
    "\n",
    "Jusqu'ici, on a utilisé des tenseurs aléatoires. En pratique, **Q, K, V sont calculés à partir des embeddings** via des matrices apprenables.\n",
    "\n",
    "```\n",
    "x (embeddings) ──┬──► W_q ──► Q   (ce que je cherche)\n",
    "                 ├──► W_k ──► K   (comment je me présente)\n",
    "                 └──► W_v ──► V   (l'info que je transmets)\n",
    "```\n",
    "\n",
    "### Pourquoi 3 matrices différentes ?\n",
    "\n",
    "Si on utilisait directement `x @ x.T`, on calculerait les **similarités sémantiques** entre mots. \"Pikachu\" serait attentif à \"Raichu\", \"électrique\"...\n",
    "\n",
    "Avec des projections différentes (W_q, W_k, W_v), le modèle peut capturer d'autres types de relations : **syntaxiques** (sujet-verbe), **contextuelles**, etc.\n",
    "\n",
    "Les matrices sont **apprises** pendant l'entraînement : elles s'ajustent pour que la formule `softmax(Q @ K.T) @ V` produise des résultats utiles pour la tâche.\n",
    "\n",
    "> **Pour approfondir** : voir la note *\"Comprendre le Mécanisme d'Attention\"* qui détaille le rôle de Q, K, V et comment les matrices apprennent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Module de Self-Attention.\n",
    "    \n",
    "    Projette l'input x vers Q, K, V puis applique l'attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: Dimension des embeddings d'entrée\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # CORRECTION 1: Créer 3 couches linéaires pour projeter vers Q, K, V\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Embeddings, shape (batch, seq_len, embed_dim)\n",
    "        \n",
    "        Returns:\n",
    "            output: Résultat de l'attention\n",
    "            attention_weights: Poids d'attention\n",
    "        \"\"\"\n",
    "        # CORRECTION 2: Projeter x vers Q, K, V\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # CORRECTION 3: Appliquer la fonction scaled_dot_product_attention\n",
    "        output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du module\n",
    "embed_dim = 32\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "attention_layer = SelfAttention(embed_dim)\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "output, weights = attention_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")   # Attendu: (2, 5, 32)\n",
    "print(f\"Weights shape: {weights.shape}\") # Attendu: (2, 5, 5)\n",
    "\n",
    "# Compter les paramètres\n",
    "n_params = sum(p.numel() for p in attention_layer.parameters())\n",
    "print(f\"\\nNombre de paramètres: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. Visualisation sur CamemBERT\n\nMaintenant qu'on a compris et implémenté l'attention, regardons ce que ça donne sur un modèle **réellement entraîné**.\n\nOn utilise **CamemBERT**, le modèle français qu'on a découvert au TP précédent.\n\n### Tokens spéciaux CamemBERT\n\n| Token | Rôle |\n|-------|------|\n| **\\<s\\>** | Début de phrase (équivalent [CLS]) |\n| **\\</s\\>** | Fin de phrase (équivalent [SEP]) |\n\n### Choix de la couche et de la tête\n\nCamemBERT a **12 couches** et **12 têtes par couche** = 144 matrices d'attention différentes !\n\nToutes ne sont pas intéressantes à visualiser. On va explorer différentes têtes pour voir lesquelles capturent la **coréférence** (le lien entre un pronom et son antécédent).\n\n### Plan de cette section\n\n1. **Phrase standard** : observer la coréférence sur du vocabulaire courant\n2. **Phrase Pokémon** : observer les limites avec du vocabulaire hors-domaine"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertModel, CamembertTokenizer\n",
    "\n",
    "# Charger CamemBERT\n",
    "print(\"Chargement de CamemBERT...\")\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model = CamembertModel.from_pretrained(\"camembert-base\", output_attentions=True)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Modèle chargé !\")\n",
    "print(f\"  - Couches : {model.config.num_hidden_layers}\")\n",
    "print(f\"  - Têtes par couche : {model.config.num_attention_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. PHRASE STANDARD - Coréférence claire\nphrase_standard = \"Le chat dort sur le canapé car il est fatigué\"\n\ninputs = tokenizer(phrase_standard, return_tensors=\"pt\")\ntokens_standard = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nattentions_standard = outputs.attentions\n\nprint(\"=== PHRASE STANDARD ===\")\nprint(f\"Phrase: {phrase_standard}\")\nprint(f\"Tokens: {tokens_standard}\")\nprint(\"\\nObservation : chaque mot = 1 token (vocabulaire courant)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.1 Visualisation de la coréférence (phrase standard)\n\nAvec une phrase standard, le pronom \"il\" devrait pointer vers \"chat\"."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention_matrix, tokens, title=\"Attention\"):\n",
    "    \"\"\"Affiche une matrice d'attention avec matplotlib.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(attention_matrix, cmap='Blues')\n",
    "    plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(tokens)), tokens)\n",
    "    plt.xlabel(\"Tokens regardés (Keys)\")\n",
    "    plt.ylabel(\"Tokens qui regardent (Queries)\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label=\"Poids d'attention\")\n",
    "    \n",
    "    # Ajouter les valeurs\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            val = attention_matrix[i, j]\n",
    "            color = 'white' if val > 0.3 else 'black'\n",
    "            plt.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                    color=color, fontsize=7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualiser l'attention sur la phrase standard\nlayer = 7  # Couche 8 (0-indexed)\nhead = 9   # Tête 10 (0-indexed)\n\nattention_matrix = attentions_standard[layer][0, head].numpy()\nplot_attention(attention_matrix, tokens_standard, f\"Phrase standard - Couche {layer+1}, Tête {head+1}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Que regarde le pronom \"il\" dans la phrase standard ?\nil_index = None\nfor i, t in enumerate(tokens_standard):\n    if t == \"▁il\":\n        il_index = i\n        break\n\nif il_index:\n    print(f\"Attention de 'il' (Couche {layer+1}, Tête {head+1}) :\")\n    print(\"-\" * 50)\n    \n    for i, (token, weight) in enumerate(zip(tokens_standard, attention_matrix[il_index])):\n        bar = \"*\" * int(weight * 30)\n        highlight = \" <-- antécédent !\" if \"chat\" in token.lower() else \"\"\n        print(f\"  {token:15} {weight:.3f} {bar}{highlight}\")\n    \n    print(\"\\n'il' devrait regarder principalement 'chat' (coréférence)\")\nelse:\n    print(\"Token 'il' non trouvé\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 Limites avec le vocabulaire Pokémon\n\nEssayons maintenant avec notre corpus Pokémon. Les noms comme \"Pikachu\" ou \"Dracaufeu\" sont **hors du vocabulaire** de CamemBERT (entraîné sur du français standard).\n\n**Conséquence** : ces mots sont découpés en sous-tokens, ce qui rend la coréférence plus difficile à capturer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 2. PHRASE POKÉMON - Vocabulaire hors-domaine\nphrase_pokemon = \"Pikachu a utilisé Tonnerre sur Dracaufeu car il était très efficace\"\n\ninputs_pokemon = tokenizer(phrase_pokemon, return_tensors=\"pt\")\ntokens_pokemon = tokenizer.convert_ids_to_tokens(inputs_pokemon[\"input_ids\"][0])\n\nwith torch.no_grad():\n    outputs_pokemon = model(**inputs_pokemon)\n\nattentions_pokemon = outputs_pokemon.attentions\n\nprint(\"=== PHRASE POKÉMON ===\")\nprint(f\"Phrase: {phrase_pokemon}\")\nprint(f\"Tokens: {tokens_pokemon}\")\nprint(f\"\\nObservation : 'Pikachu' et 'Dracaufeu' sont découpés en sous-tokens !\")\nprint(\"Le modèle n'a jamais vu ces mots pendant son entraînement.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualiser l'attention sur la phrase Pokémon\nattention_matrix_pokemon = attentions_pokemon[layer][0, head].numpy()\nplot_attention(attention_matrix_pokemon, tokens_pokemon, f\"Phrase Pokémon - Couche {layer+1}, Tête {head+1}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Que regarde \"il\" dans la phrase Pokémon ?\nil_index_pokemon = None\nfor i, t in enumerate(tokens_pokemon):\n    if \"il\" in t.lower():\n        il_index_pokemon = i\n        break\n\nif il_index_pokemon:\n    print(f\"Attention de 'il' (Couche {layer+1}, Tête {head+1}) :\")\n    print(\"-\" * 50)\n    \n    for i, (token, weight) in enumerate(zip(tokens_pokemon, attention_matrix_pokemon[il_index_pokemon])):\n        bar = \"*\" * int(weight * 30)\n        print(f\"  {token:15} {weight:.3f} {bar}\")\n    \n    print(\"\\nLa coréférence est moins claire : 'il' peut référer à Pikachu, Tonnerre, ou Dracaufeu.\")\n    print(\"De plus, ces entités sont fragmentées en sous-tokens.\")\nelse:\n    print(\"Token 'il' non trouvé\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Comment améliorer ?\n",
    "\n",
    "Le modèle a du mal avec le vocabulaire Pokémon car :\n",
    "1. **Tokenization fragmentée** : \"Pikachu\" = plusieurs sous-tokens\n",
    "2. **Embeddings non spécialisés** : le modèle n'a jamais appris ces concepts\n",
    "\n",
    "**Solutions possibles** :\n",
    "- **Fine-tuning** : ré-entraîner sur un corpus Pokémon pour adapter les embeddings\n",
    "- **Enrichir le tokenizer** : ajouter \"Pikachu\", \"Dracaufeu\" comme tokens uniques avec `tokenizer.add_tokens()`, puis fine-tuner\n",
    "\n",
    "Ces techniques seront abordées plus tard.\n",
    "\n",
    "### 4.4 Visualisation interactive avec BertViz\n",
    "\n",
    "**BertViz** est un outil spécialisé pour visualiser l'attention des Transformers.\n",
    "\n",
    "- **head_view** : voir les connexions d'attention pour une couche (carrés colorés = têtes)\n",
    "- **model_view** : vue globale de toutes les couches et têtes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from bertviz import head_view, model_view\n\n# BertViz sur la phrase standard (coréférence plus visible)\nprint(\"head_view sur la phrase standard\")\nprint(\"Cliquez sur une tête pour voir ses connexions d'attention\")\nprint(\"=\"*50)\n\nhead_view(attentions_standard, tokens_standard)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# BertViz sur la phrase Pokémon (pour comparer)\nprint(\"head_view sur la phrase Pokémon\")\nprint(\"Comparez avec la phrase standard : la coréférence est-elle aussi claire ?\")\nprint(\"=\"*50)\n\nhead_view(attentions_pokemon, tokens_pokemon)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Pourquoi Multi-Head Attention ?\n",
    "\n",
    "### Le problème avec une seule tête\n",
    "\n",
    "Une seule tête d'attention calcule **une** représentation des relations entre mots.\n",
    "\n",
    "Mais dans une phrase, il y a **plusieurs types de relations** :\n",
    "- Relations **syntaxiques** (sujet-verbe)\n",
    "- Relations **sémantiques** (sens, coréférence)\n",
    "- Relations de **proximité** (mots voisins)\n",
    "- etc.\n",
    "\n",
    "### La solution : plusieurs têtes\n",
    "\n",
    "Chaque tête peut apprendre à détecter un type de relation différent !\n",
    "\n",
    "**Attention** : En pratique, une tête ne correspond pas forcément à un type de relation précis. Une relation peut être portée par une combinaison de têtes, et une tête peut capturer plusieurs types de patterns. C'est ce que le modèle apprend pendant l'entraînement.\n",
    "\n",
    "**Analogie** : C'est comme avoir plusieurs experts qui analysent une phrase sous différents angles, puis combinent leurs analyses.\n",
    "\n",
    "### Exemple : comparons deux têtes de CamemBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Comparaison de 2 têtes sur la phrase standard\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Tête des couches profondes\nhead_deep = attentions_standard[9][0, 5].numpy()  # Couche 10, Tête 6\n\n# Tête des premières couches (attention locale)\nhead_local = attentions_standard[1][0, 0].numpy()  # Couche 2, Tête 1\n\nfor idx, (attn, title) in enumerate([\n    (head_deep, \"Couche 10, Tête 6\\n(couches profondes)\"),\n    (head_local, \"Couche 2, Tête 1\\n(attention locale)\")\n]):\n    ax = axes[idx]\n    im = ax.imshow(attn, cmap='Blues')\n    ax.set_xticks(range(len(tokens_standard)))\n    ax.set_xticklabels(tokens_standard, rotation=45, ha='right', fontsize=9)\n    ax.set_yticks(range(len(tokens_standard)))\n    ax.set_yticklabels(tokens_standard, fontsize=9)\n    ax.set_title(title, fontsize=12)\n    plt.colorbar(im, ax=ax)\n\nplt.suptitle(\"Deux têtes capturent des patterns différents\", fontsize=13, y=1.02)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Que regarde \"il\" selon chaque tête ? (phrase standard)\nil_idx = tokens_standard.index(\"▁il\") if \"▁il\" in tokens_standard else None\n\nif il_idx:\n    print(f\"Que regarde 'il' selon chaque tête ?\")\n    print(\"=\" * 50)\n    \n    print(f\"Tête profonde (C10-T6) : \", end=\"\")\n    for i, w in enumerate(head_deep[il_idx]):\n        if w > 0.08:\n            print(f\"{tokens_standard[i]}({w:.2f}) \", end=\"\")\n    print()\n    \n    print(f\"Tête locale (C2-T1) :    \", end=\"\")\n    for i, w in enumerate(head_local[il_idx]):\n        if w > 0.08:\n            print(f\"{tokens_standard[i]}({w:.2f}) \", end=\"\")\n    print()\n    \n    print(\"\\nLes couches profondes capturent mieux la coréférence (il -> chat)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Architecture Multi-Head Attention\n",
    "\n",
    "### Schéma\n",
    "\n",
    "```\n",
    "Input (seq_len, embed_dim)\n",
    "        |\n",
    "   +----+----+--------+--------+\n",
    "   |         |        |        |\n",
    "   v         v        v        v\n",
    " Head 1   Head 2   Head 3   Head 4\n",
    "   |         |        |        |\n",
    "   +----+----+--------+--------+\n",
    "        |\n",
    "    Concat\n",
    "        |\n",
    "   Linear (W_o)\n",
    "        |\n",
    "Output (seq_len, embed_dim)\n",
    "```\n",
    "\n",
    "### Dimensions\n",
    "\n",
    "| Paramètre | Exemple | Description |\n",
    "|-----------|---------|-------------|\n",
    "| embed_dim | 512 | Dimension des embeddings |\n",
    "| num_heads | 8 | Nombre de têtes |\n",
    "| d_k | 64 | Dimension par tête = embed_dim / num_heads |\n",
    "\n",
    "Chaque tête travaille avec une **dimension réduite** ($d_k$), puis on **concatène** les résultats.\n",
    "\n",
    "### Pourquoi W_o après le concat ?\n",
    "\n",
    "Après concaténation, on a un vecteur de dimension `embed_dim` (4 têtes x 64 = 512).\n",
    "\n",
    "La matrice **W_o** (output projection) permet de :\n",
    "1. **Mélanger** les informations des différentes têtes\n",
    "2. **Apprendre** comment combiner leurs \"points de vue\"\n",
    "\n",
    "Sans W_o, les têtes resteraient indépendantes. Avec W_o, le modèle peut apprendre des combinaisons utiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercice 3 : split_heads (Multi-Head)\n",
    "\n",
    "La première étape du Multi-Head est de **séparer les têtes**.\n",
    "\n",
    "On transforme `(batch, seq_len, embed_dim)` en `(batch, num_heads, seq_len, d_k)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "embed_dim = 32\n",
    "num_heads = 4\n",
    "d_k = embed_dim // num_heads  # 32 / 4 = 8\n",
    "\n",
    "print(f\"embed_dim: {embed_dim}\")\n",
    "print(f\"num_heads: {num_heads}\")\n",
    "print(f\"d_k (dim par tête): {d_k}\")\n",
    "\n",
    "# Input\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "print(f\"\\nInput shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, num_heads):\n",
    "    \"\"\"\n",
    "    Sépare les têtes d'attention.\n",
    "    \n",
    "    Reshape: (batch, seq_len, embed_dim) -> (batch, num_heads, seq_len, d_k)\n",
    "    \n",
    "    Args:\n",
    "        x: Tensor de shape (batch, seq_len, embed_dim)\n",
    "        num_heads: Nombre de têtes\n",
    "    \n",
    "    Returns:\n",
    "        Tensor de shape (batch, num_heads, seq_len, d_k)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, embed_dim = x.shape\n",
    "    d_k = embed_dim // num_heads\n",
    "    \n",
    "    # CORRECTION:\n",
    "    # Étape 1: Séparer embed_dim en (num_heads, d_k)\n",
    "    x = x.view(batch_size, seq_len, num_heads, d_k)\n",
    "    # Étape 2: Réorganiser pour avoir num_heads en position 1\n",
    "    x = x.transpose(1, 2)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "x_test = torch.randn(2, 6, 32)  # batch=2, seq=6, embed=32\n",
    "x_heads = split_heads(x_test, num_heads=4)\n",
    "\n",
    "print(f\"Avant split: {x_test.shape}\")   # (2, 6, 32)\n",
    "print(f\"Après split: {x_heads.shape}\")  # Attendu: (2, 4, 6, 8)\n",
    "\n",
    "if x_heads is not None and x_heads.shape == (2, 4, 6, 8):\n",
    "    print(\"\\nCorrect !\")\n",
    "else:\n",
    "    print(\"\\nVérifiez votre implémentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8. Récapitulatif\n\n### Ce que nous avons appris aujourd'hui\n\n| Concept | Ce qu'on a fait |\n|---------|----------------|\n| **Exercice 1** | Fonction `scaled_dot_product_attention()` complète |\n| **Exercice 2** | Classe `SelfAttention` avec projections W_q, W_k, W_v |\n| **Visualisation** | Explorer l'attention de CamemBERT avec BertViz |\n| **Multi-Head** | Comprendre pourquoi plusieurs têtes |\n| **Exercice 3** | Fonction `split_heads()` pour séparer les têtes |\n\n### Questions de compréhension\n\nAvant de passer au TP suivant, vérifiez que vous pouvez répondre à ces questions :\n\n1. **Scaling** : Que se passe-t-il si on ne divise pas par sqrt(d_k) ? Pourquoi ?\n\n2. **Softmax** : Pourquoi la somme des poids d'attention fait toujours 1 ?\n\n3. **Paramètres** : Combien de paramètres a une couche `SelfAttention` avec `embed_dim=512` ? (indice : 3 matrices W_q, W_k, W_v)\n\n4. **Multi-Head** : Si on a `embed_dim=512` et `num_heads=8`, quelle est la dimension d_k de chaque tête ?\n\n5. **Q != K** : Pourquoi utilise-t-on des matrices différentes pour Q et K au lieu de faire simplement `x @ x.T` ?\n\n<details>\n<summary>Réponses</summary>\n\n1. Les scores deviennent très grands -> softmax donne des poids proches de 0 ou 1 -> gradients instables\n\n2. Les poids d'attention sont obtenus en appliquant softmax aux scores. Par définition, softmax produit des valeurs positives qui somment à 1 (distribution de probabilités).\n\n3. 3 x (512 x 512 + 512) = 3 x 262 656 = **787 968** paramètres (poids + biais)\n\n4. d_k = 512 / 8 = **64**\n\n5. Pour permettre au modèle de capturer d'autres relations que la similarité sémantique (relations syntaxiques, contextuelles, etc.)\n</details>\n\n### Prochaine session\n\nOn va :\n1. Terminer l'implémentation Multi-Head (`concat_heads`, classe complète)\n2. Ajouter le **Feed-Forward Network**\n3. Comprendre le **masque causal** (GPT vs BERT)\n4. Assembler un **bloc Transformer** complet !"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Pour aller plus loin (optionnel)\n",
    "\n",
    "### 9.1 Explorer toutes les têtes d'une couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualiser 4 têtes différentes de la même couche (phrase standard)\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\nheads_to_show = [0, 3, 7, 11]\nlayer = 8  # Couche 9\n\nfor idx, head in enumerate(heads_to_show):\n    ax = axes[idx // 2, idx % 2]\n    w = attentions_standard[layer][0, head].numpy()\n    \n    im = ax.imshow(w, cmap='Blues')\n    ax.set_xticks(range(len(tokens_standard)))\n    ax.set_xticklabels(tokens_standard, rotation=45, ha='right', fontsize=8)\n    ax.set_yticks(range(len(tokens_standard)))\n    ax.set_yticklabels(tokens_standard, fontsize=8)\n    ax.set_title(f\"Tête {head + 1}\", fontsize=11)\n    plt.colorbar(im, ax=ax)\n\nplt.suptitle(f\"Différentes têtes de la couche {layer+1} - CamemBERT\", fontsize=13)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyse : que regarde \"il\" selon chaque tête ?\nil_idx = tokens_standard.index(\"▁il\") if \"▁il\" in tokens_standard else None\n\nif il_idx:\n    print(f\"Que regarde 'il' selon chaque tête de la couche {layer+1} ?\")\n    print(\"=\" * 60)\n    \n    for head in heads_to_show:\n        weights = attentions_standard[layer][0, head, il_idx].numpy()\n        top_idx = weights.argsort()[-3:][::-1]  # Top 3\n        print(f\"Tête {head+1:2d}: \", end=\"\")\n        for i in top_idx:\n            print(f\"{tokens_standard[i]} ({weights[i]:.2f})  \", end=\"\")\n        print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 9.2 Testez vos propres phrases\n\nUtilisez cette fonction pour analyser n'importe quelle phrase :"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def analyser_phrase(phrase):\n    \"\"\"Analyse l'attention de CamemBERT sur une phrase.\"\"\"\n    inputs = tokenizer(phrase, return_tensors=\"pt\")\n    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    print(f\"Phrase: {phrase}\")\n    print(f\"Tokens: {tokens}\\n\")\n    \n    return head_view(outputs.attentions, tokens)\n\n# Testez avec vos phrases !\nanalyser_phrase(\"Marie a appelé son frère car il était en retard\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
