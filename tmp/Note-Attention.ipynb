{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprendre le Mécanisme d'Attention\n",
    "\n",
    "**Note de synthèse**\n",
    "\n",
    "---\n",
    "\n",
    "## L'idée fondamentale\n",
    "\n",
    "L'attention répond à une question simple :\n",
    "\n",
    "> **\"Pour comprendre ce mot, quels autres mots dois-je regarder ?\"**\n",
    "\n",
    "Dans la phrase *\"Le chat qui dormait sur le canapé a sauté\"*, pour comprendre \"a sauté\", il faut regarder \"chat\" (le sujet), pas \"canapé\".\n",
    "\n",
    "L'attention permet à chaque mot de **récupérer de l'information** des autres mots de manière **pondérée**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## La formule de l'attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### D'où vient cette formule ?\n",
    "\n",
    "Elle est construite étape par étape pour répondre au besoin :\n",
    "\n",
    "| Étape | Opération | Pourquoi |\n",
    "|-------|-----------|----------|\n",
    "| 1 | $Q \\cdot K^T$ | Mesurer la **compatibilité** entre mots (produit scalaire) |\n",
    "| 2 | $\\div \\sqrt{d_k}$ | **Stabiliser** les valeurs (éviter que softmax sature) |\n",
    "| 3 | softmax | Transformer en **poids** positifs qui somment à 1 |\n",
    "| 4 | $\\times V$ | **Récupérer** l'information pondérée |\n",
    "\n",
    "C'est une façon **différentiable** de dire : \"regarde les autres mots et récupère l'info pertinente\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Implémentation de l'attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries (seq_len, d_k)\n",
    "        K: Keys (seq_len, d_k)\n",
    "        V: Values (seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: Résultat de l'attention\n",
    "        weights: Poids d'attention\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # Étape 1 : Compatibilité entre chaque paire de mots\n",
    "    scores = Q @ K.transpose(-2, -1)\n",
    "    \n",
    "    # Étape 2 : Stabilisation\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # Étape 3 : Poids normalisés\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Étape 4 : Récupération pondérée\n",
    "    output = weights @ V\n",
    "    \n",
    "    return output, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q, K, V : Qu'est-ce que c'est ?\n",
    "\n",
    "### Les noms\n",
    "\n",
    "| Notation | Nom complet | Intuition |\n",
    "|----------|-------------|----------|\n",
    "| **Q** | Query | Ce que le mot **cherche** |\n",
    "| **K** | Key | Comment le mot **se présente** aux autres |\n",
    "| **V** | Value | L'**information** que le mot transmet |\n",
    "\n",
    "### Attention : c'est une métaphore !\n",
    "\n",
    "Les matrices W_q, W_k, W_v n'ont pas \"conscience\" d'être des queries, keys ou values. Ces noms sont une **analogie** pour comprendre l'architecture, pas une description de ce que les matrices apprennent.\n",
    "\n",
    "### D'où viennent Q, K, V ?\n",
    "\n",
    "Ils sont calculés à partir des **embeddings** via des matrices apprenables :\n",
    "\n",
    "```\n",
    "x (embedding du mot)\n",
    "    │\n",
    "    ├──► W_q @ x ──► Q\n",
    "    ├──► W_k @ x ──► K  \n",
    "    └──► W_v @ x ──► V\n",
    "```\n",
    "\n",
    "W_q, W_k, W_v sont des matrices de **projection** initialisées aléatoirement, puis ajustées pendant l'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Les dimensions : clarification\n",
    "\n",
    "### Trois concepts différents\n",
    "\n",
    "| Concept | Notation | Exemple | C'est quoi |\n",
    "|---------|----------|---------|------------|\n",
    "| **Vocabulaire** | vocab_size | 50 000 | Nombre total de mots/tokens distincts |\n",
    "| **Dimension d'embedding** | d_model | 512 | Taille du vecteur représentant UN mot |\n",
    "| **Longueur de séquence** | seq_len | 10 | Nombre de mots dans la phrase |\n",
    "\n",
    "### Pour UN mot\n",
    "\n",
    "```\n",
    "embedding de \"chat\" : [0.1, 0.3, ..., 0.2]   ← 512 nombres (d_model)\n",
    "Q de \"chat\"         : [0.4, 0.1, ..., 0.8]   ← 512 nombres (d_model)\n",
    "K de \"chat\"         : [0.2, 0.5, ..., 0.3]   ← 512 nombres (d_model)\n",
    "V de \"chat\"         : [0.7, 0.2, ..., 0.1]   ← 512 nombres (d_model)\n",
    "```\n",
    "\n",
    "Chaque mot a UN vecteur embedding, UNE query, UNE key, UNE value. Tous de même dimension (d_model).\n",
    "\n",
    "### Pour une phrase de 3 mots\n",
    "\n",
    "On empile les vecteurs :\n",
    "\n",
    "```\n",
    "X (embeddings) : shape (3, 512)\n",
    "Q (queries)    : shape (3, 512)   ← 3 queries de dim 512\n",
    "K (keys)       : shape (3, 512)   ← 3 keys de dim 512\n",
    "V (values)     : shape (3, 512)   ← 3 values de dim 512\n",
    "```\n",
    "\n",
    "### Où intervient le vocabulaire ?\n",
    "\n",
    "Uniquement au **début** (embedding lookup) et à la **fin** (projection vers le vocabulaire) du modèle. L'attention ne voit **jamais** le vocabulaire directement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démonstration des dimensions\n",
    "vocab_size = 50000  # Nombre de mots distincts\n",
    "d_model = 512       # Dimension d'un embedding\n",
    "seq_len = 3         # Nombre de mots dans la phrase\n",
    "\n",
    "# Simulation\n",
    "X = torch.randn(seq_len, d_model)  # 3 embeddings de dim 512\n",
    "\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "Q = W_q(X)  # (3, 512)\n",
    "K = W_k(X)  # (3, 512)\n",
    "V = W_v(X)  # (3, 512)\n",
    "\n",
    "print(f\"Vocabulaire     : {vocab_size} mots distincts\")\n",
    "print(f\"Dimension (d)   : {d_model}\")\n",
    "print(f\"Séquence        : {seq_len} mots\")\n",
    "print()\n",
    "print(f\"X shape : {X.shape}  ← {seq_len} embeddings de dim {d_model}\")\n",
    "print(f\"Q shape : {Q.shape}  ← {seq_len} queries de dim {d_model}\")\n",
    "print(f\"K shape : {K.shape}  ← {seq_len} keys de dim {d_model}\")\n",
    "print(f\"V shape : {V.shape}  ← {seq_len} values de dim {d_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q, K, V concernent quels mots ?\n",
    "\n",
    "**Tous les mots à la fois.**\n",
    "\n",
    "Pour la phrase \"Le chat mange\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple concret\n",
    "mots = [\"Le\", \"chat\", \"mange\"]\n",
    "seq_len = len(mots)\n",
    "d_model = 8  # Petite dimension pour l'exemple\n",
    "\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(seq_len, d_model)\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "Q = W_q(X)\n",
    "K = W_k(X)\n",
    "V = W_v(X)\n",
    "\n",
    "print(\"Chaque mot a sa propre Query, Key, Value :\")\n",
    "print(\"=\" * 50)\n",
    "for i, mot in enumerate(mots):\n",
    "    print(f\"\\n{mot}:\")\n",
    "    print(f\"  Q[{i}] = {Q[i][:4].tolist()}...  (ce que '{mot}' cherche)\")\n",
    "    print(f\"  K[{i}] = {K[i][:4].tolist()}...  (comment '{mot}' se présente)\")\n",
    "    print(f\"  V[{i}] = {V[i][:4].tolist()}...  (info que '{mot}' transmet)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Le produit Q @ K.T : que calcule-t-on ?\n",
    "\n",
    "Le produit `Q @ K.T` donne une matrice de **scores de compatibilité** entre chaque paire de mots.\n",
    "\n",
    "```\n",
    "scores[i, j] = Q[i] · K[j] = \"À quel point le mot i est attentif au mot j\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des scores\n",
    "scores = Q @ K.T\n",
    "\n",
    "print(\"Matrice des scores (Q @ K.T) :\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(f\"{'':>10}\", end=\"\")\n",
    "for mot in mots:\n",
    "    print(f\"{mot:>10}\", end=\"\")\n",
    "print(\"   ← Keys (comment ils se présentent)\")\n",
    "print()\n",
    "\n",
    "for i, mot in enumerate(mots):\n",
    "    print(f\"{mot:>10}\", end=\"\")\n",
    "    for j in range(len(mots)):\n",
    "        print(f\"{scores[i,j].item():>10.2f}\", end=\"\")\n",
    "    if i == 0:\n",
    "        print(\"   ← Queries\")\n",
    "    elif i == 1:\n",
    "        print(\"      (ce qu'ils cherchent)\")\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "print()\n",
    "print(\"Lecture : scores[1,2] = compatibilité entre Q['chat'] et K['mange']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de l'attention complète\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(\"Poids d'attention (après softmax) :\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "\n",
    "for i, mot in enumerate(mots):\n",
    "    print(f\"{mot} regarde : \", end=\"\")\n",
    "    for j, mot_j in enumerate(mots):\n",
    "        poids = weights[i, j].item()\n",
    "        bar = \"█\" * int(poids * 20)\n",
    "        print(f\"{mot_j}({poids:.2f}){bar}  \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(weights.detach().numpy(), cmap='Blues')\n",
    "plt.xticks(range(len(mots)), mots)\n",
    "plt.yticks(range(len(mots)), mots)\n",
    "plt.xlabel(\"Mots regardés (Keys)\")\n",
    "plt.ylabel(\"Mots qui regardent (Queries)\")\n",
    "plt.title(\"Poids d'attention\")\n",
    "plt.colorbar(label=\"Poids\")\n",
    "\n",
    "for i in range(len(mots)):\n",
    "    for j in range(len(mots)):\n",
    "        val = weights[i, j].item()\n",
    "        color = 'white' if val > 0.5 else 'black'\n",
    "        plt.text(j, i, f'{val:.2f}', ha='center', va='center', color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pourquoi 3 matrices différentes (W_q, W_k, W_v) ?\n",
    "\n",
    "### Pourquoi ne pas faire Q = K = V = x ?\n",
    "\n",
    "Si on utilisait directement les embeddings sans projection :\n",
    "\n",
    "```\n",
    "scores = x @ x.T\n",
    "```\n",
    "\n",
    "On calculerait les **similarités sémantiques brutes** entre mots. \"Chat\" serait attentif à \"chien\", \"félin\", etc.\n",
    "\n",
    "Mais ce qu'on veut souvent, c'est capturer des relations **syntaxiques** (sujet-verbe) ou **contextuelles**, pas seulement sémantiques.\n",
    "\n",
    "### Avec 3 matrices différentes\n",
    "\n",
    "Les projections permettent de transformer les embeddings dans des espaces où d'**autres types de relations** deviennent visibles.\n",
    "\n",
    "Le modèle peut apprendre :\n",
    "- W_q : projeter pour \"chercher\" certains patterns\n",
    "- W_k : projeter pour \"être trouvé\" par certains patterns\n",
    "- W_v : projeter pour \"transmettre\" certaines informations\n",
    "\n",
    "Ces rôles ne sont pas programmés, ils **émergent** de l'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comment les matrices apprennent-elles ?\n",
    "\n",
    "### Ce n'est pas magique !\n",
    "\n",
    "Les matrices W_q, W_k, W_v sont ajustées par **backpropagation**, comme tous les autres poids du réseau.\n",
    "\n",
    "### L'architecture comme \"moule\"\n",
    "\n",
    "L'idée clé :\n",
    "\n",
    "1. **On définit une architecture** avec une contrainte structurelle :\n",
    "   ```\n",
    "   output = softmax(Q @ K.T / √d) @ V\n",
    "   ```\n",
    "\n",
    "2. **Les matrices sont dans un \"moule\"** : Q est toujours à gauche, K toujours transposé à droite, V toujours à la fin.\n",
    "\n",
    "3. **L'entraînement ajuste les matrices** pour que cette formule produise des résultats utiles pour la tâche.\n",
    "\n",
    "4. **Les propriétés émergent** : les matrices acquièrent des propriétés qui \"fonctionnent\" dans leur position.\n",
    "\n",
    "### Analogie\n",
    "\n",
    "C'est comme une **clé et une serrure** :\n",
    "- La forme de la clé (W_q) et la forme de la serrure (W_k) s'ajustent **ensemble**\n",
    "- On ne dit pas que la clé \"apprend à être une clé\"\n",
    "- Mais sa forme s'affine pour fonctionner avec la serrure\n",
    "\n",
    "Le \"rôle\" de Q, K, V est imposé par leur **position dans la formule**, pas par ce qu'ils \"savent\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## L'attention dans le modèle complet\n",
    "\n",
    "### L'attention n'a pas de loss propre !\n",
    "\n",
    "L'attention est une **brique** dans un pipeline plus grand. La loss est calculée à la **fin** du modèle.\n",
    "\n",
    "### Flux complet (exemple : prédire le mot suivant)\n",
    "\n",
    "```\n",
    "\"Le chat mange\" (input)\n",
    "       ↓\n",
    "   Token IDs : [42, 1337, 856]\n",
    "       ↓\n",
    "   Embedding lookup (matrice vocab_size × d_model)\n",
    "       ↓\n",
    "   Embeddings : (3, 512)\n",
    "       ↓\n",
    "   + Positional Encoding\n",
    "       ↓\n",
    "┌─────────────────────────────────┐\n",
    "│  Bloc Transformer (× N fois)   │\n",
    "│  ┌───────────────────────────┐ │\n",
    "│  │ Self-Attention            │ │  ← Q, K, V sont ici\n",
    "│  └───────────────────────────┘ │\n",
    "│  ┌───────────────────────────┐ │\n",
    "│  │ Feed-Forward              │ │\n",
    "│  └───────────────────────────┘ │\n",
    "└─────────────────────────────────┘\n",
    "       ↓\n",
    "   Projection (d_model → vocab_size)\n",
    "       ↓\n",
    "   Softmax → Probabilités sur le vocabulaire\n",
    "       ↓\n",
    "   Prédiction : \"la\" (mot le plus probable)\n",
    "```\n",
    "\n",
    "### Comment ça s'entraîne\n",
    "\n",
    "```\n",
    "1. Forward : \"Le chat mange\" → modèle → P(\"la\") = 0.1\n",
    "\n",
    "2. Loss : -log(P(\"la\")) = -log(0.1) = 2.3  (erreur élevée)\n",
    "\n",
    "3. Backpropagation :\n",
    "   Loss → ∂/∂(projection) → ∂/∂(FFN) → ∂/∂(W_v) → ∂/∂(W_k) → ∂/∂(W_q) → ...\n",
    "\n",
    "4. Mise à jour :\n",
    "   W_q = W_q - lr × ∂Loss/∂W_q\n",
    "   W_k = W_k - lr × ∂Loss/∂W_k\n",
    "   W_v = W_v - lr × ∂Loss/∂W_v\n",
    "```\n",
    "\n",
    "Les gradients de la loss **remontent** jusqu'aux matrices d'attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Self-Attention vs Cross-Attention\n",
    "\n",
    "### Self-Attention (ce qu'on a vu)\n",
    "\n",
    "Q, K, V viennent de la **même** séquence :\n",
    "\n",
    "```python\n",
    "Q = W_q @ X  # X = embeddings de \"Le chat mange\"\n",
    "K = W_k @ X  # même X\n",
    "V = W_v @ X  # même X\n",
    "```\n",
    "\n",
    "Chaque mot regarde les autres mots **de la même phrase**.\n",
    "\n",
    "### Cross-Attention (traduction, etc.)\n",
    "\n",
    "Q vient d'une séquence, K et V d'une **autre** :\n",
    "\n",
    "```python\n",
    "# Traduction anglais → français\n",
    "Q = W_q @ X_français  # \"Le chat mange\"\n",
    "K = W_k @ X_anglais   # \"The cat eats\"\n",
    "V = W_v @ X_anglais   # \"The cat eats\"\n",
    "```\n",
    "\n",
    "Le décodeur français \"interroge\" l'encodeur anglais pour savoir quels mots anglais regarder.\n",
    "\n",
    "### Cas d'usage\n",
    "\n",
    "| Type | Usage |\n",
    "|------|-------|\n",
    "| **Self-Attention** | GPT, BERT, comprendre une phrase |\n",
    "| **Cross-Attention** | Traduction, question-réponse, image captioning |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pourquoi diviser par √d_k ?\n",
    "\n",
    "### Le problème\n",
    "\n",
    "Quand d_k est grand, les produits scalaires Q·K deviennent grands (en valeur absolue).\n",
    "\n",
    "Softmax sur des grandes valeurs → distribution très \"piquée\" (une valeur proche de 1, les autres proches de 0).\n",
    "\n",
    "Conséquence : gradients très petits → apprentissage bloqué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démonstration : effet du scaling\n",
    "d_k = 512  # Grande dimension\n",
    "\n",
    "torch.manual_seed(42)\n",
    "Q = torch.randn(5, d_k)\n",
    "K = torch.randn(5, d_k)\n",
    "\n",
    "# Sans scaling\n",
    "scores_sans = Q @ K.T\n",
    "weights_sans = F.softmax(scores_sans, dim=-1)\n",
    "\n",
    "# Avec scaling\n",
    "scores_avec = (Q @ K.T) / math.sqrt(d_k)\n",
    "weights_avec = F.softmax(scores_avec, dim=-1)\n",
    "\n",
    "print(\"SANS SCALING :\")\n",
    "print(f\"  Scores : min={scores_sans.min():.1f}, max={scores_sans.max():.1f}\")\n",
    "print(f\"  Poids max par ligne : {weights_sans.max(dim=-1).values.tolist()}\")\n",
    "print(f\"  → Distribution très piquée (proches de 1)\")\n",
    "\n",
    "print(\"\\nAVEC SCALING (÷ √512 ≈ ÷ 22.6) :\")\n",
    "print(f\"  Scores : min={scores_avec.min():.1f}, max={scores_avec.max():.1f}\")\n",
    "print(f\"  Poids max par ligne : {weights_avec.max(dim=-1).values.tolist()}\")\n",
    "print(f\"  → Distribution plus douce, meilleur apprentissage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Récapitulatif\n",
    "\n",
    "| Question | Réponse |\n",
    "|----------|--------|\n",
    "| **Que fait l'attention ?** | Permet à chaque mot de récupérer de l'info des autres mots |\n",
    "| **D'où vient la formule ?** | Construction pour mesurer compatibilité (Q·K), normaliser (softmax), récupérer (×V) |\n",
    "| **Q, K, V c'est quoi ?** | Projections des embeddings via matrices apprenables |\n",
    "| **Pourquoi 3 matrices ?** | Pour projeter dans des espaces différents (pas juste similarité sémantique) |\n",
    "| **Comment ça apprend ?** | Backpropagation depuis la loss finale du modèle |\n",
    "| **Dimensions ?** | Q, K, V : (seq_len, d_model). Le vocabulaire n'intervient pas. |\n",
    "| **√d_k ?** | Stabilise les scores pour éviter que softmax sature |\n",
    "\n",
    "### L'essentiel\n",
    "\n",
    "L'attention est une **brique différentiable** qui permet aux mots de \"communiquer\". Les matrices W_q, W_k, W_v sont ajustées pendant l'entraînement pour que cette communication soit **utile** pour la tâche finale.\n",
    "\n",
    "Le \"rôle\" de Q, K, V (questions, clés, valeurs) est une **métaphore** pour comprendre l'architecture. En réalité, c'est la **position dans la formule** qui détermine leur fonction, et l'entraînement qui affine leurs propriétés."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
