{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 01 - Fondamentaux NLP pour les Transformers\n",
    "\n",
    "**Module** : Réseaux de Neurones Approfondissement  \n",
    "**Durée** : 2h  \n",
    "**Objectif** : Comprendre comment représenter du texte pour un modèle de deep learning\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs pédagogiques\n",
    "\n",
    "À la fin de ce TP, vous serez capable de :\n",
    "1. Expliquer les différentes stratégies de tokenization\n",
    "2. Comprendre ce qu'est un embedding et pourquoi c'est utile\n",
    "3. Utiliser Word2Vec pour explorer les similarités sémantiques\n",
    "4. Visualiser intuitivement ce que fait l'attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports\n",
    "\n",
    "Exécutez cette cellule pour installer les dépendances nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances (Google Colab)\n",
    "!pip install torch matplotlib numpy gensim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction : Du texte aux nombres\n",
    "\n",
    "### Le problème fondamental\n",
    "\n",
    "Un réseau de neurones ne comprend que des **nombres**. Comment transformer du texte en quelque chose qu'un modèle peut traiter ?\n",
    "\n",
    "```\n",
    "\"Le chat dort sur le canapé\"\n",
    "            ↓\n",
    "        ????????\n",
    "            ↓\n",
    "    Tenseur exploitable\n",
    "```\n",
    "\n",
    "### Le pipeline complet\n",
    "\n",
    "```\n",
    "Texte brut\n",
    "    ↓\n",
    "┌─────────────────┐\n",
    "│  TOKENIZATION   │  \"Le chat\" → [\"Le\", \"chat\"] ou [\"Le\", \"ch\", \"##at\"]\n",
    "└─────────────────┘\n",
    "    ↓\n",
    "┌─────────────────┐\n",
    "│   NUMÉRISATION  │  [\"Le\", \"chat\"] → [45, 892]\n",
    "└─────────────────┘\n",
    "    ↓\n",
    "┌─────────────────┐\n",
    "│    EMBEDDING    │  [45, 892] → [[0.2, -0.1, ...], [0.8, 0.3, ...]]\n",
    "└─────────────────┘\n",
    "    ↓\n",
    "Tenseur prêt pour le modèle\n",
    "```\n",
    "\n",
    "> **Note** : Pour les Transformers, une étape supplémentaire (Positional Encoding) sera ajoutée. Nous la verrons au TP4 quand nous assemblerons l'architecture complète.\n",
    "\n",
    "Dans ce TP, nous allons explorer les étapes fondamentales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Tokenization : Découper le texte\n",
    "\n",
    "La **tokenization** consiste à découper le texte en unités (tokens). Il existe plusieurs stratégies.\n",
    "\n",
    "### 2.1 Tokenization par mots (Word-level)\n",
    "\n",
    "La plus intuitive : on découpe sur les espaces et la ponctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization simple par mots\n",
    "def tokenize_words(text):\n",
    "    \"\"\"Tokenization basique par espaces et ponctuation.\"\"\"\n",
    "    import re\n",
    "    # Sépare sur espaces et garde la ponctuation comme tokens\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text.lower())\n",
    "    return tokens\n",
    "\n",
    "texte = \"Le chat mange la souris. La souris court vite !\"\n",
    "tokens = tokenize_words(texte)\n",
    "\n",
    "print(f\"Texte : {texte}\")\n",
    "print(f\"Tokens : {tokens}\")\n",
    "print(f\"Nombre de tokens : {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problème** : Que faire avec un mot inconnu (hors vocabulaire) ?\n",
    "\n",
    "```\n",
    "Vocabulaire : [\"le\", \"chat\", \"mange\", \"souris\", ...]\n",
    "Nouveau mot : \"anticonstitutionnellement\" → ???\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization par caractères (Character-level)\n",
    "\n",
    "Une solution : découper caractère par caractère. Plus de mots inconnus !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_chars(text):\n",
    "    \"\"\"Tokenization par caractères.\"\"\"\n",
    "    return list(text.lower())\n",
    "\n",
    "texte = \"Le chat dort.\"\n",
    "tokens = tokenize_chars(texte)\n",
    "\n",
    "print(f\"Texte : {texte}\")\n",
    "print(f\"Tokens : {tokens}\")\n",
    "print(f\"Nombre de tokens : {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problème** : Séquences très longues ! \"anticonstitutionnellement\" = 25 tokens.\n",
    "\n",
    "Le modèle doit \"réapprendre\" que `c-h-a-t` forme le concept de chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tokenization Subword (BPE)\n",
    "\n",
    "**Byte Pair Encoding (BPE)** : le meilleur des deux mondes.\n",
    "\n",
    "- Mots fréquents → tokens entiers (`chat`, `le`)\n",
    "- Mots rares → découpés en sous-mots (`anti`, `constitution`, `nelle`, `ment`)\n",
    "\n",
    "C'est ce qu'utilisent GPT, BERT, et la plupart des LLMs modernes.\n",
    "\n",
    "**Ressource** : [Les tokenizers en NLP (FR)](https://lbourdois.github.io/blog/nlp/Les-tokenizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démonstration avec un vrai tokenizer (GPT-2)\n",
    "!pip install transformers -q\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "textes = [\n",
    "    \"Le chat mange.\",\n",
    "    \"anticonstitutionnellement\",\n",
    "    \"Hello world!\",\n",
    "    \"Transformers are amazing!\"\n",
    "]\n",
    "\n",
    "print(\"=== Tokenization BPE (GPT-2) ===\")\n",
    "for texte in textes:\n",
    "    tokens = tokenizer.tokenize(texte)\n",
    "    ids = tokenizer.encode(texte)\n",
    "    print(f\"\\n'{texte}'\")\n",
    "    print(f\"  Tokens : {tokens}\")\n",
    "    print(f\"  IDs    : {ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations** :\n",
    "- Les mots courants anglais sont souvent des tokens uniques\n",
    "- Les mots français/rares sont découpés\n",
    "- Le caractère `Ġ` indique un espace avant le token\n",
    "\n",
    "### Exercice 1 : Comparer les tokenizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Comparer les tokenizations\n",
    "# ============================================\n",
    "\n",
    "texte_test = \"L'intelligence artificielle révolutionne le monde.\"\n",
    "\n",
    "# TODO: Tokenizer avec les 3 méthodes et comparer le nombre de tokens\n",
    "\n",
    "# 1. Par mots\n",
    "tokens_mots = None  # tokenize_words(texte_test)\n",
    "\n",
    "# 2. Par caractères  \n",
    "tokens_chars = None  # tokenize_chars(texte_test)\n",
    "\n",
    "# 3. BPE (GPT-2)\n",
    "tokens_bpe = None  # tokenizer.tokenize(texte_test)\n",
    "\n",
    "print(f\"Texte : {texte_test}\")\n",
    "print(f\"\\nMots      : {len(tokens_mots) if tokens_mots else '?'} tokens\")\n",
    "print(f\"Caractères: {len(tokens_chars) if tokens_chars else '?'} tokens\")\n",
    "print(f\"BPE       : {len(tokens_bpe) if tokens_bpe else '?'} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Construction d'un vocabulaire\n",
    "\n",
    "Une fois la stratégie choisie, on construit un **vocabulaire** : une table de correspondance token ↔ index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construire un vocabulaire simple\n",
    "corpus = [\n",
    "    \"le chat mange\",\n",
    "    \"le chien dort\",\n",
    "    \"la souris court\"\n",
    "]\n",
    "\n",
    "# Collecter tous les tokens uniques\n",
    "all_tokens = set()\n",
    "for phrase in corpus:\n",
    "    all_tokens.update(tokenize_words(phrase))\n",
    "\n",
    "# Créer le vocabulaire avec tokens spéciaux\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # Padding et Unknown\n",
    "for i, token in enumerate(sorted(all_tokens)):\n",
    "    vocab[token] = i + 2\n",
    "\n",
    "# Vocabulaire inverse\n",
    "id_to_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "print(\"Vocabulaire :\")\n",
    "for token, idx in vocab.items():\n",
    "    print(f\"  {idx}: '{token}'\")\n",
    "\n",
    "# Encoder une phrase\n",
    "def encode(text, vocab):\n",
    "    return [vocab.get(t, vocab[\"<UNK>\"]) for t in tokenize_words(text)]\n",
    "\n",
    "def decode(ids, id_to_token):\n",
    "    return [id_to_token[i] for i in ids]\n",
    "\n",
    "phrase = \"le chat court\"\n",
    "encoded = encode(phrase, vocab)\n",
    "decoded = decode(encoded, id_to_token)\n",
    "\n",
    "print(f\"\\nPhrase : '{phrase}'\")\n",
    "print(f\"Encodé : {encoded}\")\n",
    "print(f\"Décodé : {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Embeddings : Des indices aux vecteurs\n",
    "\n",
    "### Le problème des indices\n",
    "\n",
    "Les indices (0, 1, 2, ...) n'ont pas de **sens sémantique**. \n",
    "\n",
    "- `chat = 3` et `chien = 4` → sont-ils proches ? (oui, ce sont des animaux)\n",
    "- `chat = 3` et `voiture = 42` → sont-ils proches ? (non)\n",
    "\n",
    "Mais avec des indices, on ne peut pas mesurer cette proximité !\n",
    "\n",
    "### La solution : Embeddings\n",
    "\n",
    "On associe à chaque token un **vecteur dense** de dimension fixe (ex: 128, 256, 768).\n",
    "\n",
    "```\n",
    "\"chat\"    → [0.2, -0.5, 0.8, 0.1, ...] (128 dimensions)\n",
    "\"chien\"   → [0.3, -0.4, 0.7, 0.2, ...] (proche de chat !)\n",
    "\"voiture\" → [-0.8, 0.2, -0.3, 0.9, ...] (loin de chat)\n",
    "```\n",
    "\n",
    "Ces vecteurs sont **appris** pendant l'entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démonstration : Embedding en PyTorch\n",
    "vocab_size = 10  # 10 tokens dans notre vocabulaire\n",
    "embed_dim = 4    # Chaque token → vecteur de dimension 4\n",
    "\n",
    "# Créer une couche d'embedding\n",
    "embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "print(f\"Matrice d'embedding : {embedding.weight.shape}\")\n",
    "print(f\"  → {vocab_size} tokens × {embed_dim} dimensions\\n\")\n",
    "\n",
    "# Récupérer l'embedding d'un token\n",
    "token_id = torch.tensor([3])  # Token d'indice 3\n",
    "vector = embedding(token_id)\n",
    "\n",
    "print(f\"Token 3 → {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding d'une séquence complète\n",
    "sequence = torch.tensor([2, 5, 3, 7])  # 4 tokens\n",
    "embedded = embedding(sequence)\n",
    "\n",
    "print(f\"Séquence : {sequence.tolist()}\")\n",
    "print(f\"Shape après embedding : {embedded.shape}\")  # (4, 4)\n",
    "print(f\"\\nVecteurs :\\n{embedded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesurer la similarité : Distance cosinus\n",
    "\n",
    "Avec des vecteurs, on peut mesurer la **similarité** entre mots :\n",
    "\n",
    "$$\\text{similarité}(A, B) = \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
    "\n",
    "- Résultat entre -1 (opposés) et 1 (identiques)\n",
    "- 0 = orthogonaux (pas de relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calcule la similarité cosinus entre deux vecteurs.\"\"\"\n",
    "    return torch.dot(a, b) / (torch.norm(a) * torch.norm(b))\n",
    "\n",
    "# Exemple avec nos embeddings aléatoires\n",
    "vec_2 = embedding(torch.tensor(2))\n",
    "vec_3 = embedding(torch.tensor(3))\n",
    "vec_7 = embedding(torch.tensor(7))\n",
    "\n",
    "sim_2_3 = cosine_similarity(vec_2, vec_3)\n",
    "sim_2_7 = cosine_similarity(vec_2, vec_7)\n",
    "\n",
    "print(f\"Similarité(token 2, token 3) = {sim_2_3:.4f}\")\n",
    "print(f\"Similarité(token 2, token 7) = {sim_2_7:.4f}\")\n",
    "print(\"\\n(Valeurs aléatoires car embeddings non entraînés)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Word2Vec : Embeddings pré-entraînés\n",
    "\n",
    "**Word2Vec** (2013) a révolutionné le NLP en montrant qu'on peut apprendre des embeddings qui capturent le sens des mots.\n",
    "\n",
    "### L'idée clé\n",
    "\n",
    "> \"Tu connais un mot par les mots qui l'entourent\"\n",
    "\n",
    "Si \"chat\" et \"chien\" apparaissent dans des contextes similaires (\"le ___ mange\", \"mon ___ dort\"), leurs vecteurs seront proches.\n",
    "\n",
    "### La magie des analogies\n",
    "\n",
    "Les embeddings Word2Vec permettent des **opérations arithmétiques** sur les mots :\n",
    "\n",
    "```\n",
    "roi - homme + femme ≈ reine\n",
    "paris - france + italie ≈ rome\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un modèle Word2Vec pré-entraîné\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(\"Chargement du modèle Word2Vec (peut prendre 1-2 min)...\")\n",
    "model = api.load(\"glove-wiki-gigaword-100\")  # 100 dimensions, entraîné sur Wikipedia\n",
    "print(f\"Modèle chargé ! Vocabulaire : {len(model)} mots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorer les similarités\n",
    "print(\"=== Mots similaires à 'king' ===\")\n",
    "for word, score in model.most_similar(\"king\", topn=5):\n",
    "    print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n=== Mots similaires à 'computer' ===\")\n",
    "for word, score in model.most_similar(\"computer\", topn=5):\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 2 : Explorer les similarités\n",
    "# ============================================\n",
    "\n",
    "# TODO: Trouver les 5 mots les plus similaires à :\n",
    "# - \"france\"\n",
    "# - \"cat\" (chat en anglais)\n",
    "# - \"happy\"\n",
    "\n",
    "# Exemple :\n",
    "# for word, score in model.most_similar(\"france\", topn=5):\n",
    "#     print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La magie des analogies : king - man + woman = ?\n",
    "print(\"=== Analogie : king - man + woman = ? ===\")\n",
    "result = model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=3)\n",
    "for word, score in result:\n",
    "    print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n=== Analogie : paris - france + italy = ? ===\")\n",
    "result = model.most_similar(positive=[\"paris\", \"italy\"], negative=[\"france\"], topn=3)\n",
    "for word, score in result:\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 3 : Trouver des analogies\n",
    "# ============================================\n",
    "\n",
    "# TODO: Tester ces analogies (et en inventer d'autres !)\n",
    "# - \"berlin\" - \"germany\" + \"france\" = ?\n",
    "# - \"good\" - \"better\" + \"bad\" = ?\n",
    "# - \"cat\" - \"kitten\" + \"dog\" = ?\n",
    "\n",
    "# Syntaxe :\n",
    "# model.most_similar(positive=[\"A\", \"C\"], negative=[\"B\"], topn=3)\n",
    "# Pour calculer A - B + C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des embeddings en 2D\n",
    "!pip install scikit-learn -q\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sélectionner quelques mots\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"prince\", \"princess\",\n",
    "         \"cat\", \"dog\", \"lion\", \"tiger\",\n",
    "         \"car\", \"bus\", \"train\", \"plane\"]\n",
    "\n",
    "# Récupérer leurs vecteurs\n",
    "vectors = np.array([model[w] for w in words])\n",
    "\n",
    "# Réduire à 2D avec PCA\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(vectors)\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], c='blue', s=100)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (vectors_2d[i, 0] + 0.1, vectors_2d[i, 1] + 0.1), fontsize=12)\n",
    "\n",
    "plt.title(\"Embeddings Word2Vec projetés en 2D\")\n",
    "plt.xlabel(\"Composante 1\")\n",
    "plt.ylabel(\"Composante 2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation : Les mots de même catégorie sont regroupés !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Teaser : Le mécanisme d'attention\n",
    "\n",
    "Maintenant que nous savons représenter du texte (tokenization + embeddings), la prochaine étape est de permettre aux mots de **communiquer entre eux**.\n",
    "\n",
    "C'est le rôle du **mécanisme d'attention**, que nous verrons en détail au prochain TP.\n",
    "\n",
    "### L'idée\n",
    "\n",
    "> Pour comprendre un mot, il faut regarder les autres mots de la phrase.\n",
    "\n",
    "Exemple : *\"Le chat qui dormait sur le canapé a sauté\"*\n",
    "- Pour comprendre **\"a sauté\"** → regarder **\"chat\"** (le sujet)\n",
    "- Pour comprendre **\"dormait\"** → regarder **\"chat\"** et **\"canapé\"**\n",
    "\n",
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice d'attention simulée\n",
    "phrase = [\"Le\", \"chat\", \"mange\", \"la\", \"souris\"]\n",
    "\n",
    "# Chaque ligne = un mot qui \"regarde\" les autres\n",
    "# Valeurs = poids d'attention (somme = 1 par ligne)\n",
    "attention = torch.tensor([\n",
    "    [0.8, 0.1, 0.05, 0.03, 0.02],  # \"Le\" regarde surtout lui-même\n",
    "    [0.1, 0.7, 0.1, 0.05, 0.05],   # \"chat\" regarde surtout lui-même\n",
    "    [0.05, 0.4, 0.4, 0.05, 0.1],   # \"mange\" regarde \"chat\" et lui-même\n",
    "    [0.02, 0.03, 0.05, 0.8, 0.1],  # \"la\" regarde surtout lui-même\n",
    "    [0.02, 0.1, 0.2, 0.08, 0.6],   # \"souris\" regarde \"mange\" et elle-même\n",
    "])\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention, cmap='Blues')\n",
    "plt.xticks(range(5), phrase)\n",
    "plt.yticks(range(5), phrase)\n",
    "plt.xlabel(\"Mots regardés\")\n",
    "plt.ylabel(\"Mots qui regardent\")\n",
    "plt.title(\"Qui regarde qui ? (Matrice d'attention)\")\n",
    "plt.colorbar(label=\"Poids d'attention\")\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        plt.text(j, i, f'{attention[i,j]:.2f}', \n",
    "                ha='center', va='center',\n",
    "                color='white' if attention[i,j] > 0.5 else 'black')\n",
    "plt.show()\n",
    "\n",
    "print(\"Le verbe 'mange' regarde fortement 'chat' (son sujet) !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ce qu'on voit** :\n",
    "- Chaque mot peut \"regarder\" tous les autres mots\n",
    "- Les poids indiquent l'importance de chaque relation\n",
    "- Le modèle **apprend** ces poids pendant l'entraînement\n",
    "\n",
    "**Au prochain TP**, nous verrons :\n",
    "- Pourquoi les RNN/LSTM ont des limites\n",
    "- Comment calculer cette matrice d'attention\n",
    "- Les concepts Query, Key, Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Récapitulatif\n",
    "\n",
    "### Le pipeline NLP\n",
    "\n",
    "| Étape | Entrée | Sortie | Rôle |\n",
    "|-------|--------|--------|------|\n",
    "| **Tokenization** | Texte brut | Liste de tokens | Découper le texte |\n",
    "| **Vocabulaire** | Tokens | Indices | Table token ↔ ID |\n",
    "| **Embedding** | Indices | Vecteurs denses | Sens sémantique |\n",
    "\n",
    "### Points clés\n",
    "\n",
    "1. **Tokenization BPE** : meilleur compromis entre mots et caractères\n",
    "2. **Embeddings** : transforment les mots en vecteurs comparables\n",
    "3. **Word2Vec** : montre que les embeddings capturent le sens (analogies !)\n",
    "4. **Similarité cosinus** : mesure la proximité entre vecteurs\n",
    "\n",
    "### Prochaine session\n",
    "\n",
    "Nous verrons le **mécanisme d'attention** : comment les mots \"communiquent\" entre eux pour se comprendre mutuellement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Pour aller plus loin (optionnel)\n",
    "\n",
    "### Ressources\n",
    "\n",
    "- [Les tokenizers en NLP (FR)](https://lbourdois.github.io/blog/nlp/Les-tokenizers/) - Excellent article en français\n",
    "- [The Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/) - Visualisations très claires\n",
    "\n",
    "### Expérimentations suggérées\n",
    "\n",
    "1. Tester d'autres analogies Word2Vec\n",
    "2. Visualiser les embeddings de votre choix\n",
    "3. Comparer différents tokenizers (BERT, GPT-2, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
