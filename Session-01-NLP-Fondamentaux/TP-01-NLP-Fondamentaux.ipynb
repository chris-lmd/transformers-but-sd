{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# TP 01 - Fondamentaux NLP pour les Transformers\n\n**Module** : Réseaux de Neurones Approfondissement  \n**Durée** : 2h  \n**Objectif** : Comprendre comment faire interpréter du texte par une machine\n\n---\n\n## Comment faire interpréter du texte par une machine ?\n\nVous avez déjà travaillé avec des **images** : une image est naturellement une grille de pixels, chaque pixel est un nombre (0-255). Le réseau peut directement les traiter.\n\nMais le **texte** ? \n\n```\n\"L'apprentissage automatique révolutionne l'intelligence artificielle\"\n```\n\nCe n'est qu'une suite de caractères. Un réseau de neurones ne comprend que des **nombres**. Comment passer de l'un à l'autre ?\n\n---\n\n## Les deux problèmes à résoudre\n\nPour transformer du texte en représentation numérique exploitable, il faut résoudre **deux problèmes distincts** :\n\n### Problème 1 : La tokenization\n\n**Comment découper le texte en morceaux ?**\n\n```\n\"L'apprentissage automatique\" → ???\n```\n\nPlusieurs stratégies possibles :\n- Par mots : `[\"L'apprentissage\", \"automatique\"]`\n- Par caractères : `[\"L\", \"'\", \"a\", \"p\", \"p\", \"r\", ...]`\n- Par sous-mots : `[\"L'\", \"apprent\", \"issage\", \"automatique\"]`\n\nChaque stratégie a ses avantages et inconvénients. Nous les explorerons dans ce TP.\n\n### Problème 2 : L'embedding\n\n**Comment transformer ces morceaux en vecteurs qui ont du SENS ?**\n\nUne fois le texte découpé, on pourrait simplement numéroter les tokens :\n```\n\"chat\" → 42\n\"chien\" → 73\n\"voiture\" → 156\n```\n\nMais ces nombres sont **arbitraires**. Ils ne capturent pas que \"chat\" et \"chien\" sont des concepts proches (animaux domestiques), alors que \"voiture\" est complètement différent.\n\n**Il faut trouver un moyen** de transformer chaque token en un **vecteur de plusieurs dimensions** où la **proximité géométrique** reflète la **proximité sémantique** :\n\n```\n\"chat\"    → [0.2, -0.5, 0.8, ...]   ┐\n                                    ├─ vecteurs proches !\n\"chien\"   → [0.3, -0.4, 0.7, ...]   ┘\n\n\"voiture\" → [-0.8, 0.2, -0.3, ...]  ← vecteur éloigné\n```\n\nPlusieurs approches existent pour construire ces vecteurs. Dans ce TP, nous explorerons **Word2Vec**, une méthode remarquable qui a révolutionné le NLP en 2013.\n\n---\n\n## Plan du TP\n\n| Section | Problème traité | Ce que vous apprendrez |\n|---------|-----------------|------------------------|\n| §1-2 | Tokenization | Les 3 stratégies (mots, caractères, BPE) |\n| §3-4 | Embedding | Comment les vecteurs capturent le sens (Word2Vec) |\n| §5 | Bonus | Teaser sur le mécanisme d'attention |\n\nCommençons par le premier problème : **comment découper le texte ?**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports\n",
    "\n",
    "Exécutez cette cellule pour installer les dépendances nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances (Google Colab)\n",
    "!pip install torch matplotlib numpy gensim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Partie 1 : La Tokenization\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Tokenization : Découper le texte\n",
    "\n",
    "La **tokenization** consiste à découper le texte en unités (tokens). Il existe plusieurs stratégies.\n",
    "\n",
    "### 2.1 Tokenization par mots (Word-level)\n",
    "\n",
    "La plus intuitive : on découpe sur les espaces et la ponctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization simple par mots\n",
    "def tokenize_words(text):\n",
    "    \"\"\"Tokenization basique par espaces et ponctuation.\"\"\"\n",
    "    import re\n",
    "    # Sépare sur espaces et garde la ponctuation comme tokens\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text.lower())\n",
    "    return tokens\n",
    "\n",
    "texte = \"Le chat mange la souris. La souris court vite !\"\n",
    "tokens = tokenize_words(texte)\n",
    "\n",
    "print(f\"Texte : {texte}\")\n",
    "print(f\"Tokens : {tokens}\")\n",
    "print(f\"Nombre de tokens : {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problème** : Que faire avec un mot inconnu (hors vocabulaire) ?\n",
    "\n",
    "```\n",
    "Vocabulaire : [\"le\", \"chat\", \"mange\", \"souris\", ...]\n",
    "Nouveau mot : \"anticonstitutionnellement\" → ???\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization par caractères (Character-level)\n",
    "\n",
    "Une solution : découper caractère par caractère. Plus de mots inconnus !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_chars(text):\n",
    "    \"\"\"Tokenization par caractères.\"\"\"\n",
    "    return list(text.lower())\n",
    "\n",
    "texte = \"Le chat dort.\"\n",
    "tokens = tokenize_chars(texte)\n",
    "\n",
    "print(f\"Texte : {texte}\")\n",
    "print(f\"Tokens : {tokens}\")\n",
    "print(f\"Nombre de tokens : {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problème** : Séquences très longues ! \"anticonstitutionnellement\" = 25 tokens.\n",
    "\n",
    "Le modèle doit \"réapprendre\" que `c-h-a-t` forme le concept de chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.3 Tokenization Subword : BPE (Byte Pair Encoding)\n\nLes deux approches précédentes ont des défauts :\n- **Mots** : vocabulaire énorme + mots inconnus\n- **Caractères** : séquences trop longues + perte de sens\n\n**BPE** (Byte Pair Encoding) est un **compromis intelligent** utilisé par GPT, BERT, et tous les LLMs modernes.\n\n---\n\n#### Le principe\n\nBPE construit son vocabulaire en analysant un grand corpus de texte :\n\n1. **Départ** : vocabulaire = tous les caractères\n2. **Répéter** : trouver la paire de tokens adjacents la plus fréquente → la fusionner en un nouveau token\n3. **Stop** : quand le vocabulaire atteint la taille voulue (ex: 50 000 tokens)\n\n---\n\n#### L'algorithme pas à pas\n\nPour comprendre, prenons un **corpus artificiel simplifié** :\n```\n\"smartphone smartphone smartphone smartwatch smartwatch phone phone\"\n```\n\n**Étape 0 : Partir des caractères**\n```\nVocabulaire : {s, m, a, r, t, p, h, o, n, e, w, c}\n```\n\n**Étapes suivantes : Fusionner les paires les plus fréquentes**\n\n| Étape | Paire la + fréquente | Nouveau token |\n|-------|---------------------|---------------|\n| 1 | (s, m) | \"sm\" |\n| 2 | (sm, a) | \"sma\" |\n| 3 | (sma, r) | \"smar\" |\n| 4 | (smar, t) | \"smart\" |\n| 5 | (p, h) | \"ph\" |\n| 6 | (ph, o) | \"pho\" |\n| 7 | (pho, n) | \"phon\" |\n| 8 | (phon, e) | \"phone\" |\n\n**Résultat :**\n```\n\"smartphone\" → [\"smart\", \"phone\"]  ← 2 tokens réutilisables !\n\"smartwatch\" → [\"smart\", \"watch\"]\n\"phone\"      → [\"phone\"]\n```\n\n---\n\n#### Pourquoi c'est malin ?\n\nUn mot **jamais vu** comme \"smartcar\" sera découpé en :\n```\n\"smartcar\" → [\"smart\", \"car\"]\n```\n\nLe modèle connaît déjà \"smart\" ! Pas besoin de token `<UNK>`.\n\n**Le meilleur des deux mondes** :\n- Mots fréquents → tokens entiers (efficace)\n- Mots rares/nouveaux → sous-mots connus (robuste)\n\n**Ressource** : [Explication détaillée des tokenizers (FR)](https://lbourdois.github.io/blog/nlp/Les-tokenizers/)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démonstration avec un vrai tokenizer (GPT-2)\n",
    "!pip install transformers -q\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "textes = [\n",
    "    \"Le chat mange.\",\n",
    "    \"anticonstitutionnellement\",\n",
    "    \"Hello world!\",\n",
    "    \"Transformers are amazing!\"\n",
    "]\n",
    "\n",
    "print(\"=== Tokenization BPE (GPT-2) ===\")\n",
    "for texte in textes:\n",
    "    tokens = tokenizer.tokenize(texte)\n",
    "    ids = tokenizer.encode(texte)\n",
    "    print(f\"\\n'{texte}'\")\n",
    "    print(f\"  Tokens : {tokens}\")\n",
    "    print(f\"  IDs    : {ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations** :\n",
    "- Les mots courants anglais sont souvent des tokens uniques\n",
    "- Les mots français/rares sont découpés\n",
    "- Le caractère `Ġ` indique un espace avant le token\n",
    "\n",
    "### Exercice 1 : Comparer les tokenizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Comparer les tokenizations\n",
    "# ============================================\n",
    "\n",
    "texte_test = \"L'intelligence artificielle révolutionne le monde.\"\n",
    "\n",
    "# TODO: Tokenizer avec les 3 méthodes et comparer le nombre de tokens\n",
    "\n",
    "# 1. Par mots\n",
    "tokens_mots = None  # tokenize_words(texte_test)\n",
    "\n",
    "# 2. Par caractères  \n",
    "tokens_chars = None  # tokenize_chars(texte_test)\n",
    "\n",
    "# 3. BPE (GPT-2)\n",
    "tokens_bpe = None  # tokenizer.tokenize(texte_test)\n",
    "\n",
    "print(f\"Texte : {texte_test}\")\n",
    "print(f\"\\nMots      : {len(tokens_mots) if tokens_mots else '?'} tokens\")\n",
    "print(f\"Caractères: {len(tokens_chars) if tokens_chars else '?'} tokens\")\n",
    "print(f\"BPE       : {len(tokens_bpe) if tokens_bpe else '?'} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Construction d'un vocabulaire\n",
    "\n",
    "Une fois la stratégie choisie, on construit un **vocabulaire** : une table de correspondance token ↔ index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construire un vocabulaire simple\n",
    "corpus = [\n",
    "    \"le chat mange\",\n",
    "    \"le chien dort\",\n",
    "    \"la souris court\"\n",
    "]\n",
    "\n",
    "# Collecter tous les tokens uniques\n",
    "all_tokens = set()\n",
    "for phrase in corpus:\n",
    "    all_tokens.update(tokenize_words(phrase))\n",
    "\n",
    "# Créer le vocabulaire avec tokens spéciaux\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # Padding et Unknown\n",
    "for i, token in enumerate(sorted(all_tokens)):\n",
    "    vocab[token] = i + 2\n",
    "\n",
    "# Vocabulaire inverse\n",
    "id_to_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "print(\"Vocabulaire :\")\n",
    "for token, idx in vocab.items():\n",
    "    print(f\"  {idx}: '{token}'\")\n",
    "\n",
    "# Encoder une phrase\n",
    "def encode(text, vocab):\n",
    "    return [vocab.get(t, vocab[\"<UNK>\"]) for t in tokenize_words(text)]\n",
    "\n",
    "def decode(ids, id_to_token):\n",
    "    return [id_to_token[i] for i in ids]\n",
    "\n",
    "phrase = \"le chat court\"\n",
    "encoded = encode(phrase, vocab)\n",
    "decoded = decode(encoded, id_to_token)\n",
    "\n",
    "print(f\"\\nPhrase : '{phrase}'\")\n",
    "print(f\"Encodé : {encoded}\")\n",
    "print(f\"Décodé : {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Embeddings : Des indices aux vecteurs\n",
    "\n",
    "### Le problème des indices\n",
    "\n",
    "Les indices (0, 1, 2, ...) n'ont pas de **sens sémantique**. \n",
    "\n",
    "- `chat = 3` et `chien = 4` → sont-ils proches ? (oui, ce sont des animaux)\n",
    "- `chat = 3` et `voiture = 42` → sont-ils proches ? (non)\n",
    "\n",
    "Mais avec des indices, on ne peut pas mesurer cette proximité !\n",
    "\n",
    "### La solution : Embeddings\n",
    "\n",
    "On associe à chaque token un **vecteur dense** de dimension fixe (ex: 128, 256, 768).\n",
    "\n",
    "```\n",
    "\"chat\"    → [0.2, -0.5, 0.8, 0.1, ...] (128 dimensions)\n",
    "\"chien\"   → [0.3, -0.4, 0.7, 0.2, ...] (proche de chat !)\n",
    "\"voiture\" → [-0.8, 0.2, -0.3, 0.9, ...] (loin de chat)\n",
    "```\n",
    "\n",
    "Ces vecteurs sont **appris** pendant l'entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démonstration : Embedding en PyTorch\n",
    "vocab_size = 10  # 10 tokens dans notre vocabulaire\n",
    "embed_dim = 4    # Chaque token → vecteur de dimension 4\n",
    "\n",
    "# Créer une couche d'embedding\n",
    "embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "print(f\"Matrice d'embedding : {embedding.weight.shape}\")\n",
    "print(f\"  → {vocab_size} tokens × {embed_dim} dimensions\\n\")\n",
    "\n",
    "# Récupérer l'embedding d'un token\n",
    "token_id = torch.tensor([3])  # Token d'indice 3\n",
    "vector = embedding(token_id)\n",
    "\n",
    "print(f\"Token 3 → {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding d'une séquence complète\n",
    "sequence = torch.tensor([2, 5, 3, 7])  # 4 tokens\n",
    "embedded = embedding(sequence)\n",
    "\n",
    "print(f\"Séquence : {sequence.tolist()}\")\n",
    "print(f\"Shape après embedding : {embedded.shape}\")  # (4, 4)\n",
    "print(f\"\\nVecteurs :\\n{embedded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesurer la similarité : Distance cosinus\n",
    "\n",
    "Avec des vecteurs, on peut mesurer la **similarité** entre mots :\n",
    "\n",
    "$$\\text{similarité}(A, B) = \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
    "\n",
    "- Résultat entre -1 (opposés) et 1 (identiques)\n",
    "- 0 = orthogonaux (pas de relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calcule la similarité cosinus entre deux vecteurs.\"\"\"\n",
    "    return torch.dot(a, b) / (torch.norm(a) * torch.norm(b))\n",
    "\n",
    "# Exemple avec nos embeddings aléatoires\n",
    "vec_2 = embedding(torch.tensor(2))\n",
    "vec_3 = embedding(torch.tensor(3))\n",
    "vec_7 = embedding(torch.tensor(7))\n",
    "\n",
    "sim_2_3 = cosine_similarity(vec_2, vec_3)\n",
    "sim_2_7 = cosine_similarity(vec_2, vec_7)\n",
    "\n",
    "print(f\"Similarité(token 2, token 3) = {sim_2_3:.4f}\")\n",
    "print(f\"Similarité(token 2, token 7) = {sim_2_7:.4f}\")\n",
    "print(\"\\n(Valeurs aléatoires car embeddings non entraînés)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. Word2Vec : Apprendre des embeddings qui ont du sens\n\nJusqu'ici, nos embeddings étaient **aléatoires** (initialisés au hasard dans PyTorch). Comment obtenir des vecteurs où \"chat\" et \"chien\" sont vraiment proches ?\n\n**Word2Vec** (Mikolov et al., 2013) a révolutionné le NLP en montrant qu'on peut **apprendre** des embeddings à partir de texte brut, sans supervision.\n\n---\n\n### L'intuition fondamentale\n\n> **\"Tu connais un mot par les mots qui l'entourent\"** (hypothèse distributionnelle)\n\nObservez ces phrases :\n```\n\"Le chat mange sa pâtée\"\n\"Le chat dort sur le canapé\"  \n\"Mon chat joue avec une balle\"\n\n\"Le chien mange sa pâtée\"\n\"Le chien dort sur le canapé\"\n\"Mon chien joue avec une balle\"\n```\n\n\"Chat\" et \"chien\" apparaissent dans les **mêmes contextes**. Word2Vec va leur attribuer des vecteurs similaires.\n\n---\n\n### Comment Word2Vec apprend ? (Skip-gram)\n\nL'idée est simple : entraîner un réseau à **prédire les mots du contexte** à partir d'un mot central.\n\n```\nPhrase : \"Le chat noir dort paisiblement\"\n                  ↑\n            mot central\n\nFenêtre de contexte (±2 mots) :\n    Entrée : \"noir\"\n    Cibles : [\"chat\", \"dort\"] (le réseau essaie de les prédire)\n```\n\n**Architecture simplifiée :**\n\n```\n    \"noir\"                      \"chat\" ?\n      ↓                            ↑\n ┌──────────┐                ┌──────────┐\n │ Embedding │  → vecteur →  │ Prédiction│\n │ (lookup)  │    256 dim    │ (softmax) │\n └──────────┘                └──────────┘\n```\n\n**Pendant l'entraînement :**\n- Le réseau voit des millions de paires (mot central, mot contexte)\n- Il ajuste les embeddings pour que les mots apparaissant dans les mêmes contextes aient des vecteurs proches\n- Les embeddings **émergent** de cette tâche de prédiction\n\n---\n\n### Pourquoi les analogies marchent ?\n\nAprès entraînement, les vecteurs encodent des **relations** :\n\n```\nvecteur(\"roi\") - vecteur(\"homme\") ≈ vecteur(\"reine\") - vecteur(\"femme\")\n```\n\nAutrement dit, la \"direction\" homme→femme dans l'espace vectoriel est la même que roi→reine :\n\n```\n        homme ─────────────→ femme\n          ↑    (même         ↑\n          │   direction)     │\n         roi ─────────────→ reine\n```\n\nC'est pour ça que `roi - homme + femme ≈ reine` fonctionne !\n\n---\n\n### Chargeons un modèle pré-entraîné\n\nNous allons utiliser **GloVe** (similaire à Word2Vec), entraîné sur Wikipedia."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un modèle Word2Vec pré-entraîné\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(\"Chargement du modèle Word2Vec (peut prendre 1-2 min)...\")\n",
    "model = api.load(\"glove-wiki-gigaword-100\")  # 100 dimensions, entraîné sur Wikipedia\n",
    "print(f\"Modèle chargé ! Vocabulaire : {len(model)} mots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorer les similarités\n",
    "print(\"=== Mots similaires à 'king' ===\")\n",
    "for word, score in model.most_similar(\"king\", topn=5):\n",
    "    print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n=== Mots similaires à 'computer' ===\")\n",
    "for word, score in model.most_similar(\"computer\", topn=5):\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 2 : Explorer les similarités\n",
    "# ============================================\n",
    "\n",
    "# TODO: Trouver les 5 mots les plus similaires à :\n",
    "# - \"france\"\n",
    "# - \"cat\" (chat en anglais)\n",
    "# - \"happy\"\n",
    "\n",
    "# Exemple :\n",
    "# for word, score in model.most_similar(\"france\", topn=5):\n",
    "#     print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La magie des analogies : king - man + woman = ?\n",
    "print(\"=== Analogie : king - man + woman = ? ===\")\n",
    "result = model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=3)\n",
    "for word, score in result:\n",
    "    print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n=== Analogie : paris - france + italy = ? ===\")\n",
    "result = model.most_similar(positive=[\"paris\", \"italy\"], negative=[\"france\"], topn=3)\n",
    "for word, score in result:\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 3 : Trouver des analogies\n",
    "# ============================================\n",
    "\n",
    "# TODO: Tester ces analogies (et en inventer d'autres !)\n",
    "# - \"berlin\" - \"germany\" + \"france\" = ?\n",
    "# - \"good\" - \"better\" + \"bad\" = ?\n",
    "# - \"cat\" - \"kitten\" + \"dog\" = ?\n",
    "\n",
    "# Syntaxe :\n",
    "# model.most_similar(positive=[\"A\", \"C\"], negative=[\"B\"], topn=3)\n",
    "# Pour calculer A - B + C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des embeddings en 2D\n",
    "!pip install scikit-learn -q\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sélectionner quelques mots\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"prince\", \"princess\",\n",
    "         \"cat\", \"dog\", \"lion\", \"tiger\",\n",
    "         \"car\", \"bus\", \"train\", \"plane\"]\n",
    "\n",
    "# Récupérer leurs vecteurs\n",
    "vectors = np.array([model[w] for w in words])\n",
    "\n",
    "# Réduire à 2D avec PCA\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(vectors)\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], c='blue', s=100)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (vectors_2d[i, 0] + 0.1, vectors_2d[i, 1] + 0.1), fontsize=12)\n",
    "\n",
    "plt.title(\"Embeddings Word2Vec projetés en 2D\")\n",
    "plt.xlabel(\"Composante 1\")\n",
    "plt.ylabel(\"Composante 2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation : Les mots de même catégorie sont regroupés !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Teaser : Le mécanisme d'attention\n",
    "\n",
    "Maintenant que nous savons représenter du texte (tokenization + embeddings), la prochaine étape est de permettre aux mots de **communiquer entre eux**.\n",
    "\n",
    "C'est le rôle du **mécanisme d'attention**, que nous verrons en détail au prochain TP.\n",
    "\n",
    "### L'idée\n",
    "\n",
    "> Pour comprendre un mot, il faut regarder les autres mots de la phrase.\n",
    "\n",
    "Exemple : *\"Le chat qui dormait sur le canapé a sauté\"*\n",
    "- Pour comprendre **\"a sauté\"** → regarder **\"chat\"** (le sujet)\n",
    "- Pour comprendre **\"dormait\"** → regarder **\"chat\"** et **\"canapé\"**\n",
    "\n",
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice d'attention simulée\n",
    "phrase = [\"Le\", \"chat\", \"mange\", \"la\", \"souris\"]\n",
    "\n",
    "# Chaque ligne = un mot qui \"regarde\" les autres\n",
    "# Valeurs = poids d'attention (somme = 1 par ligne)\n",
    "attention = torch.tensor([\n",
    "    [0.8, 0.1, 0.05, 0.03, 0.02],  # \"Le\" regarde surtout lui-même\n",
    "    [0.1, 0.7, 0.1, 0.05, 0.05],   # \"chat\" regarde surtout lui-même\n",
    "    [0.05, 0.4, 0.4, 0.05, 0.1],   # \"mange\" regarde \"chat\" et lui-même\n",
    "    [0.02, 0.03, 0.05, 0.8, 0.1],  # \"la\" regarde surtout lui-même\n",
    "    [0.02, 0.1, 0.2, 0.08, 0.6],   # \"souris\" regarde \"mange\" et elle-même\n",
    "])\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention, cmap='Blues')\n",
    "plt.xticks(range(5), phrase)\n",
    "plt.yticks(range(5), phrase)\n",
    "plt.xlabel(\"Mots regardés\")\n",
    "plt.ylabel(\"Mots qui regardent\")\n",
    "plt.title(\"Qui regarde qui ? (Matrice d'attention)\")\n",
    "plt.colorbar(label=\"Poids d'attention\")\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        plt.text(j, i, f'{attention[i,j]:.2f}', \n",
    "                ha='center', va='center',\n",
    "                color='white' if attention[i,j] > 0.5 else 'black')\n",
    "plt.show()\n",
    "\n",
    "print(\"Le verbe 'mange' regarde fortement 'chat' (son sujet) !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ce qu'on voit** :\n",
    "- Chaque mot peut \"regarder\" tous les autres mots\n",
    "- Les poids indiquent l'importance de chaque relation\n",
    "- Le modèle **apprend** ces poids pendant l'entraînement\n",
    "\n",
    "**Au prochain TP**, nous verrons :\n",
    "- Pourquoi les RNN/LSTM ont des limites\n",
    "- Comment calculer cette matrice d'attention\n",
    "- Les concepts Query, Key, Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Récapitulatif\n",
    "\n",
    "### Le pipeline NLP\n",
    "\n",
    "| Étape | Entrée | Sortie | Rôle |\n",
    "|-------|--------|--------|------|\n",
    "| **Tokenization** | Texte brut | Liste de tokens | Découper le texte |\n",
    "| **Vocabulaire** | Tokens | Indices | Table token ↔ ID |\n",
    "| **Embedding** | Indices | Vecteurs denses | Sens sémantique |\n",
    "\n",
    "### Points clés\n",
    "\n",
    "1. **Tokenization BPE** : meilleur compromis entre mots et caractères\n",
    "2. **Embeddings** : transforment les mots en vecteurs comparables\n",
    "3. **Word2Vec** : montre que les embeddings capturent le sens (analogies !)\n",
    "4. **Similarité cosinus** : mesure la proximité entre vecteurs\n",
    "\n",
    "### Prochaine session\n",
    "\n",
    "Nous verrons le **mécanisme d'attention** : comment les mots \"communiquent\" entre eux pour se comprendre mutuellement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Pour aller plus loin (optionnel)\n",
    "\n",
    "### Ressources\n",
    "\n",
    "- [Les tokenizers en NLP (FR)](https://lbourdois.github.io/blog/nlp/Les-tokenizers/) - Excellent article en français\n",
    "- [The Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/) - Visualisations très claires\n",
    "\n",
    "### Expérimentations suggérées\n",
    "\n",
    "1. Tester d'autres analogies Word2Vec\n",
    "2. Visualiser les embeddings de votre choix\n",
    "3. Comparer différents tokenizers (BERT, GPT-2, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}