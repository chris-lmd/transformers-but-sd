{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 01 - Fondamentaux NLP pour les Transformers\n",
    "\n",
    "**Module** : R√©seaux de Neurones Approfondissement  \n",
    "**Dur√©e** : 2h  \n",
    "**Objectif** : Mieux comprendre comment une machine peut interpreter du texte\n",
    "\n",
    "---\n",
    "\n",
    "## Comment faire interpr√©ter du texte par une machine ?\n",
    "\n",
    "Si vous avez d√©j√† travaill√© avec des **images** le probl√®me est de prime abord plus simple : une image peu naturellement se d√©couper en une grille de pixels, chaque pixel est un nombre (0-255). Le r√©seau peut directement les traiter.\n",
    "\n",
    "Mais le pour **texte** suivant ?\n",
    "\n",
    "```\n",
    "\"L'apprentissage automatique r√©volutionne l'intelligence artificielle\"\n",
    "```\n",
    "\n",
    "Ce n'est qu'une suite de caract√®res. Un r√©seau de neurones ne comprend que des **nombres**. Comment passer de l'un √† l'autre ?\n",
    "\n",
    "---\n",
    "\n",
    "## Les probl√®mes √† r√©soudre\n",
    "\n",
    "Pour transformer du texte en repr√©sentation num√©rique exploitable, il faut r√©soudre **deux probl√®mes distincts** :\n",
    "\n",
    "### Probl√®me 1 : La tokenization\n",
    "\n",
    "**Comment d√©couper le texte suivant en morceaux ?**\n",
    "\n",
    "```\n",
    "\"L'apprentissage automatique\" ‚Üí ???\n",
    "```\n",
    "\n",
    "Plusieurs strat√©gies sont possibles :\n",
    "- Par mots : `[\"L'apprentissage\", \"automatique\"]`\n",
    "- Par caract√®res : `[\"L\", \"'\", \"a\", \"p\", \"p\", \"r\", ...]`\n",
    "- Par sous-mots : `[\"L'\", \"apprent\", \"issage\", \"automatique\"]`\n",
    "\n",
    "Chaque strat√©gie a ses avantages et inconv√©nients. Nous les explorerons dans ce TP.\n",
    "\n",
    "### Probl√®me 2 : L'embedding\n",
    "\n",
    "**Comment transformer ces morceaux en vecteurs qui ont du SENS ?**\n",
    "\n",
    "Une fois le texte d√©coup√©, on pourrait simplement num√©roter les tokens :\n",
    "```\n",
    "\"chat\" ‚Üí 42\n",
    "\"chien\" ‚Üí 73\n",
    "\"voiture\" ‚Üí 156\n",
    "```\n",
    "\n",
    "Mais ces nombres sont **arbitraires**. Ils ne capturent pas que \"chat\" et \"chien\" sont des concepts proches (animaux domestiques), alors que \"voiture\" est compl√®tement diff√©rent.\n",
    "\n",
    "**Il faut trouver un moyen** de transformer chaque token en un **vecteur de plusieurs dimensions** o√π la **proximit√© g√©om√©trique** refl√®te la **proximit√© s√©mantique** :\n",
    "\n",
    "```\n",
    "\"chat\"    ‚Üí [0.2, -0.5, 0.8, ...]   ‚îê\n",
    "                                    ‚îú‚îÄ vecteurs proches !\n",
    "\"chien\"   ‚Üí [0.3, -0.4, 0.7, ...]   ‚îò\n",
    "\n",
    "\"voiture\" ‚Üí [-0.8, 0.2, -0.3, ...]  ‚Üê vecteur √©loign√©\n",
    "```\n",
    "\n",
    "Plusieurs approches existent pour construire ces vecteurs. Dans ce TP, nous explorerons **Word2Vec**, une m√©thode remarquable qui a r√©volutionn√© le NLP en 2013.\n",
    "\n",
    "---\n",
    "\n",
    "## Plan du TP\n",
    "\n",
    "| Section | Probl√®me trait√© | Ce que vous apprendrez |\n",
    "|---------|-----------------|------------------------|\n",
    "| ¬ß1-2 | Tokenization | Les 3 strat√©gies (mots, caract√®res, BPE) |\n",
    "| ¬ß3-4 | Embedding | Comment les vecteurs capturent le sens (Word2Vec) |\n",
    "| ¬ß5 | Bonus | Teaser sur le m√©canisme d'attention |\n",
    "\n",
    "Commen√ßons par le premier probl√®me : **comment d√©couper le texte ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports\n",
    "\n",
    "Ex√©cutez cette cellule pour installer les d√©pendances n√©cessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab)\n",
    "!pip install torch matplotlib numpy gensim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Partie 1 : La Tokenization\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Tokenization : D√©couper le texte\n",
    "\n",
    "La **tokenization** consiste √† d√©couper le texte en unit√©s (tokens). Il existe plusieurs strat√©gies.\n",
    "\n",
    "### 2.1 Tokenization par mots (Word-level)\n",
    "\n",
    "La plus intuitive : on d√©coupe sur les espaces et la ponctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization simple par mots\n",
    "def tokenize_words(text):\n",
    "    \"\"\"Tokenization basique par espaces et ponctuation.\"\"\"\n",
    "    import re\n",
    "    # S√©pare sur espaces et garde la ponctuation comme tokens\n",
    "    return \"\" #TODO\n",
    "\n",
    "texte = \"Le chat mange la souris. La souris court vite !\"\n",
    "tokens = tokenize_words(texte)\n",
    "\n",
    "print(f\"Texte : {texte}\")\n",
    "print(f\"Tokens : {tokens}\")\n",
    "print(f\"Nombre de tokens : {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Probl√®me** : Le vocabulaire peut devenir √©norme !\n\nAjoutez les noms propres, les n√©ologismes, les mots √©trangers, les fautes de frappe... Le vocabulaire explose.\n\n√Ä un moment, il faut **fixer une taille de vocabulaire** (ex: 50 000 mots). Mais alors, que faire des mots inconnus ?\n\n```\nVocabulaire : [\"le\", \"chat\", \"mange\", ...] (50 000 mots)\nNouveau mot : \"anticonstitutionnellement\" ‚Üí <UNK> ?\n```\n\n‚Üí On perd l'information. Pas id√©al."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization par caract√®res (Character-level)\n",
    "\n",
    "Une solution : d√©couper caract√®re par caract√®re. Plus de mots inconnus !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_chars(text):\n",
    "    \"\"\"Tokenization par caract√®res.\"\"\"\n",
    "    return \"\" #TODO\n",
    "\n",
    "texte = \"Le chat dort.\"\n",
    "tokens = tokenize_chars(texte)\n",
    "\n",
    "print(f\"Texte : {texte}\")\n",
    "print(f\"Tokens : {tokens}\")\n",
    "print(f\"Nombre de tokens : {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Probl√®me historique** : S√©quences tr√®s longues ! \"anticonstitutionnellement\" = 25 tokens.\n\nLe mod√®le doit \"r√©apprendre\" que `c-h-a-t` forme le concept de chat. Le co√ªt computationnel √©tait longtemps consid√©r√© comme prohibitif.\n\n---\n\n> üìö **Pour aller plus loin : le retour du byte-level**\n> \n> Des travaux r√©cents montrent que la tokenization byte-level redevient comp√©titive !\n> \n> **Bolmo** (Allen AI, 2024) op√®re directement sur les **bytes UTF-8** (256 tokens possibles) avec une architecture adapt√©e :\n> - Un encodeur local (mLSTM) traite les bytes\n> - Un \"boundary predictor\" regroupe les bytes en patches de taille variable\n> - Le Transformer traite ces patches (pas les bytes bruts)\n> \n> **Avantages** : pas de vocabulaire fig√©, robuste aux typos, meilleure compr√©hension caract√®re.\n\nPour ce TP, nous utiliserons **BPE** qui reste le standard actuel, mais gardez en t√™te que le domaine √©volue rapidement !"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.3 Tokenization Subword : BPE (Byte Pair Encoding)\n\nLes deux approches pr√©c√©dentes ont des d√©fauts :\n- **Mots** : vocabulaire √©norme + mots inconnus\n- **Caract√®res** : s√©quences trop longues + perte de sens\n\n**BPE** (Byte Pair Encoding) est un **compromis intelligent** utilis√© par GPT, BERT, et tous les LLMs modernes.\n\n---\n\n#### Le principe\n\nBPE construit son vocabulaire en analysant un grand corpus de texte :\n\n1. **D√©part** : vocabulaire = tous les caract√®res\n2. **R√©p√©ter** : trouver la paire de tokens adjacents la plus fr√©quente ‚Üí la fusionner en un nouveau token\n3. **Stop** : quand le vocabulaire atteint la taille voulue (ex: 50 000 tokens)\n\n---\n\n#### L'algorithme pas √† pas\n\nPour comprendre, prenons un **corpus artificiel simplifi√©** :\n```\n\"smartphone smartphone smartphone smartwatch smartwatch phone phone\"\n```\n\n**√âtape 0 : Partir des caract√®res**\n```\nVocabulaire : {s, m, a, r, t, p, h, o, n, e, w, c}\n```\n\n**√âtapes suivantes : Fusionner les paires les plus fr√©quentes**\n\n| √âtape | Paire la + fr√©quente | Nouveau token |\n|-------|---------------------|---------------|\n| 1 | (s, m) | \"sm\" |\n| 2 | (sm, a) | \"sma\" |\n| 3 | (sma, r) | \"smar\" |\n| 4 | (smar, t) | \"smart\" |\n| 5 | (p, h) | \"ph\" |\n| 6 | (ph, o) | \"pho\" |\n| 7 | (pho, n) | \"phon\" |\n| 8 | (phon, e) | \"phone\" |\n\n**R√©sultat :**\n```\n\"smartphone\" ‚Üí [\"smart\", \"phone\"]  ‚Üê 2 tokens r√©utilisables !\n\"smartwatch\" ‚Üí [\"smart\", \"watch\"]\n\"phone\"      ‚Üí [\"phone\"]\n```\n\n---\n\n#### Pourquoi c'est malin ?\n\nUn mot **jamais vu** comme \"smartcar\" sera d√©coup√© en :\n```\n\"smartcar\" ‚Üí [\"smart\", \"car\"]\n```\n\nLe mod√®le conna√Æt d√©j√† \"smart\" ! Pas besoin de token `<UNK>`.\n\n**Bonus** : les sous-mots fr√©quents ont de **bons embeddings** (on verra pourquoi dans la section Word2Vec). Donc m√™me un mot rare b√©n√©ficie de repr√©sentations de qualit√© via ses composants.\n\n**Le meilleur des deux mondes** :\n- Mots fr√©quents ‚Üí tokens entiers (efficace)\n- Mots rares/nouveaux ‚Üí sous-mots connus (robuste)\n\n**Ressource** : [Explication d√©taill√©e des tokenizers (FR)](https://lbourdois.github.io/blog/nlp/Les-tokenizers/)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration avec un vrai tokenizer (GPT-2)\n",
    "!pip install transformers -q\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "textes = [\n",
    "    \"Le chat mange.\",\n",
    "    \"anticonstitutionnellement\",\n",
    "    \"Hello world!\",\n",
    "    \"Transformers are amazing!\"\n",
    "]\n",
    "\n",
    "print(\"=== Tokenization BPE (GPT-2) ===\")\n",
    "for texte in textes:\n",
    "    tokens = tokenizer.tokenize(texte)\n",
    "    ids = tokenizer.encode(texte)\n",
    "    print(f\"\\n'{texte}'\")\n",
    "    print(f\"  Tokens : {tokens}\")\n",
    "    print(f\"  IDs    : {ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations** :\n",
    "- Les mots courants anglais sont souvent des tokens uniques\n",
    "- Les mots fran√ßais/rares sont d√©coup√©s\n",
    "- Le caract√®re `ƒ†` indique un espace avant le token\n",
    "\n",
    "### Exercice 1 : Comparer les tokenizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Comparer les tokenizations\n",
    "# ============================================\n",
    "\n",
    "texte_test = \"L'intelligence artificielle r√©volutionne le monde.\"\n",
    "\n",
    "# TODO: Tokenizer avec les 3 m√©thodes et comparer le nombre de tokens\n",
    "\n",
    "# 1. Par mots\n",
    "tokens_mots = None  # tokenize_words(texte_test)\n",
    "\n",
    "# 2. Par caract√®res  \n",
    "tokens_chars = None  # tokenize_chars(texte_test)\n",
    "\n",
    "# 3. BPE (GPT-2)\n",
    "tokens_bpe = None  # tokenizer.tokenize(texte_test)\n",
    "\n",
    "print(f\"Texte : {texte_test}\")\n",
    "print(f\"\\nMots      : {len(tokens_mots) if tokens_mots else '?'} tokens\")\n",
    "print(f\"Caract√®res: {len(tokens_chars) if tokens_chars else '?'} tokens\")\n",
    "print(f\"BPE       : {len(tokens_bpe) if tokens_bpe else '?'} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Construction d'un vocabulaire\n",
    "\n",
    "Une fois la strat√©gie choisie, on construit un **vocabulaire** : une table de correspondance token ‚Üî index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construire un vocabulaire simple\n",
    "corpus = [\n",
    "    \"le chat mange\",\n",
    "    \"le chien dort\",\n",
    "    \"la souris court\"\n",
    "]\n",
    "\n",
    "# Collecter tous les tokens uniques\n",
    "all_tokens = set()\n",
    "for phrase in corpus:\n",
    "    all_tokens.update(tokenize_words(phrase))\n",
    "\n",
    "# Cr√©er le vocabulaire avec tokens sp√©ciaux\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # Padding et Unknown\n",
    "for i, token in enumerate(sorted(all_tokens)):\n",
    "    vocab[token] = i + 2\n",
    "\n",
    "# Vocabulaire inverse\n",
    "id_to_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "print(\"Vocabulaire :\")\n",
    "for token, idx in vocab.items():\n",
    "    print(f\"  {idx}: '{token}'\")\n",
    "\n",
    "# Encoder une phrase\n",
    "def encode(text, vocab):\n",
    "    return [vocab.get(t, vocab[\"<UNK>\"]) for t in tokenize_words(text)]\n",
    "\n",
    "def decode(ids, id_to_token):\n",
    "    return [id_to_token[i] for i in ids]\n",
    "\n",
    "phrase = \"le chat court\"\n",
    "encoded = encode(phrase, vocab)\n",
    "decoded = decode(encoded, id_to_token)\n",
    "\n",
    "print(f\"\\nPhrase : '{phrase}'\")\n",
    "print(f\"Encod√© : {encoded}\")\n",
    "print(f\"D√©cod√© : {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Embeddings : Des indices aux vecteurs\n",
    "\n",
    "### Le probl√®me des indices\n",
    "\n",
    "Les indices (0, 1, 2, ...) n'ont pas de **sens s√©mantique**. \n",
    "\n",
    "- `chat = 3` et `chien = 4` ‚Üí sont-ils proches ? (oui, ce sont des animaux)\n",
    "- `chat = 3` et `voiture = 42` ‚Üí sont-ils proches ? (non)\n",
    "\n",
    "Mais avec des indices, on ne peut pas mesurer cette proximit√© !\n",
    "\n",
    "### La solution : Embeddings\n",
    "\n",
    "On associe √† chaque token un **vecteur dense** de dimension fixe (ex: 128, 256, 768).\n",
    "\n",
    "```\n",
    "\"chat\"    ‚Üí [0.2, -0.5, 0.8, 0.1, ...] (128 dimensions)\n",
    "\"chien\"   ‚Üí [0.3, -0.4, 0.7, 0.2, ...] (proche de chat !)\n",
    "\"voiture\" ‚Üí [-0.8, 0.2, -0.3, 0.9, ...] (loin de chat)\n",
    "```\n",
    "\n",
    "Ces vecteurs sont **appris** pendant l'entra√Ænement du mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration : Embedding en PyTorch\n",
    "vocab_size = 10  # 10 tokens dans notre vocabulaire\n",
    "embed_dim = 4    # Chaque token ‚Üí vecteur de dimension 4\n",
    "\n",
    "# Cr√©er une couche d'embedding\n",
    "embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "print(f\"Matrice d'embedding : {embedding.weight.shape}\")\n",
    "print(f\"  ‚Üí {vocab_size} tokens √ó {embed_dim} dimensions\\n\")\n",
    "\n",
    "# R√©cup√©rer l'embedding d'un token\n",
    "token_id = torch.tensor([3])  # Token d'indice 3\n",
    "vector = embedding(token_id)\n",
    "\n",
    "print(f\"Token 3 ‚Üí {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding d'une s√©quence compl√®te\n",
    "sequence = torch.tensor([2, 5, 3, 7])  # 4 tokens\n",
    "embedded = embedding(sequence)\n",
    "\n",
    "print(f\"S√©quence : {sequence.tolist()}\")\n",
    "print(f\"Shape apr√®s embedding : {embedded.shape}\")  # (4, 4)\n",
    "print(f\"\\nVecteurs :\\n{embedded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesurer la similarit√© : Distance cosinus\n",
    "\n",
    "Avec des vecteurs, on peut mesurer la **similarit√©** entre mots :\n",
    "\n",
    "$$\\text{similarit√©}(A, B) = \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
    "\n",
    "- R√©sultat entre -1 (oppos√©s) et 1 (identiques)\n",
    "- 0 = orthogonaux (pas de relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calcule la similarit√© cosinus entre deux vecteurs.\"\"\"\n",
    "    return torch.dot(a, b) / (torch.norm(a) * torch.norm(b))\n",
    "\n",
    "# Exemple avec nos embeddings al√©atoires\n",
    "vec_2 = embedding(torch.tensor(2))\n",
    "vec_3 = embedding(torch.tensor(3))\n",
    "vec_7 = embedding(torch.tensor(7))\n",
    "\n",
    "sim_2_3 = cosine_similarity(vec_2, vec_3)\n",
    "sim_2_7 = cosine_similarity(vec_2, vec_7)\n",
    "\n",
    "print(f\"Similarit√©(token 2, token 3) = {sim_2_3:.4f}\")\n",
    "print(f\"Similarit√©(token 2, token 7) = {sim_2_7:.4f}\")\n",
    "print(\"\\n(Valeurs al√©atoires car embeddings non entra√Æn√©s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. Word2Vec : Apprendre des embeddings qui ont du sens\n\nJusqu'ici, nos embeddings √©taient **al√©atoires** (initialis√©s au hasard dans PyTorch). Comment obtenir des vecteurs o√π \"chat\" et \"chien\" sont vraiment proches ?\n\n**Word2Vec** (Mikolov et al., 2013) a r√©volutionn√© le NLP en montrant qu'on peut **apprendre** des embeddings √† partir de texte brut, sans supervision.\n\n---\n\n### L'intuition fondamentale\n\n> **\"Tu connais un mot par les mots qui l'entourent\"** (hypoth√®se distributionnelle)\n\nObservez ces phrases :\n```\n\"Le chat mange sa p√¢t√©e\"\n\"Le chat dort sur le canap√©\"  \n\"Mon chat joue avec une balle\"\n\n\"Le chien mange sa p√¢t√©e\"\n\"Le chien dort sur le canap√©\"\n\"Mon chien joue avec une balle\"\n```\n\n\"Chat\" et \"chien\" apparaissent dans les **m√™mes contextes**. Word2Vec va leur attribuer des vecteurs similaires.\n\n---\n\n### Comment Word2Vec apprend ? (Skip-gram)\n\nL'id√©e est simple : entra√Æner un r√©seau √† **pr√©dire les mots du contexte** √† partir d'un mot central.\n\n```\nPhrase : \"Le chat noir dort paisiblement\"\n                  ‚Üë\n            mot central\n\nFen√™tre de contexte (¬±2 mots) :\n    Entr√©e : \"noir\"\n    Cibles : [\"chat\", \"dort\"] (le r√©seau essaie de les pr√©dire)\n```\n\n**Architecture simplifi√©e :**\n\n```\n    \"noir\"                      \"chat\" ?\n      ‚Üì                            ‚Üë\n ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n ‚îÇ Embedding ‚îÇ  ‚Üí vecteur ‚Üí  ‚îÇ Pr√©diction‚îÇ\n ‚îÇ (lookup)  ‚îÇ    256 dim    ‚îÇ (softmax) ‚îÇ\n ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Pendant l'entra√Ænement :**\n- Le r√©seau voit des millions de paires (mot central, mot contexte)\n- Il ajuste les embeddings pour que les mots apparaissant dans les m√™mes contextes aient des vecteurs proches\n- Les embeddings **√©mergent** de cette t√¢che de pr√©diction\n\n---\n\n### Cons√©quence : mots fr√©quents vs mots rares\n\nPlus un mot est **fr√©quent** dans le corpus, plus son embedding est ajust√© souvent ‚Üí meilleure qualit√©.\n\nUn mot vu 100 000 fois aura un excellent embedding. Un mot vu 3 fois restera proche de son initialisation al√©atoire.\n\n> C'est pour √ßa que BPE aide : un mot rare comme \"smartcar\" est d√©coup√© en \"smart\" + \"car\", deux sous-mots tr√®s fr√©quents avec d'excellents embeddings !\n\n---\n\n### Pourquoi les analogies marchent ?\n\nApr√®s entra√Ænement, les vecteurs encodent des **relations** :\n\n```\nvecteur(\"roi\") - vecteur(\"homme\") ‚âà vecteur(\"reine\") - vecteur(\"femme\")\n```\n\nAutrement dit, la \"direction\" homme‚Üífemme dans l'espace vectoriel est la m√™me que roi‚Üíreine :\n\n```\n        homme ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí femme\n          ‚Üë    (m√™me         ‚Üë\n          ‚îÇ   direction)     ‚îÇ\n         roi ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí reine\n```\n\nC'est pour √ßa que `roi - homme + femme ‚âà reine` fonctionne !\n\n---\n\n### Chargeons un mod√®le pr√©-entra√Æn√©\n\nNous allons utiliser **GloVe** (similaire √† Word2Vec), entra√Æn√© sur Wikipedia."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un mod√®le Word2Vec pr√©-entra√Æn√©\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(\"Chargement du mod√®le Word2Vec (peut prendre 1-2 min)...\")\n",
    "model = api.load(\"glove-wiki-gigaword-100\")  # 100 dimensions, entra√Æn√© sur Wikipedia\n",
    "print(f\"Mod√®le charg√© ! Vocabulaire : {len(model)} mots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorer les similarit√©s\n",
    "print(\"=== Mots similaires √† 'king' ===\")\n",
    "for word, score in model.most_similar(\"king\", topn=5):\n",
    "    print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n=== Mots similaires √† 'computer' ===\")\n",
    "for word, score in model.most_similar(\"computer\", topn=5):\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 2 : Explorer les similarit√©s\n",
    "# ============================================\n",
    "\n",
    "# TODO: Trouver les 5 mots les plus similaires √† :\n",
    "# - \"france\"\n",
    "# - \"cat\" (chat en anglais)\n",
    "# - \"happy\"\n",
    "\n",
    "# Exemple :\n",
    "# for word, score in model.most_similar(\"france\", topn=5):\n",
    "#     print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La magie des analogies : king - man + woman = ?\n",
    "print(\"=== Analogie : king - man + woman = ? ===\")\n",
    "result = model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=3)\n",
    "for word, score in result:\n",
    "    print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n=== Analogie : paris - france + italy = ? ===\")\n",
    "result = model.most_similar(positive=[\"paris\", \"italy\"], negative=[\"france\"], topn=3)\n",
    "for word, score in result:\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 3 : Trouver des analogies\n",
    "# ============================================\n",
    "\n",
    "# TODO: Tester ces analogies (et en inventer d'autres !)\n",
    "# - \"berlin\" - \"germany\" + \"france\" = ?\n",
    "# - \"good\" - \"better\" + \"bad\" = ?\n",
    "# - \"cat\" - \"kitten\" + \"dog\" = ?\n",
    "\n",
    "# Syntaxe :\n",
    "# model.most_similar(positive=[\"A\", \"C\"], negative=[\"B\"], topn=3)\n",
    "# Pour calculer A - B + C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des embeddings en 2D\n",
    "!pip install scikit-learn -q\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# S√©lectionner quelques mots\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"prince\", \"princess\",\n",
    "         \"cat\", \"dog\", \"lion\", \"tiger\",\n",
    "         \"car\", \"bus\", \"train\", \"plane\"]\n",
    "\n",
    "# R√©cup√©rer leurs vecteurs\n",
    "vectors = np.array([model[w] for w in words])\n",
    "\n",
    "# R√©duire √† 2D avec PCA\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(vectors)\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], c='blue', s=100)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (vectors_2d[i, 0] + 0.1, vectors_2d[i, 1] + 0.1), fontsize=12)\n",
    "\n",
    "plt.title(\"Embeddings Word2Vec projet√©s en 2D\")\n",
    "plt.xlabel(\"Composante 1\")\n",
    "plt.ylabel(\"Composante 2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation : Les mots de m√™me cat√©gorie sont regroup√©s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Teaser : Le m√©canisme d'attention\n",
    "\n",
    "Maintenant que nous savons repr√©senter du texte (tokenization + embeddings), la prochaine √©tape est de permettre aux mots de **communiquer entre eux**.\n",
    "\n",
    "C'est le r√¥le du **m√©canisme d'attention**, que nous verrons en d√©tail au prochain TP.\n",
    "\n",
    "### L'id√©e\n",
    "\n",
    "> Pour comprendre un mot, il faut regarder les autres mots de la phrase.\n",
    "\n",
    "Exemple : *\"Le chat qui dormait sur le canap√© a saut√©\"*\n",
    "- Pour comprendre **\"a saut√©\"** ‚Üí regarder **\"chat\"** (le sujet)\n",
    "- Pour comprendre **\"dormait\"** ‚Üí regarder **\"chat\"** et **\"canap√©\"**\n",
    "\n",
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice d'attention simul√©e\n",
    "phrase = [\"Le\", \"chat\", \"mange\", \"la\", \"souris\"]\n",
    "\n",
    "# Chaque ligne = un mot qui \"regarde\" les autres\n",
    "# Valeurs = poids d'attention (somme = 1 par ligne)\n",
    "attention = torch.tensor([\n",
    "    [0.8, 0.1, 0.05, 0.03, 0.02],  # \"Le\" regarde surtout lui-m√™me\n",
    "    [0.1, 0.7, 0.1, 0.05, 0.05],   # \"chat\" regarde surtout lui-m√™me\n",
    "    [0.05, 0.4, 0.4, 0.05, 0.1],   # \"mange\" regarde \"chat\" et lui-m√™me\n",
    "    [0.02, 0.03, 0.05, 0.8, 0.1],  # \"la\" regarde surtout lui-m√™me\n",
    "    [0.02, 0.1, 0.2, 0.08, 0.6],   # \"souris\" regarde \"mange\" et elle-m√™me\n",
    "])\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention, cmap='Blues')\n",
    "plt.xticks(range(5), phrase)\n",
    "plt.yticks(range(5), phrase)\n",
    "plt.xlabel(\"Mots regard√©s\")\n",
    "plt.ylabel(\"Mots qui regardent\")\n",
    "plt.title(\"Qui regarde qui ? (Matrice d'attention)\")\n",
    "plt.colorbar(label=\"Poids d'attention\")\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        plt.text(j, i, f'{attention[i,j]:.2f}', \n",
    "                ha='center', va='center',\n",
    "                color='white' if attention[i,j] > 0.5 else 'black')\n",
    "plt.show()\n",
    "\n",
    "print(\"Le verbe 'mange' regarde fortement 'chat' (son sujet) !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ce qu'on voit** :\n",
    "- Chaque mot peut \"regarder\" tous les autres mots\n",
    "- Les poids indiquent l'importance de chaque relation\n",
    "- Le mod√®le **apprend** ces poids pendant l'entra√Ænement\n",
    "\n",
    "**Au prochain TP**, nous verrons :\n",
    "- Pourquoi les RNN/LSTM ont des limites\n",
    "- Comment calculer cette matrice d'attention\n",
    "- Les concepts Query, Key, Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. R√©capitulatif\n",
    "\n",
    "### Le pipeline NLP\n",
    "\n",
    "| √âtape | Entr√©e | Sortie | R√¥le |\n",
    "|-------|--------|--------|------|\n",
    "| **Tokenization** | Texte brut | Liste de tokens | D√©couper le texte |\n",
    "| **Vocabulaire** | Tokens | Indices | Table token ‚Üî ID |\n",
    "| **Embedding** | Indices | Vecteurs denses | Sens s√©mantique |\n",
    "\n",
    "### Points cl√©s\n",
    "\n",
    "1. **Tokenization BPE** : meilleur compromis entre mots et caract√®res\n",
    "2. **Embeddings** : transforment les mots en vecteurs comparables\n",
    "3. **Word2Vec** : montre que les embeddings capturent le sens (analogies !)\n",
    "4. **Similarit√© cosinus** : mesure la proximit√© entre vecteurs\n",
    "\n",
    "### Prochaine session\n",
    "\n",
    "Nous verrons le **m√©canisme d'attention** : comment les mots \"communiquent\" entre eux pour se comprendre mutuellement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 7. Pour aller plus loin (optionnel)\n\n### Ressources\n\n- [Les tokenizers en NLP (FR)](https://lbourdois.github.io/blog/nlp/Les-tokenizers/) - Excellent article en fran√ßais\n- [The Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/) - Visualisations tr√®s claires\n- [Bolmo: Byte-level Language Models (Allen AI, 2024)](https://allenai.org/blog/bolmo) - L'avenir de la tokenization ?\n\n### Exp√©rimentations sugg√©r√©es\n\n1. Tester d'autres analogies Word2Vec\n2. Visualiser les embeddings de votre choix\n3. Comparer diff√©rents tokenizers (BERT, GPT-2, etc.)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}