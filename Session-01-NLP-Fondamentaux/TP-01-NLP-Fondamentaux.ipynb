{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# TP 01 - Fondamentaux NLP pour les Transformers\n\n**Module** : R√©seaux de Neurones Approfondissement  \n**Dur√©e** : 2h  \n**Objectif** : Mieux comprendre comment une machine peut interpr√©ter du texte\n\n---\n\n## Comment faire interpr√©ter du texte par une machine ?\n\nSi vous avez d√©j√† travaill√© avec des **images** le probl√®me est de prime abord plus simple : une image peut naturellement se d√©couper en une grille de pixels, chaque pixel est un nombre (0-255). Le r√©seau peut directement les traiter.\n\nMais pour le **texte** suivant ?\n\n```\n\"L'apprentissage automatique r√©volutionne l'intelligence artificielle\"\n```\n\nCe n'est qu'une suite de caract√®res. Un r√©seau de neurones, par exemple, ne comprend que des **nombres**. Comment passer de l'un √† l'autre ?\n\n---\n\n## Les probl√®mes √† r√©soudre\n\nPour transformer du texte en repr√©sentation num√©rique exploitable, il faut r√©soudre **deux probl√®mes distincts** :\n\n### Probl√®me 1 : La tokenization\n\n**Comment d√©couper le texte suivant en morceaux ?**\n\n```\n\"L'apprentissage automatique\" ‚Üí ???\n```\n\nPlusieurs strat√©gies sont possibles :\n- Par mots : `[\"L'apprentissage\", \"automatique\"]`\n- Par caract√®res : `[\"L\", \"'\", \"a\", \"p\", \"p\", \"r\", ...]`\n- Par sous-mots : `[\"L'\", \"apprent\", \"issage\", \"auto\", \"matique\"]`\n\nChaque strat√©gie a ses avantages et inconv√©nients. Nous les explorerons dans ce TP.\n\n### Probl√®me 2 : L'embedding\n\n**Comment transformer ces morceaux en vecteurs qui ont du SENS ?**\n\nUne fois le texte d√©coup√©, on pourrait simplement num√©roter les tokens :\n```\n\"chat\" ‚Üí 42\n\"voiture\" ‚Üí 46\n\"chien\" ‚Üí 73\n```\n\nMais ces nombres sont **arbitraires**. Ils ne capturent pas que \"chat\" et \"chien\" sont des concepts proches (animaux domestiques), alors que \"voiture\" est compl√®tement diff√©rent.\n\n**Il faut trouver un moyen** de transformer chaque token en un **vecteur de plusieurs dimensions** o√π la **proximit√© g√©om√©trique** refl√®te la **proximit√© s√©mantique** :\n\n```\n\"chat\"    ‚Üí [0.2, -0.5, 0.8, ...]   ‚îê\n                                    ‚îú‚îÄ vecteurs proches !\n\"chien\"   ‚Üí [0.3, -0.4, 0.7, ...]   ‚îò\n\n\"voiture\" ‚Üí [-0.8, 0.2, -0.3, ...]  ‚Üê vecteur √©loign√©\n```\n\nPlusieurs approches existent pour construire ces vecteurs. Dans ce TP, nous explorerons **Word2Vec**, une m√©thode remarquable qui a r√©volutionn√© le NLP en 2013.\n\n---\n\n## Plan du TP\n\n| Section | Th√®me | Ce que vous apprendrez |\n|---------|-------|------------------------|\n| ¬ß2 | Tokenization | Les 3 strat√©gies (mots, caract√®res, BPE) |\n| ¬ß3-4 | Embeddings | Comment les vecteurs capturent le sens (Word2Vec) |\n| ¬ß5 | Attention (teaser) | Aper√ßu du m√©canisme cl√© des Transformers |\n| ¬ß6 | R√©capitulatif | Synth√®se des concepts |\n| ¬ß7 | Ressources | Pour aller plus loin |\n| ¬ß8 | Mini-projet | Pipeline NLP complet (optionnel) |\n\nCommen√ßons par le premier probl√®me : **comment d√©couper le texte ?**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports\n",
    "\n",
    "Ex√©cutez cette cellule pour installer les d√©pendances n√©cessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab)\n",
    "!pip install torch matplotlib numpy gensim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Partie 1 : La Tokenization\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Tokenization : D√©couper le texte\n",
    "\n",
    "La **tokenization** consiste √† d√©couper le texte en unit√©s (tokens). Il existe plusieurs strat√©gies.\n",
    "\n",
    "### 2.1 Tokenization par mots (Word-level)\n",
    "\n",
    "La plus intuitive : on d√©coupe sur les espaces et la ponctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Exercice 0a : Impl√©menter la tokenization par mots\n\nCompl√©tez la fonction `tokenize_words` ci-dessous. Elle doit :\n- S√©parer le texte sur les espaces\n- Garder la ponctuation comme tokens s√©par√©s\n\n**Indice** : Utilisez `re.findall()` avec le pattern `r\"\\w+|[^\\w\\s]\"` qui capture les mots OU la ponctuation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization simple par mots\n",
    "def tokenize_words(text):\n",
    "    \"\"\"Tokenization basique par espaces et ponctuation.\"\"\"\n",
    "    import re\n",
    "    # S√©pare sur espaces et garde la ponctuation comme tokens\n",
    "    return \"\" #TODO\n",
    "\n",
    "texte = \"Le chat mange la souris. La souris court vite !\"\n",
    "tokens = tokenize_words(texte)\n",
    "\n",
    "print(f\"Texte : {texte}\")\n",
    "print(f\"Tokens : {tokens}\")\n",
    "print(f\"Nombre de tokens : {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Probl√®me** : Le vocabulaire peut devenir √©norme !\n\nAjoutez les noms propres, les n√©ologismes, les mots √©trangers, les fautes de frappe... Le vocabulaire explose.\n\n√Ä un moment, il faut **fixer une taille de vocabulaire** (ex: 50 000 mots). Mais alors, que faire des mots inconnus ?\n\nImaginons un vocabulaire constitu√© au pr√©alable :\n```\n[\"le\", \"chat\", \"mange\", ...] (50 000 mots)\n```\nSi un mot trait√© n'appartient pas au vocabulaire, l'information est perdue\n```\nNouveau mot : \"transformers\" ‚Üí <UNK> ?\n```\n‚Üí Pas id√©al."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization par caract√®res (Character-level)\n",
    "\n",
    "Une solution : d√©couper caract√®re par caract√®re. Plus de mots inconnus !"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercice 0b : Impl√©menter la tokenization par caract√®res\n",
    "\n",
    "Compl√©tez la fonction `tokenize_chars` ci-dessous.\n",
    "\n",
    "**Indice** : En Python, une cha√Æne est d√©j√† it√©rable caract√®re par caract√®re..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_chars(text):\n",
    "    \"\"\"Tokenization par caract√®res.\"\"\"\n",
    "    return \"\" #TODO\n",
    "\n",
    "texte = \"Le chat dort.\"\n",
    "tokens = tokenize_chars(texte)\n",
    "\n",
    "print(f\"Texte : {texte}\")\n",
    "print(f\"Tokens : {tokens}\")\n",
    "print(f\"Nombre de tokens : {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Probl√®me historique** : S√©quences tr√®s longues ! \"anticonstitutionnellement\" = 25 tokens.\n\nLe mod√®le doit \"r√©apprendre\" que `c-h-a-t` forme le concept de chat. Le co√ªt computationnel √©tait longtemps consid√©r√© comme prohibitif.\n\n---\n\n> üìö **Des travaux r√©cents montrent que la tokenization byte-level peut devenir comp√©titive**\n>\n> \n> **Bolmo** (Allen AI, 2025) op√®re directement sur les **bytes UTF-8** (256 tokens possibles) avec une architecture adapt√©e :\n> - Un encodeur local (mLSTM) traite les bytes\n> - Un \"boundary predictor\" regroupe les bytes en patches de taille variable\n> - Le Transformer traite ces patches (pas les bytes bruts)\n> \n> **Avantages** : pas de vocabulaire fig√©, robuste aux typos, meilleure compr√©hension caract√®re.\n\nPour ce TP, nous utiliserons **BPE** que nous allons voir tout de suite et qui reste le standard actuel, mais gardez en t√™te que le domaine √©volue rapidement !"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tokenization Subword : BPE (Byte Pair Encoding)\n",
    "\n",
    "Les deux approches pr√©c√©dentes ont des d√©fauts :\n",
    "- **Mots** : vocabulaire √©norme + mots inconnus\n",
    "- **Caract√®res** : s√©quences trop longues + perte de sens\n",
    "\n",
    "**BPE** (Byte Pair Encoding) est un **compromis intelligent** utilis√© par GPT, BERT, et une grande partie des LLMs modernes.\n",
    "\n",
    "---\n",
    "\n",
    "#### Le principe\n",
    "\n",
    "BPE construit son vocabulaire en analysant un grand corpus de texte :\n",
    "\n",
    "1. **D√©part** : vocabulaire = tous les caract√®res\n",
    "2. **R√©p√©ter** : trouver la paire de tokens adjacents la plus fr√©quente ‚Üí la fusionner en un nouveau token\n",
    "3. **Stop** : quand le vocabulaire atteint la taille voulue (ex: 50 000 tokens)\n",
    "\n",
    "---\n",
    "\n",
    "#### L'algorithme pas √† pas\n",
    "\n",
    "Pour comprendre, prenons un **corpus artificiel simplifi√©** :\n",
    "```\n",
    "\"smartphone smartphone smartphone smartwatch smartwatch phone phone\"\n",
    "```\n",
    "\n",
    "**√âtape 0 : Partir des caract√®res**\n",
    "```\n",
    "Vocabulaire : {s, m, a, r, t, p, h, o, n, e, w, c}\n",
    "```\n",
    "\n",
    "**√âtapes suivantes : Fusionner les paires les plus fr√©quentes**\n",
    "\n",
    "| √âtape | Paire la + fr√©quente | Nouveau token |\n",
    "|-------|---------------------|---------------|\n",
    "| 1 | (s, m) | \"sm\" |\n",
    "| 2 | (sm, a) | \"sma\" |\n",
    "| 3 | (sma, r) | \"smar\" |\n",
    "| 4 | (smar, t) | \"smart\" |\n",
    "| 5 | (p, h) | \"ph\" |\n",
    "| 6 | (ph, o) | \"pho\" |\n",
    "| 7 | (pho, n) | \"phon\" |\n",
    "| 8 | (phon, e) | \"phone\" |\n",
    "\n",
    "**R√©sultat :**\n",
    "```\n",
    "\"smartphone\" ‚Üí [\"smart\", \"phone\"]  ‚Üê 2 tokens r√©utilisables !\n",
    "\"smartwatch\" ‚Üí [\"smart\", \"watch\"]\n",
    "\"phone\"      ‚Üí [\"phone\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Pourquoi c'est malin ?\n",
    "\n",
    "Un mot **jamais vu** comme \"smartcar\" sera d√©coup√© en :\n",
    "```\n",
    "\"smartcar\" ‚Üí [\"smart\", \"car\"]\n",
    "```\n",
    "\n",
    "Le mod√®le conna√Æt d√©j√† \"smart\" ! Pas besoin de token `<UNK>`.\n",
    "\n",
    "**Bonus** : les sous-mots fr√©quents ont de **bons embeddings** (on verra pourquoi dans la section Word2Vec). Donc m√™me un mot rare peut b√©n√©ficier de repr√©sentations de qualit√© via ses composants.\n",
    "\n",
    "**Nous obtenons le meilleur des deux mondes** :\n",
    "- Mots fr√©quents ‚Üí tokens entiers (efficace)\n",
    "- Mots rares/nouveaux ‚Üí sous-mots connus (robuste)\n",
    "\n",
    "Bien que ce mod√®le soit tr√®s performant dans un grand nombre de cas, il faut rester conscient que certains mots tr√®s sp√©cifiques peuvent avoir une repr√©sentation de moindre qualit√©.\n",
    "\n",
    "**Ressource** : [Explication d√©taill√©e des tokenizers (FR)](https://lbourdois.github.io/blog/nlp/Les-tokenizers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n#### En pratique : le tokenizer de GPT-2\n\nVoyons maintenant comment un **vrai tokenizer BPE** d√©coupe du texte. Nous allons utiliser celui de GPT-2 via la biblioth√®que Hugging Face `transformers`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration avec un vrai tokenizer (GPT-2)\n",
    "!pip install transformers -q\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "textes = [\n",
    "    \"Le chat mange.\",\n",
    "    \"anticonstitutionnellement\",\n",
    "    \"Hello world!\",\n",
    "    \"Transformers are amazing!\"\n",
    "]\n",
    "\n",
    "print(\"=== Tokenization BPE (GPT-2) ===\")\n",
    "for texte in textes:\n",
    "    tokens = gpt2_tokenizer.tokenize(texte)\n",
    "    ids = gpt2_tokenizer.encode(texte)\n",
    "    print(f\"\\n'{texte}'\")\n",
    "    print(f\"  Tokens : {tokens}\")\n",
    "    print(f\"  IDs    : {ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations** :\n",
    "- Les mots courants anglais sont souvent des tokens uniques\n",
    "- Les mots fran√ßais/rares sont d√©coup√©s\n",
    "- Le caract√®re `ƒ†` indique un espace avant le token\n",
    "\n",
    "### Exercice 1 : Comparer les tokenizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Comparer les tokenizations\n",
    "# ============================================\n",
    "\n",
    "texte_test = \"L'intelligence artificielle r√©volutionne le monde.\"\n",
    "\n",
    "# TODO: Tokenizer avec les 3 m√©thodes et comparer le nombre de tokens\n",
    "\n",
    "# 1. Par mots\n",
    "tokens_mots = None\n",
    "\n",
    "# 2. Par caract√®res  \n",
    "tokens_chars = None\n",
    "\n",
    "# 3. BPE (GPT-2)\n",
    "tokens_bpe = None\n",
    "\n",
    "print(f\"Texte : {texte_test}\")\n",
    "print(f\"\\nMots      : {len(tokens_mots) if tokens_mots else '?'} tokens\")\n",
    "print(f\"Caract√®res: {len(tokens_chars) if tokens_chars else '?'} tokens\")\n",
    "print(f\"BPE       : {len(tokens_bpe) if tokens_bpe else '?'} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Construction d'un vocabulaire : Exemple avec le word tokenizer\n",
    "\n",
    "Une fois la strat√©gie choisie, on construit un **vocabulaire** : une table de correspondance token ‚Üî index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construire un vocabulaire simple\n",
    "corpus = [\n",
    "    \"le chat mange\",\n",
    "    \"le chien dort\",\n",
    "    \"la souris court\"\n",
    "]\n",
    "\n",
    "# Collecter tous les tokens uniques\n",
    "all_tokens = set()\n",
    "for phrase in corpus:\n",
    "    all_tokens.update(tokenize_words(phrase))\n",
    "\n",
    "# Cr√©er le vocabulaire avec tokens sp√©ciaux\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # Padding et Unknown\n",
    "for i, token in enumerate(sorted(all_tokens)):\n",
    "    vocab[token] = i + 2\n",
    "\n",
    "# Vocabulaire inverse\n",
    "id_to_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "print(\"Vocabulaire :\")\n",
    "for token, idx in vocab.items():\n",
    "    print(f\"  {idx}: '{token}'\")\n",
    "\n",
    "# Encoder une phrase\n",
    "def tokens_to_ids(text, vocab):\n",
    "    return [vocab.get(t, vocab[\"<UNK>\"]) for t in tokenize_words(text)]\n",
    "\n",
    "def ids_to_token(ids, id_to_token):\n",
    "    return [id_to_token[i] for i in ids]\n",
    "\n",
    "phrase = \"le chat court\"\n",
    "ids = tokens_to_ids(phrase, vocab)\n",
    "tokens = ids_to_token(ids, id_to_token)\n",
    "\n",
    "print(f\"\\nPhrase : '{phrase}'\")\n",
    "print(f\"Conversion des tokens en ids : {ids}\")\n",
    "print(f\"Conversion des ids en token : {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# EXERCICE 1b : Manipuler le vocabulaire\n# ============================================\n\n# Notre vocabulaire actuel :\nprint(\"Vocabulaire actuel :\", list(vocab.keys()))\n\n# Partie A : Ajouter des tokens sp√©ciaux\n# TODO: Ajoutez les tokens \"<START>\" et \"<END>\" au vocabulaire\n# Indice : vocab[\"<START>\"] = len(vocab)\n\n\n# Partie B : Encoder une phrase inconnue\n# TODO: Encodez la phrase \"le hamster mange\" avec tokens_to_ids()\n# Quel token devient <UNK> ? Pourquoi ?\n\n\n# Partie C : √âtendre le vocabulaire\n# TODO: Ajoutez \"hamster\" au vocabulaire, puis r√©-encodez la phrase\n# Comparez les r√©sultats",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. Embeddings : Des indices aux vecteurs\n\n### Le probl√®me des indices\n\nLes indices (0, 1, 2, ...) n'ont pas de **sens s√©mantique**. \n\n- `chat = 3` et `chien = 5` ‚Üí sont-ils proches ? (oui, ce sont des animaux et ils ont beaucoup de choses en commun)\n- `chat = 3` et `voiture = 4` ‚Üí sont-ils proches ? (non, beaucoup moins que chien et chat)\n\nCes indices ne permettent pas de mesurer cette proximit√© !\n\n### Approches classiques pour encoder des phrases : Bag-of-Words et TF-IDF\n\nHistoriquement, on repr√©sentait un texte par un vecteur de la taille du vocabulaire :\n- **Bag-of-Words** : compter les occurrences de chaque mot\n- **TF-IDF** : pond√©rer par la raret√© des mots dans le corpus\n\n```\nExemple simplifi√© (BOW vs TF-IDF) :\n\nCorpus : [\"le chat mange\", \"le chat dort\"]\nVocabulaire : [le, chat, mange, dort]\n\nBOW (comptage brut) :\n  \"le chat mange\" ‚Üí [1, 1, 1, 0]\n  \"le chat dort\"  ‚Üí [1, 1, 0, 1]\n\nTF-IDF (pond√©r√© par raret√©) :\n  \"le chat mange\" ‚Üí [0.3, 0.3, 0.7, 0]    ‚Üê \"mange\" p√®se plus (mot distinctif)\n  \"le chat dort\"  ‚Üí [0.3, 0.3, 0, 0.7]    ‚Üê \"dort\" p√®se plus (mot distinctif)\n                     ‚Üë    ‚Üë\n              mots communs ‚Üí poids r√©duit\n```\n\nEn pratique, le vocabulaire contient 50 000+ mots ‚Üí vecteurs **sparse** (majoritairement des z√©ros) :\n\n```\n\"le chat dort\" ‚Üí [0, 0, ..., 1, ..., 0, 1, ..., 0]  (50 000 dimensions)\n                              ‚Üë         ‚Üë\n                            chat      dort\n```\n\nPour TF-IDF et BOW, on obtient des vecteurs qui permettent de comparer des morceaux de texte entre eux (descriptions de produits sur un site marchand par exemple). Pour autant, ils ne permettent pas de capturer le sens des mots et leurs proximit√©s relatives. Une m√©thode apparue en 2013 (Word2Vec) a permis d'apporter cette compr√©hension plus profonde, de mani√®re automatis√©e et sans supervision."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---\n\n## 4. Word2Vec : Apprendre des embeddings qui ont du sens\n\nComme son nom l'indique, Word2Vec transforme des mots en vecteurs. Mais comment apprend-il des vecteurs o√π \"chat\" et \"chien\" sont vraiment proches ?\n\n---\n\n### L'intuition fondamentale\n\n> **\"Tu connais un mot par les mots qui l'entourent\"** (hypoth√®se distributionnelle)\n\nObservez ces phrases :\n```\n\"Le chat mange sa p√¢t√©e\"\n\"Le chat dort sur le canap√©\"  \n\"Mon chat joue avec une balle\"\n\n\"Le chien mange sa p√¢t√©e\"\n\"Le chien dort sur le canap√©\"\n\"Mon chien joue avec une balle\"\n```\n\n\"Chat\" et \"chien\" apparaissent dans les **m√™mes contextes**. Word2Vec va leur attribuer des vecteurs similaires.\n\n---\n\n### L'architecture de Word2Vec\n\nWord2Vec repose sur une architecture similaire √† un r√©seau de neurones **√©tonnamment simple** : une entr√©e, une couche cach√©e, une sortie. Pas d'activation (simple multiplication matricielle).\n\n#### Le r√©seau\n\n```\n      ENTR√âE                 COUCHE CACH√âE              SORTIE\n     (one-hot)               (embeddings)              (softmax)\n  \n   ‚îå‚îÄ‚îÄ‚îÄ‚îê                                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n   ‚îÇ 0 ‚îÇ  \"le\"                                        ‚îÇ 0.02  ‚îÇ \"le\"\n   ‚îú‚îÄ‚îÄ‚îÄ‚î§                                              ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n   ‚îÇ 0 ‚îÇ  \"chat\"             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ 0.41  ‚îÇ \"chat\"\n   ‚îú‚îÄ‚îÄ‚îÄ‚î§                     ‚îÇ           ‚îÇ            ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n   ‚îÇ 1 ‚îÇ  \"noir\"  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂  ‚îÇ  vecteur  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ 0.05  ‚îÇ \"noir\"\n   ‚îú‚îÄ‚îÄ‚îÄ‚î§            W        ‚îÇ  128 dim  ‚îÇ     W'     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n   ‚îÇ 0 ‚îÇ  \"dort\"             ‚îÇ           ‚îÇ            ‚îÇ 0.38  ‚îÇ \"dort\"\n   ‚îú‚îÄ‚îÄ‚îÄ‚î§                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n   ‚îÇ 0 ‚îÇ  \"sur\"                                       ‚îÇ 0.14  ‚îÇ \"sur\"\n   ‚îî‚îÄ‚îÄ‚îÄ‚îò                                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n  \n  V dimensions              D dimensions              V dimensions\n  (taille vocab)            (ex: 128)                 (probabilit√©s)\n```\n\n| Couche | Dimensions | Ce qu'elle contient |\n|--------|------------|---------------------|\n| Entr√©e | V (ex: 50 000) | Vecteur one-hot du mot |\n| Cach√©e | D (ex: 128) | **Le vecteur qu'on veut r√©cup√©rer** |\n| Sortie | V (ex: 50 000) | Probabilit√© de chaque mot |\n\n---\n\n#### Le but : r√©cup√©rer la couche cach√©e\n\nL'objectif de Word2Vec n'est **pas** de faire des pr√©dictions. C'est de construire de bons vecteurs.\n\nLa **matrice W** (entre l'entr√©e et la couche cach√©e) contient tous les embeddings :\n\n```\nMatrice W (V √ó D)\n                    D dimensions\n              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n   \"le\"     ‚Üí ‚îÇ 0.2  -0.1  0.5 ... ‚îÇ\n   \"chat\"   ‚Üí ‚îÇ 0.3  -0.4  0.7 ... ‚îÇ ‚Üê Ces deux lignes\n   \"chien\"  ‚Üí ‚îÇ 0.3  -0.3  0.6 ... ‚îÇ ‚Üê sont proches !\n   \"noir\"   ‚Üí ‚îÇ 0.1   0.5 -0.2 ... ‚îÇ\n   ...        ‚îÇ        ...         ‚îÇ\n              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Apr√®s l'entra√Ænement** :\n- On **garde** la matrice W ‚Üí c'est notre table d'embeddings\n- On **jette** tout le reste (matrice W', couche de sortie)\n\nLa sortie n'√©tait qu'un **pr√©texte** pour entra√Æner le r√©seau !\n\n---\n\n#### L'entra√Ænement : deux techniques\n\nPour que les vecteurs capturent le sens des mots, on entra√Æne le r√©seau √† pr√©dire les relations entre mots voisins dans un corpus.\n\n**Deux approches sym√©triques existent :**\n\n##### Skip-gram : mot central ‚Üí mots voisins\n\nOn donne un mot, le r√©seau pr√©dit les mots qui l'entourent.\n\n```\nPhrase : \"Le chat noir dort sur\"\n                  ‚Üë\n             mot central\n\nFen√™tre de contexte (¬±1 mot) :\n    Entr√©e  : \"noir\"\n    Cibles  : \"chat\", \"dort\" (trait√©s un par un)\n```\n\n```\n    \"noir\"                     \"chat\" ?\n       ‚îÇ                          ‚Üë\n       ‚ñº                          ‚îÇ\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    vecteur     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n   ‚îÇ   W   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ   W'    ‚îÇ\n   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   (128 dim)    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n##### CBOW : mots voisins ‚Üí mot central\n\nOn donne les mots du contexte, le r√©seau pr√©dit le mot du milieu.\n\n```\nPhrase : \"Le chat noir dort sur\"\n              ‚Üë         ‚Üë\n            contexte (¬±1)\n\nEntr√©e  : \"chat\" + \"dort\" (moyenn√©s)\nCible   : \"noir\"\n```\n\n```\n\"chat\" + \"dort\"                \"noir\" ?\n       ‚îÇ                          ‚Üë\n       ‚ñº                          ‚îÇ\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    vecteur     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n   ‚îÇ   W   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ   W'    ‚îÇ\n   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   (128 dim)    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n##### Comparaison\n\n| | Skip-gram | CBOW |\n|--|-----------|------|\n| Entr√©e | 1 mot | plusieurs mots |\n| Sortie | pr√©dire les voisins | pr√©dire le mot central |\n| Mots rares | ‚úÖ meilleur | moins bon |\n| Vitesse | plus lent | ‚úÖ plus rapide |\n\nEn pratique, **Skip-gram** est plus utilis√© car il donne de meilleurs r√©sultats sur les mots peu fr√©quents.\n\n---\n\n#### Cons√©quence : mots fr√©quents vs mots rares\n\nPlus un mot est **fr√©quent** dans le corpus, plus son embedding est ajust√© souvent ‚Üí meilleure qualit√©.\n\nUn mot vu 100 000 fois aura un excellent embedding. Un mot vu 3 fois restera proche de son initialisation al√©atoire.\n\n> C'est pour √ßa que BPE aide : un mot rare comme \"smartcar\" est d√©coup√© en \"smart\" + \"car\", deux sous-mots tr√®s fr√©quents avec d'excellents embeddings !\n\n---\n\n#### Limitation : l'ordre des mots est ignor√©\n\nWord2Vec traite chaque paire (mot, voisin) **ind√©pendamment**. Il ne sait pas quel mot vient avant ou apr√®s.\n\n```\n\"Le chat mange la souris\"\n\"La souris mange le chat\"\n\n‚Üí M√™mes paires d'entra√Ænement !\n‚Üí Word2Vec ne voit pas la diff√©rence\n```\n\nC'est le **m√©canisme d'attention combin√© au positional encoding** (section 5) qui permettra de capturer l'ordre et les relations entre positions.\n\n---\n\n> üí° **En pratique** : Les impl√©mentations r√©elles (gensim, FastText...) ajoutent des optimisations pour acc√©l√©rer l'entra√Ænement sur de gros vocabulaires. L'architecture de base reste la m√™me.\n\n> üìö **Pour aller plus loin** : [The Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/) ‚Äî Visualisations d√©taill√©es de l'architecture et de l'entra√Ænement.\n\n---\n\n### Pourquoi les analogies marchent ?\n\nApr√®s entra√Ænement, les vecteurs encodent des **relations** :\n\n```\nvecteur(\"roi\") - vecteur(\"homme\") ‚âà vecteur(\"reine\") - vecteur(\"femme\")\n```\n\nAutrement dit, la \"direction\" homme‚Üífemme dans l'espace vectoriel est la m√™me que roi‚Üíreine :\n\n```\n        homme ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí femme\n          ‚Üë    (m√™me         ‚Üë\n          ‚îÇ   direction)     ‚îÇ\n         roi ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí reine\n```\n\nC'est pour √ßa que `roi - homme + femme ‚âà reine` fonctionne !\n\n---\n\n### Mesurer la similarit√© : Distance cosinus\n\nAvec des vecteurs, on peut mesurer la **similarit√©** entre mots :\n\n$$\\text{similarit√©}(A, B) = \\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n\n- R√©sultat entre -1 (oppos√©s) et 1 (identiques)\n- 0 = orthogonaux (pas de relation)\n\n---\n\n### Chargeons un mod√®le pr√©-entra√Æn√©\n\nNous allons utiliser **GloVe** (similaire √† Word2Vec), entra√Æn√© sur Wikipedia."
  },
  {
   "cell_type": "code",
   "source": "# Charger un mod√®le pr√©-entra√Æn√© (GloVe, similaire √† Word2Vec)\nimport gensim.downloader as api\n\nprint(\"Chargement du mod√®le GloVe (peut prendre 1-2 min)...\")\nmodel = api.load(\"glove-wiki-gigaword-100\")  # 100 dimensions, entra√Æn√© sur Wikipedia\nprint(f\"Mod√®le charg√© ! Vocabulaire : {len(model)} mots\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Explorer les similarit√©s\nprint(\"=== Mots similaires √† 'king' ===\")\nfor word, score in model.most_similar(\"king\", topn=5):\n    print(f\"  {word}: {score:.4f}\")\n\nprint(\"\\n=== Mots similaires √† 'computer' ===\")\nfor word, score in model.most_similar(\"computer\", topn=5):\n    print(f\"  {word}: {score:.4f}\")"
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# EXERCICE 2 : Explorer les similarit√©s\n# ============================================\n\n# Partie A : Trouver les mots similaires √† \"france\", \"cat\", \"happy\"\n# TODO: Utilisez model.most_similar(mot, topn=5)\n\n\n# Partie B : Exploration libre\n# TODO: Trouvez une paire de mots avec similarit√© > 0.7\n# Indice : model.similarity(\"mot1\", \"mot2\") retourne un score entre -1 et 1\n\n\n# Partie C : Trouver l'intrus\n# TODO: Utilisez model.doesnt_match([\"mot1\", \"mot2\", \"mot3\", \"mot4\"])\n# Exemple : model.doesnt_match([\"breakfast\", \"lunch\", \"dinner\", \"car\"])\n# Testez avec vos propres listes !",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# La magie des analogies : king - man + woman = ?\nprint(\"=== Analogie : king - man + woman = ? ===\")\nresult = model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=3)\nfor word, score in result:\n    print(f\"  {word}: {score:.4f}\")\n\nprint(\"\\n=== Analogie : paris - france + italy = ? ===\")\nresult = model.most_similar(positive=[\"paris\", \"italy\"], negative=[\"france\"], topn=3)\nfor word, score in result:\n    print(f\"  {word}: {score:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# EXERCICE 3 : Ma√Ætriser les analogies\n# ============================================\n\n# Partie A : Tester ces analogies classiques\n# - \"berlin\" - \"germany\" + \"france\" = ?\n# - \"good\" - \"better\" + \"bad\" = ?\n# - \"walked\" - \"walk\" + \"swim\" = ?\n\n# TODO: Syntaxe : model.most_similar(positive=[\"A\", \"C\"], negative=[\"B\"], topn=3)\n\n\n# Partie B : Inventer une analogie qui fonctionne\n# TODO: Trouvez une analogie originale qui donne le r√©sultat attendu\n# Exemples de domaines : m√©tiers, pays/capitales, animaux, verbes...\n\n\n# Partie C : Trouver une analogie qui √©choue\n# TODO: Trouvez une analogie qui devrait marcher logiquement mais √©choue\n# Expliquez pourquoi dans un commentaire (indice : fr√©quence des mots, biais du corpus...)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualisation des embeddings en 2D\n!pip install scikit-learn -q\nfrom sklearn.decomposition import PCA\n\n# S√©lectionner quelques mots\nwords = [\"king\", \"queen\", \"man\", \"woman\", \"prince\", \"princess\",\n         \"cat\", \"dog\", \"lion\", \"tiger\",\n         \"car\", \"bus\", \"train\", \"plane\"]\n\n# R√©cup√©rer leurs vecteurs\nvectors = np.array([model[w] for w in words])\n\n# R√©duire √† 2D avec PCA\npca = PCA(n_components=2)\nvectors_2d = pca.fit_transform(vectors)\n\n# Visualiser\nplt.figure(figsize=(12, 8))\nplt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], c='blue', s=100)\n\nfor i, word in enumerate(words):\n    plt.annotate(word, (vectors_2d[i, 0] + 0.1, vectors_2d[i, 1] + 0.1), fontsize=12)\n\nplt.title(\"Embeddings GloVe projet√©s en 2D\")\nplt.xlabel(\"Composante 1\")\nplt.ylabel(\"Composante 2\")\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"Observation : Les mots de m√™me cat√©gorie sont regroup√©s !\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# EXERCICE 4 : Visualisation personnalis√©e\n# ============================================\n\n# TODO: Cr√©ez votre propre visualisation avec 15-20 mots de votre choix\n# Choisissez des mots de 3-4 cat√©gories diff√©rentes (ex: sports, √©motions, pays, m√©tiers)\n\n# Vos mots (modifiez cette liste) :\nmy_words = [\n    # Cat√©gorie 1 (ex: sports) : \n    \n    # Cat√©gorie 2 (ex: √©motions) : \n    \n    # Cat√©gorie 3 (ex: pays) : \n    \n    # Cat√©gorie 4 (ex: m√©tiers) : \n]\n\n# TODO: V√©rifiez que tous vos mots sont dans le vocabulaire\n# for word in my_words:\n#     if word not in model:\n#         print(f\"'{word}' n'est pas dans le vocabulaire !\")\n\n# TODO: Copiez et adaptez le code de visualisation de la cellule pr√©c√©dente\n# Remplacez 'words' par 'my_words'\n\n\n# QUESTION : Les mots de m√™me cat√©gorie sont-ils regroup√©s ? \n# Y a-t-il des surprises ?",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 5. Teaser : Le m√©canisme d'attention\n\nMaintenant que nous savons repr√©senter du texte (tokenization + embeddings), la prochaine √©tape est de permettre aux mots de **communiquer entre eux**.\n\nC'est le r√¥le du **m√©canisme d'attention**, que nous verrons en d√©tail au prochain TP.\n\n### L'id√©e\n\n> Pour comprendre un mot, il faut regarder les autres mots de la phrase.\n\nExemple : *\"Le chat qui dormait sur le canap√© a saut√©\"*\n- Pour comprendre **\"a saut√©\"** ‚Üí regarder **\"chat\"** (le sujet)\n- Pour comprendre **\"dormait\"** ‚Üí regarder **\"chat\"** et **\"canap√©\"**\n\n### Visualisation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Matrice d'attention simul√©e\nphrase = [\"Le\", \"chat\", \"mange\", \"la\", \"souris\"]\n\n# Chaque ligne = un mot qui \"regarde\" les autres\n# Valeurs = poids d'attention (somme = 1 par ligne)\nattention = torch.tensor([\n    [0.8, 0.1, 0.05, 0.03, 0.02],  # \"Le\" regarde surtout lui-m√™me\n    [0.1, 0.7, 0.1, 0.05, 0.05],   # \"chat\" regarde surtout lui-m√™me\n    [0.05, 0.4, 0.4, 0.05, 0.1],   # \"mange\" regarde \"chat\" et lui-m√™me\n    [0.02, 0.03, 0.05, 0.8, 0.1],  # \"la\" regarde surtout lui-m√™me\n    [0.02, 0.1, 0.2, 0.08, 0.6],   # \"souris\" regarde \"mange\" et elle-m√™me\n])\n\n# Visualisation\nplt.figure(figsize=(8, 6))\nplt.imshow(attention, cmap='Blues')\nplt.xticks(range(5), phrase)\nplt.yticks(range(5), phrase)\nplt.xlabel(\"Mots regard√©s\")\nplt.ylabel(\"Mots qui regardent\")\nplt.title(\"Qui regarde qui ? (Matrice d'attention)\")\nplt.colorbar(label=\"Poids d'attention\")\n\nfor i in range(5):\n    for j in range(5):\n        plt.text(j, i, f'{attention[i,j]:.2f}', \n                ha='center', va='center',\n                color='white' if attention[i,j] > 0.5 else 'black')\nplt.show()\n\nprint(\"Le verbe 'mange' regarde fortement 'chat' (son sujet) !\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Ce qu'on voit** :\n- Chaque mot peut \"regarder\" tous les autres mots\n- Les poids indiquent l'importance de chaque relation\n- Le mod√®le **apprend** ces poids pendant l'entra√Ænement\n\n**Au prochain TP**, nous verrons :\n- Comment calculer cette matrice d'attention\n- Les concepts Query, Key, Value\n- L'architecture compl√®te du Transformer",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. R√©capitulatif\n",
    "\n",
    "### Le pipeline NLP\n",
    "\n",
    "| √âtape | Entr√©e | Sortie | R√¥le |\n",
    "|-------|--------|--------|------|\n",
    "| **Tokenization** | Texte brut | Liste de tokens | D√©couper le texte |\n",
    "| **Vocabulaire** | Tokens | Indices | Table token ‚Üî ID |\n",
    "| **Embedding** | Indices | Vecteurs denses | Sens s√©mantique |\n",
    "\n",
    "### Points cl√©s\n",
    "\n",
    "1. **Tokenization BPE** : meilleur compromis entre mots et caract√®res\n",
    "2. **Embeddings** : transforment les mots en vecteurs comparables\n",
    "3. **Word2Vec** : montre que les embeddings capturent le sens (analogies !)\n",
    "4. **Similarit√© cosinus** : mesure la proximit√© entre vecteurs\n",
    "\n",
    "### Prochaine session\n",
    "\n",
    "Nous verrons le **m√©canisme d'attention** : comment les mots \"communiquent\" entre eux pour se comprendre mutuellement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 7. Pour aller plus loin (optionnel)\n\n### Ressources\n\n- [Les tokenizers en NLP (FR)](https://lbourdois.github.io/blog/nlp/Les-tokenizers/) - Excellent article en fran√ßais\n- [The Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/) - Visualisations tr√®s claires\n- [Bolmo: Byte-level Language Models (Allen AI, 2025)](https://allenai.org/blog/bolmo) - Une alternative √† BPE ?\n\n### Exp√©rimentations sugg√©r√©es\n\n1. Tester d'autres analogies Word2Vec\n2. Visualiser les embeddings de votre choix\n3. Comparer diff√©rents tokenizers (BERT, GPT-2, etc.)"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 8. Mini-projet de synth√®se (optionnel)\n\nMettez en pratique tout ce que vous avez appris en analysant vos propres phrases. Ce projet combine tokenization, vocabulaire et embeddings.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# MINI-PROJET : Pipeline NLP complet\n# ============================================\n\n# Choisissez 3 phrases en anglais sur un th√®me commun\nmes_phrases = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"A fast red cat leaps across the sleepy mouse\", \n    \"The slow black bear walks around the tired wolf\"\n]\n\n# TODO √âtape 1 : Tokenisez chaque phrase avec les 3 m√©thodes\n# Comparez le nombre de tokens\n# for phrase in mes_phrases:\n#     print(f\"Phrase: {phrase}\")\n#     print(f\"  Mots: {len(tokenize_words(phrase))}\")\n#     print(f\"  Chars: {len(tokenize_chars(phrase))}\")\n#     print(f\"  BPE: {len(gpt2_tokenizer.tokenize(phrase))}\")\n\n\n# TODO √âtape 2 : Pour la tokenization par mots, construisez un vocabulaire unifi√©\n# Indice : collectez les tokens de toutes les phrases dans un set\n\n\n# TODO √âtape 3 : R√©cup√©rez les embeddings GloVe des mots communs aux 3 phrases\n# Indice : utilisez des sets et l'intersection (&)\n\n\n# TODO √âtape 4 : Calculez la similarit√© moyenne entre les phrases\n# Indice : moyennez les embeddings de chaque phrase, puis calculez la similarit√© cosinus\n# from numpy.linalg import norm\n# def cosine_sim(v1, v2): return np.dot(v1, v2) / (norm(v1) * norm(v2))\n\n\n# TODO √âtape 5 : Visualisez les mots des 3 phrases sur un m√™me graphique\n# Utilisez des couleurs diff√©rentes pour chaque phrase\n\n\n# QUESTIONS DE R√âFLEXION :\n# 1. Quelle m√©thode de tokenization donne le vocabulaire le plus compact ?\n# 2. Les phrases similaires ont-elles des embeddings moyens proches ?\n# 3. Quelles limites de Word2Vec observez-vous sur vos phrases ?",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}