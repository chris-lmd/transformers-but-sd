{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "> **‚ö†Ô∏è EN COURS DE CONSTRUCTION**\n>\n> Ce notebook est en cours de finalisation. Merci de ne pas le consulter pour l'instant.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 03 - Multi-Head Attention\n",
    "\n",
    "**Module** : R√©seaux de Neurones Approfondissement  \n",
    "**Dur√©e** : 2h  \n",
    "**Objectif** : Comprendre et impl√©menter le Multi-Head Attention\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs p√©dagogiques\n",
    "\n",
    "√Ä la fin de ce TP, vous serez capable de :\n",
    "1. Expliquer pourquoi plusieurs t√™tes d'attention sont utiles\n",
    "2. Impl√©menter le Multi-Head Attention from scratch\n",
    "3. Visualiser ce que chaque t√™te apprend\n",
    "4. Comprendre le lien avec les Transformers\n",
    "\n",
    "---\n",
    "\n",
    "## Pr√©requis\n",
    "\n",
    "Ce TP suppose que vous avez compl√©t√© le **TP 02 - M√©canisme d'Attention** o√π vous avez :\n",
    "- Impl√©ment√© le Scaled Dot-Product Attention\n",
    "- Compris les concepts de Query, Key, Value\n",
    "- Visualis√© l'attention sur un vrai mod√®le\n",
    "\n",
    "Ici, nous allons voir comment **combiner plusieurs t√™tes d'attention** pour capturer diff√©rents types de relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab)\n",
    "!pip install torch matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rappel : Single-Head Attention\n",
    "\n",
    "Reprenons notre fonction d'attention du TP pr√©c√©dent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries, shape (..., seq_len, d_k)\n",
    "        K: Keys, shape (..., seq_len, d_k)\n",
    "        V: Values, shape (..., seq_len, d_v)\n",
    "        mask: Masque optionnel\n",
    "    \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Pourquoi Multi-Head ?\n",
    "\n",
    "### Le probl√®me avec une seule t√™te\n",
    "\n",
    "Une seule t√™te d'attention calcule **une** repr√©sentation des relations entre mots.\n",
    "\n",
    "Mais dans une phrase, il y a **plusieurs types de relations** :\n",
    "- Relations syntaxiques (sujet-verbe)\n",
    "- Relations s√©mantiques (sens)\n",
    "- Relations de proximit√©\n",
    "- etc.\n",
    "\n",
    "### La solution : plusieurs t√™tes\n",
    "\n",
    "Chaque t√™te peut apprendre √† d√©tecter un type de relation diff√©rent !\n",
    "\n",
    "**Analogie** : C'est comme avoir plusieurs experts qui analysent une phrase sous diff√©rents angles, puis combinent leurs analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration : diff√©rentes t√™tes peuvent capturer diff√©rentes relations\n",
    "phrase = [\"Le\", \"chat\", \"noir\", \"mange\", \"la\", \"souris\"]\n",
    "\n",
    "# T√™te 1 : Relations syntaxiques (sujet-verbe)\n",
    "attention_syntaxe = torch.tensor([\n",
    "    [0.3, 0.5, 0.1, 0.05, 0.03, 0.02],  # \"Le\" ‚Üí \"chat\"\n",
    "    [0.1, 0.3, 0.1, 0.4, 0.05, 0.05],   # \"chat\" ‚Üí \"mange\"\n",
    "    [0.1, 0.6, 0.2, 0.05, 0.03, 0.02],  # \"noir\" ‚Üí \"chat\"\n",
    "    [0.05, 0.5, 0.05, 0.2, 0.1, 0.1],   # \"mange\" ‚Üí \"chat\"\n",
    "    [0.02, 0.03, 0.02, 0.03, 0.3, 0.6], # \"la\" ‚Üí \"souris\"\n",
    "    [0.02, 0.1, 0.02, 0.4, 0.06, 0.4],  # \"souris\" ‚Üí \"mange\"\n",
    "])\n",
    "\n",
    "# T√™te 2 : Relations de proximit√©\n",
    "attention_proximite = torch.tensor([\n",
    "    [0.5, 0.4, 0.08, 0.01, 0.005, 0.005],\n",
    "    [0.3, 0.4, 0.25, 0.04, 0.005, 0.005],\n",
    "    [0.1, 0.35, 0.35, 0.15, 0.03, 0.02],\n",
    "    [0.02, 0.1, 0.3, 0.35, 0.18, 0.05],\n",
    "    [0.01, 0.02, 0.05, 0.25, 0.4, 0.27],\n",
    "    [0.005, 0.01, 0.02, 0.1, 0.35, 0.515],\n",
    "])\n",
    "\n",
    "# Visualisation c√¥te √† c√¥te\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (attn, title) in enumerate([\n",
    "    (attention_syntaxe, \"T√™te 1 : Relations syntaxiques\"),\n",
    "    (attention_proximite, \"T√™te 2 : Proximit√©\")\n",
    "]):\n",
    "    ax = axes[idx]\n",
    "    im = ax.imshow(attn, cmap='Blues')\n",
    "    ax.set_xticks(range(6))\n",
    "    ax.set_xticklabels(phrase, rotation=45)\n",
    "    ax.set_yticks(range(6))\n",
    "    ax.set_yticklabels(phrase)\n",
    "    ax.set_title(title)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Architecture Multi-Head Attention\n",
    "\n",
    "### Sch√©ma\n",
    "\n",
    "```\n",
    "Input (seq_len, embed_dim)\n",
    "        ‚Üì\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚Üì         ‚Üì        ‚Üì        ‚Üì\n",
    " Head 1   Head 2   Head 3   Head 4   (chaque t√™te a sa propre projection Q, K, V)\n",
    "   ‚Üì         ‚Üì        ‚Üì        ‚Üì\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚Üì\n",
    "    Concat\n",
    "        ‚Üì\n",
    "   Linear (projection de sortie)\n",
    "        ‚Üì\n",
    "Output (seq_len, embed_dim)\n",
    "```\n",
    "\n",
    "### Dimensions\n",
    "\n",
    "- **embed_dim** : Dimension des embeddings (ex: 512)\n",
    "- **num_heads** : Nombre de t√™tes (ex: 8)\n",
    "- **d_k = embed_dim / num_heads** : Dimension par t√™te (ex: 64)\n",
    "\n",
    "Chaque t√™te travaille avec une dimension r√©duite, puis on concat√®ne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Impl√©mentation √©tape par √©tape\n",
    "\n",
    "### √âtape 1 : Projections Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "embed_dim = 32\n",
    "num_heads = 4\n",
    "d_k = embed_dim // num_heads  # 32 / 4 = 8\n",
    "\n",
    "print(f\"embed_dim: {embed_dim}\")\n",
    "print(f\"num_heads: {num_heads}\")\n",
    "print(f\"d_k (dim par t√™te): {d_k}\")\n",
    "\n",
    "# Input\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "print(f\"\\nInput shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er les projections\n",
    "# On projette vers embed_dim (pas num_heads * d_k, c'est la m√™me chose)\n",
    "W_q = nn.Linear(embed_dim, embed_dim)\n",
    "W_k = nn.Linear(embed_dim, embed_dim)\n",
    "W_v = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "# Projeter\n",
    "Q = W_q(x)  # (batch, seq_len, embed_dim)\n",
    "K = W_k(x)\n",
    "V = W_v(x)\n",
    "\n",
    "print(f\"Q shape apr√®s projection: {Q.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 2 : Reshape pour s√©parer les t√™tes\n",
    "\n",
    "On doit transformer `(batch, seq_len, embed_dim)` en `(batch, num_heads, seq_len, d_k)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# EXERCICE 1 : Reshape pour multi-head\n# ============================================\n\n# Transformer (batch, seq_len, embed_dim) en (batch, num_heads, seq_len, d_k)\n# Indice : utilisez view() et transpose()\n\ndef split_heads(x, num_heads):\n    \"\"\"\n    Reshape (batch, seq_len, embed_dim) -> (batch, num_heads, seq_len, d_k)\n    \"\"\"\n    batch_size, seq_len, embed_dim = x.shape\n    d_k = embed_dim // num_heads\n    \n    # TODO: Impl√©menter le reshape en 2 √©tapes\n    # - S√©parer embed_dim en (num_heads, d_k)\n    # - R√©organiser pour avoir num_heads en position 1\n    \n    x = None  # √Ä compl√©ter\n    \n    return x\n\n# Test (d√©commentez apr√®s avoir compl√©t√© l'exercice)\n# Q_heads = split_heads(Q, num_heads)\n# print(f\"Q shape apr√®s split: {Q_heads.shape}\")  # Attendu: (2, 4, 6, 8)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 3 : Attention par t√™te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 4 : Concat√©ner les t√™tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# EXERCICE 2 : Concat des t√™tes\n# ============================================\n\ndef concat_heads(x):\n    \"\"\"\n    Reshape (batch, num_heads, seq_len, d_k) -> (batch, seq_len, embed_dim)\n    C'est l'inverse de split_heads\n    \"\"\"\n    batch_size, num_heads, seq_len, d_k = x.shape\n    embed_dim = num_heads * d_k\n    \n    # TODO: Impl√©menter le reshape inverse\n    # Indice : inverse des op√©rations de split_heads\n    # N'oubliez pas .contiguous() si n√©cessaire\n    \n    x = None  # √Ä compl√©ter\n    \n    return x\n\n# Test (d√©commentez apr√®s avoir compl√©t√© les exercices 1 et 2)\n# Q_heads = split_heads(Q, num_heads)\n# K_heads = split_heads(K, num_heads)\n# V_heads = split_heads(V, num_heads)\n# attn_output, attn_weights = scaled_dot_product_attention(Q_heads, K_heads, V_heads)\n# concat_output = concat_heads(attn_output)\n# print(f\"Output apr√®s concat: {concat_output.shape}\")  # Attendu: (2, 6, 32)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 5 : Projection de sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection finale\n",
    "W_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "final_output = W_o(concat_output)\n",
    "print(f\"Final output shape: {final_output.shape}\")  # (2, 6, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention.\n",
    "    \n",
    "    Args:\n",
    "        embed_dim: Dimension des embeddings\n",
    "        num_heads: Nombre de t√™tes d'attention\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim doit √™tre divisible par num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads\n",
    "        \n",
    "        # TODO: Cr√©er les 4 projections lin√©aires\n",
    "        self.W_q = None  # nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = None\n",
    "        self.W_v = None\n",
    "        self.W_o = None\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"(batch, seq_len, embed_dim) -> (batch, num_heads, seq_len, d_k)\"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        # TODO: Impl√©menter\n",
    "        return None\n",
    "    \n",
    "    def concat_heads(self, x):\n",
    "        \"\"\"(batch, num_heads, seq_len, d_k) -> (batch, seq_len, embed_dim)\"\"\"\n",
    "        batch_size, _, seq_len, _ = x.shape\n",
    "        # TODO: Impl√©menter\n",
    "        return None\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input, shape (batch, seq_len, embed_dim)\n",
    "            mask: Masque optionnel\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, embed_dim)\n",
    "            attention_weights: (batch, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # TODO: Impl√©menter le forward\n",
    "        # 1. Projeter x en Q, K, V\n",
    "        Q = None\n",
    "        K = None\n",
    "        V = None\n",
    "        \n",
    "        # 2. Split en t√™tes\n",
    "        Q = None  # self.split_heads(Q)\n",
    "        K = None\n",
    "        V = None\n",
    "        \n",
    "        # 3. Attention\n",
    "        attn_output, attn_weights = None, None  # scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 4. Concat\n",
    "        concat_output = None  # self.concat_heads(attn_output)\n",
    "        \n",
    "        # 5. Projection de sortie\n",
    "        output = None  # self.W_o(concat_output)\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre impl√©mentation\n",
    "mha = MultiHeadAttention(embed_dim=32, num_heads=4)\n",
    "\n",
    "x = torch.randn(2, 6, 32)  # batch=2, seq=6, embed=32\n",
    "output, weights = mha(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")  # Attendu: (2, 6, 32)\n",
    "print(f\"Weights shape: {weights.shape}\")  # Attendu: (2, 4, 6, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Visualisation des t√™tes sur un vrai mod√®le\n",
    "\n",
    "Voyons ce que les diff√©rentes t√™tes capturent sur un mod√®le **r√©ellement entra√Æn√©**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger DistilBERT (comme au TP 02)\n",
    "!pip install transformers -q\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "model.eval()\n",
    "\n",
    "# Phrase de test\n",
    "phrase = \"The cat sat on the mat because it was tired\"\n",
    "inputs = tokenizer(phrase, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extraire les attentions de la couche 5 (12 t√™tes)\n",
    "layer = 4  # Couche 5 (index 0-5)\n",
    "attention = outputs.attentions[layer][0]  # (num_heads, seq_len, seq_len)\n",
    "\n",
    "print(f\"Phrase: {phrase}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Shape attention couche {layer+1}: {attention.shape}\")  # (12, 11, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser 4 t√™tes diff√©rentes de la m√™me couche\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# S√©lectionner 4 t√™tes int√©ressantes\n",
    "heads_to_show = [1, 2, 5, 10]  # Diff√©rentes t√™tes\n",
    "\n",
    "for idx, head in enumerate(heads_to_show):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    w = attention[head].numpy()\n",
    "    \n",
    "    im = ax.imshow(w, cmap='Blues')\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_yticklabels(tokens, fontsize=9)\n",
    "    ax.set_title(f\"T√™te {head + 1}\", fontsize=12)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle(f\"Diff√©rentes t√™tes de la couche {layer+1} - Chaque t√™te capture des relations diff√©rentes\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyse sp√©cifique : que regarde \"it\" selon diff√©rentes t√™tes ?\n",
    "it_index = tokens.index(\"it\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Que regarde le pronom 'it' selon chaque t√™te ?\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for head in heads_to_show:\n",
    "    print(f\"\\n--- T√™te {head + 1} ---\")\n",
    "    weights = attention[head, it_index].numpy()\n",
    "    top_indices = weights.argsort()[-3:][::-1]  # Top 3\n",
    "    for i in top_indices:\n",
    "        bar = \"‚ñà\" * int(weights[i] * 20)\n",
    "        print(f\"  {tokens[i]:10} {weights[i]:.2f} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Chaque t√™te a appris √† capturer des **relations diff√©rentes** :\n",
    "- Certaines t√™tes se concentrent sur la **cor√©f√©rence** (\"it\" ‚Üí \"cat\")\n",
    "- D'autres sur la **proximit√©** (mots voisins)\n",
    "- D'autres sur la **syntaxe** (sujet-verbe)\n",
    "- Certaines regardent le token **[CLS]** (repr√©sentation globale)\n",
    "\n",
    "C'est exactement ce qu'on voulait ! Multi-head = **plusieurs experts** qui analysent la phrase sous diff√©rents angles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Comparaison avec PyTorch nn.MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch fournit une impl√©mentation optimis√©e\n",
    "mha_pytorch = nn.MultiheadAttention(embed_dim=32, num_heads=4, batch_first=True)\n",
    "\n",
    "x = torch.randn(2, 6, 32)\n",
    "\n",
    "# Pour nn.MultiheadAttention, on passe query, key, value s√©par√©ment\n",
    "# En self-attention, query = key = value = x\n",
    "output_pytorch, weights_pytorch = mha_pytorch(x, x, x)\n",
    "\n",
    "print(f\"Output shape (PyTorch): {output_pytorch.shape}\")\n",
    "print(f\"Weights shape (PyTorch): {weights_pytorch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Exercice de synth√®se : Nombre de param√®tres\n",
    "\n",
    "Calculons le nombre de param√®tres dans notre MultiHeadAttention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 4 : Calcul des param√®tres\n",
    "# ============================================\n",
    "\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "# Combien de param√®tres dans :\n",
    "# - W_q : Linear(embed_dim, embed_dim) = embed_dim * embed_dim + embed_dim (weights + bias)\n",
    "# - W_k : idem\n",
    "# - W_v : idem\n",
    "# - W_o : idem\n",
    "\n",
    "params_per_linear = None  # TODO: Calculer\n",
    "total_params = None  # TODO: 4 * params_per_linear\n",
    "\n",
    "print(f\"Param√®tres par couche lin√©aire: {params_per_linear:,}\")\n",
    "print(f\"Total param√®tres MHA: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification\n",
    "mha_test = MultiHeadAttention(embed_dim=512, num_heads=8)\n",
    "real_params = sum(p.numel() for p in mha_test.parameters())\n",
    "print(f\"V√©rification PyTorch: {real_params:,} param√®tres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ============================================\n# EXERCICE 4 : Calcul des param√®tres\n# ============================================\n\nembed_dim = 512\nnum_heads = 8\n\n# Combien de param√®tres dans notre MultiHeadAttention ?\n# Rappel : nn.Linear(in, out) a (in * out + out) param√®tres (weights + bias)\n#\n# Indice : Comptez les param√®tres de chaque couche lin√©aire (W_q, W_k, W_v, W_o)\n\nparams_per_linear = None  # TODO: Calculer pour une couche Linear(embed_dim, embed_dim)\ntotal_params = None  # TODO: Calculer le total\n\nprint(f\"Param√®tres par couche lin√©aire: {params_per_linear:,}\")\nprint(f\"Total param√®tres MHA: {total_params:,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Pour aller plus loin : Cross-Attention\n",
    "\n",
    "> **Note** : Le cross-attention sera abord√© en d√©tail dans les **projets** (Mini-GPT, RAG).\n",
    "\n",
    "**Self-attention** (ce qu'on a fait) : Q, K, V viennent de la **m√™me** source.\n",
    "\n",
    "**Cross-attention** : Q vient d'une source, K et V d'une **autre** source.\n",
    "- Utilis√© en **traduction** : le d√©codeur (fran√ßais) \"interroge\" l'encodeur (anglais)\n",
    "- Utilis√© dans les **mod√®les g√©n√©ratifs** avec contexte externe (RAG)\n",
    "\n",
    "```\n",
    "Self-attention:     x ‚îÄ‚îÄ‚ñ∫ Q, K, V     (m√™me source)\n",
    "Cross-attention:    x_dec ‚îÄ‚îÄ‚ñ∫ Q       (une source)\n",
    "                    x_enc ‚îÄ‚îÄ‚ñ∫ K, V    (autre source)\n",
    "```\n",
    "\n",
    "Dans les TP 2-4, nous utilisons uniquement la **self-attention** (encodeur). Le cross-attention sera explor√© dans les projets avec les architectures g√©n√©ratives."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 11. Mini-projet : Classifier les Pok√©mon par type\n\n> **üè† BONUS / DEVOIR MAISON**\n> \n> Cette section est **optionnelle** et ne fait pas partie du TP en session.\n> Elle est propos√©e pour les √©tudiants qui souhaitent approfondir √† la maison.\n\nDans ce projet, vous allez **fine-tuner CamemBERT** (un mod√®le BERT fran√ßais) pour classifier des articles Pok√©mon par type (Feu, Eau, Plante...), puis **analyser les t√™tes d'attention** pour comprendre ce que le mod√®le a appris.\n\n### Objectifs p√©dagogiques\n\n1. Comprendre le **fine-tuning** d'un mod√®le pr√©-entra√Æn√©\n2. Observer comment les **t√™tes d'attention changent** apr√®s entra√Ænement\n3. Analyser si certaines t√™tes se **sp√©cialisent** sur des patterns sp√©cifiques\n\n### Structure du projet\n\n| Partie | Contenu | Difficult√© |\n|--------|---------|------------|\n| **Partie 1** | Fine-tuning CamemBERT pour classification | ‚≠ê‚≠ê |\n| **Partie 2** | Visualisation de l'attention avant/apr√®s | ‚≠ê‚≠ê |\n| **Partie 3** | Analyse des t√™tes sp√©cialis√©es | ‚≠ê‚≠ê‚≠ê |\n\n---\n\n### Partie 1 : Fine-tuning CamemBERT\n\n#### 1.1 Chargement du dataset Pok√©mon",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# PARTIE 1.1 : Chargement du dataset Pok√©mon\n# ============================================\n\n!pip install datasets transformers -q\n\nfrom datasets import load_dataset\nimport pandas as pd\n\n# Charger le dataset Pok√©mon\nprint(\"Chargement du dataset Pok√©mon...\")\ndataset = load_dataset(\"chris-lmd/pokepedia-fr\", \"articles\")\n\nprint(f\"Nombre d'articles : {len(dataset['train'])}\")\n\n# Aper√ßu des donn√©es\nprint(\"\\n=== Exemple d'article ===\")\nexample = dataset['train'][0]\nprint(f\"Titre : {example['title']}\")\nprint(f\"Types : {example['types']}\")\nprint(f\"Contenu (extrait) : {example['content'][:300]}...\")\n\n# Statistiques sur les types\nall_types = []\nfor article in dataset['train']:\n    all_types.extend(article['types'])\n\nfrom collections import Counter\ntype_counts = Counter(all_types)\nprint(f\"\\n=== Distribution des types (top 10) ===\")\nfor type_name, count in type_counts.most_common(10):\n    print(f\"  {type_name}: {count}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### 1.2 Pr√©paration des donn√©es pour la classification\n\nNous allons cr√©er un dataset de classification o√π chaque article est associ√© √† son type principal (le premier type list√©).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# PARTIE 1.2 : Pr√©paration des donn√©es\n# ============================================\n\nfrom transformers import CamembertTokenizer, CamembertForSequenceClassification\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\n\n# Filtrer les articles qui ont au moins un type\narticles_with_type = [a for a in dataset['train'] if a['types'] and len(a['types']) > 0]\nprint(f\"Articles avec type : {len(articles_with_type)}\")\n\n# Garder les 5 types les plus fr√©quents pour simplifier\ntop_types = [t for t, _ in type_counts.most_common(5)]\nprint(f\"Types retenus : {top_types}\")\n\n# Cr√©er le mapping type ‚Üí label\ntype_to_label = {t: i for i, t in enumerate(top_types)}\nlabel_to_type = {i: t for t, i in type_to_label.items()}\nprint(f\"Mapping : {type_to_label}\")\n\n# Filtrer et pr√©parer les donn√©es\ndata = []\nfor article in articles_with_type:\n    primary_type = article['types'][0]\n    if primary_type in type_to_label:\n        data.append({\n            'text': article['content'][:512],  # Limiter la longueur\n            'label': type_to_label[primary_type],\n            'title': article['title']\n        })\n\nprint(f\"\\nDonn√©es pr√©par√©es : {len(data)} exemples\")\n\n# Afficher la distribution\nfrom collections import Counter\nlabel_dist = Counter([d['label'] for d in data])\nfor label, count in sorted(label_dist.items()):\n    print(f\"  {label_to_type[label]}: {count}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### 1.3 Fine-tuning CamemBERT\n\nNous allons fine-tuner CamemBERT pour la classification de texte. Le processus :\n1. Charger CamemBERT pr√©-entra√Æn√©\n2. Ajouter une couche de classification\n3. Entra√Æner sur notre dataset Pok√©mon",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# PARTIE 1.3 : Fine-tuning CamemBERT\n# ============================================\n\nfrom transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nimport random\n\n# Charger le tokenizer et le mod√®le\nprint(\"Chargement de CamemBERT...\")\ntokenizer_camembert = CamembertTokenizer.from_pretrained(\"camembert-base\")\nmodel_camembert = CamembertForSequenceClassification.from_pretrained(\n    \"camembert-base\",\n    num_labels=len(top_types),\n    output_attentions=True\n)\n\n# Dataset PyTorch\nclass PokemonDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        encoding = self.tokenizer(\n            item['text'],\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'labels': torch.tensor(item['label'])\n        }\n\n# Split train/test\nrandom.shuffle(data)\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n\ntrain_dataset = PokemonDataset(train_data, tokenizer_camembert)\ntest_dataset = PokemonDataset(test_data, tokenizer_camembert)\n\nprint(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")\n\n# Configuration de l'entra√Ænement (rapide pour la d√©mo)\ntraining_args = TrainingArguments(\n    output_dir=\"./pokemon_classifier\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=100,\n    weight_decay=0.01,\n    logging_steps=50,\n    eval_strategy=\"epoch\",\n    save_strategy=\"no\",\n    report_to=\"none\"\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model_camembert,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n\n# Entra√Ænement\nprint(\"\\nüöÄ D√©but du fine-tuning...\")\ntrainer.train()\nprint(\"‚úÖ Fine-tuning termin√© !\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### Partie 2 : Visualisation de l'attention\n\nComparons l'attention **avant** et **apr√®s** fine-tuning sur des exemples Pok√©mon.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# PARTIE 2 : Visualisation de l'attention\n# ============================================\n\ndef visualize_attention(model, tokenizer, text, layer=5, heads_to_show=[0, 1, 2, 3]):\n    \"\"\"Visualise l'attention d'un mod√®le CamemBERT sur un texte.\"\"\"\n    \n    # Tokenizer\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=64)\n    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n    \n    # Forward avec attention\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs, output_attentions=True)\n    \n    # Extraire l'attention de la couche choisie\n    attention = outputs.attentions[layer][0]  # (num_heads, seq_len, seq_len)\n    \n    # Visualisation\n    fig, axes = plt.subplots(1, len(heads_to_show), figsize=(4*len(heads_to_show), 4))\n    if len(heads_to_show) == 1:\n        axes = [axes]\n    \n    for idx, head in enumerate(heads_to_show):\n        ax = axes[idx]\n        w = attention[head].numpy()[:len(tokens), :len(tokens)]\n        \n        im = ax.imshow(w, cmap='Blues')\n        ax.set_xticks(range(len(tokens)))\n        ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=7)\n        ax.set_yticks(range(len(tokens)))\n        ax.set_yticklabels(tokens, fontsize=7)\n        ax.set_title(f\"T√™te {head + 1}\", fontsize=10)\n    \n    plt.tight_layout()\n    return fig\n\n# Exemple de texte Pok√©mon\ntest_text = \"Pikachu est un Pok√©mon de type √âlectrik. Il peut lancer des attaques Tonnerre.\"\n\nprint(\"=== Visualisation sur un exemple Pok√©mon ===\")\nprint(f\"Texte : {test_text}\\n\")\n\n# Visualiser l'attention du mod√®le fine-tun√©\nfig = visualize_attention(model_camembert, tokenizer_camembert, test_text, layer=5)\nplt.suptitle(\"Attention apr√®s fine-tuning (couche 6)\", y=1.02)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### Partie 3 : Analyse des t√™tes sp√©cialis√©es (‚≠ê‚≠ê‚≠ê Bonus)\n\n**Question** : Apr√®s fine-tuning sur la classification Pok√©mon, certaines t√™tes se sont-elles sp√©cialis√©es ?\n\nPar exemple :\n- Une t√™te d√©tecte-t-elle les mots li√©s aux **types** (Feu, Eau, √âlectrik) ?\n- Une t√™te se concentre-t-elle sur les **noms de Pok√©mon** ?\n- Une t√™te capture-t-elle les **attaques** ?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# PARTIE 3 : Analyse des t√™tes sp√©cialis√©es\n# ============================================\n\ndef analyze_head_specialization(model, tokenizer, texts, keywords, layer=5):\n    \"\"\"\n    Analyse si certaines t√™tes se sp√©cialisent sur des mots-cl√©s sp√©cifiques.\n    \n    Args:\n        model: Mod√®le CamemBERT\n        tokenizer: Tokenizer correspondant\n        texts: Liste de textes √† analyser\n        keywords: Dict {cat√©gorie: [liste de mots]}\n        layer: Couche √† analyser\n    \"\"\"\n    num_heads = 12  # CamemBERT a 12 t√™tes par couche\n    head_scores = {cat: [0] * num_heads for cat in keywords}\n    \n    model.eval()\n    for text in texts:\n        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=64)\n        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n        \n        with torch.no_grad():\n            outputs = model(**inputs, output_attentions=True)\n        \n        attention = outputs.attentions[layer][0]  # (num_heads, seq_len, seq_len)\n        \n        # Pour chaque cat√©gorie de mots-cl√©s\n        for cat, kw_list in keywords.items():\n            for kw in kw_list:\n                # Trouver les tokens contenant ce mot-cl√©\n                kw_indices = [i for i, t in enumerate(tokens) if kw.lower() in t.lower()]\n                \n                if kw_indices:\n                    # Calculer l'attention moyenne vers ces tokens pour chaque t√™te\n                    for head in range(num_heads):\n                        attn = attention[head].numpy()\n                        # Attention moyenne des autres tokens vers les mots-cl√©s\n                        score = attn[:, kw_indices].mean()\n                        head_scores[cat][head] += score\n    \n    return head_scores\n\n# Mots-cl√©s √† rechercher\nkeywords = {\n    \"Types\": [\"feu\", \"eau\", \"√©lectrik\", \"plante\", \"normal\", \"combat\", \"psy\"],\n    \"Pok√©mon\": [\"pikachu\", \"dracaufeu\", \"mewtwo\", \"√©voli\", \"rondoudou\"],\n    \"Attaques\": [\"tonnerre\", \"lance\", \"attaque\", \"capacit√©\", \"puissance\"]\n}\n\n# Textes de test (utiliser des exemples du dataset)\ntest_texts = [d['text'] for d in test_data[:20]]\n\nprint(\"Analyse de la sp√©cialisation des t√™tes...\")\nhead_scores = analyze_head_specialization(model_camembert, tokenizer_camembert, test_texts, keywords)\n\n# Visualisation\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\nfor idx, (cat, scores) in enumerate(head_scores.items()):\n    ax = axes[idx]\n    ax.bar(range(1, 13), scores)\n    ax.set_xlabel(\"T√™te\")\n    ax.set_ylabel(\"Score d'attention\")\n    ax.set_title(f\"Attention vers mots '{cat}'\")\n    ax.set_xticks(range(1, 13))\n\nplt.tight_layout()\nplt.suptitle(\"Sp√©cialisation des t√™tes apr√®s fine-tuning\", y=1.02)\nplt.show()\n\n# Identifier les t√™tes les plus sp√©cialis√©es\nprint(\"\\n\" + \"=\"*60)\nprint(\"T√™tes les plus attentives par cat√©gorie :\")\nprint(\"=\"*60)\nfor cat, scores in head_scores.items():\n    top_head = scores.index(max(scores)) + 1\n    print(f\"  {cat}: T√™te {top_head} (score: {max(scores):.3f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}