{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "> **‚ö†Ô∏è EN COURS DE CONSTRUCTION**\n>\n> Ce notebook est en cours de finalisation. Merci de ne pas le consulter pour l'instant.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 03 - Multi-Head Attention\n",
    "\n",
    "**Module** : R√©seaux de Neurones Approfondissement  \n",
    "**Dur√©e** : 2h  \n",
    "**Objectif** : Comprendre et impl√©menter le Multi-Head Attention\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs p√©dagogiques\n",
    "\n",
    "√Ä la fin de ce TP, vous serez capable de :\n",
    "1. Expliquer pourquoi plusieurs t√™tes d'attention sont utiles\n",
    "2. Impl√©menter le Multi-Head Attention from scratch\n",
    "3. Visualiser ce que chaque t√™te apprend\n",
    "4. Comprendre le lien avec les Transformers\n",
    "\n",
    "---\n",
    "\n",
    "## Pr√©requis\n",
    "\n",
    "Ce TP suppose que vous avez compl√©t√© le **TP 02 - M√©canisme d'Attention** o√π vous avez :\n",
    "- Impl√©ment√© le Scaled Dot-Product Attention\n",
    "- Compris les concepts de Query, Key, Value\n",
    "- Visualis√© l'attention sur un vrai mod√®le\n",
    "\n",
    "Ici, nous allons voir comment **combiner plusieurs t√™tes d'attention** pour capturer diff√©rents types de relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab)\n",
    "!pip install torch matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rappel : Single-Head Attention\n",
    "\n",
    "Reprenons notre fonction d'attention du TP pr√©c√©dent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries, shape (..., seq_len, d_k)\n",
    "        K: Keys, shape (..., seq_len, d_k)\n",
    "        V: Values, shape (..., seq_len, d_v)\n",
    "        mask: Masque optionnel\n",
    "    \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Pourquoi Multi-Head ?\n",
    "\n",
    "### Le probl√®me avec une seule t√™te\n",
    "\n",
    "Une seule t√™te d'attention calcule **une** repr√©sentation des relations entre mots.\n",
    "\n",
    "Mais dans une phrase, il y a **plusieurs types de relations** :\n",
    "- Relations syntaxiques (sujet-verbe)\n",
    "- Relations s√©mantiques (sens)\n",
    "- Relations de proximit√©\n",
    "- etc.\n",
    "\n",
    "### La solution : plusieurs t√™tes\n",
    "\n",
    "Chaque t√™te peut apprendre √† d√©tecter un type de relation diff√©rent !\n",
    "\n",
    "**Analogie** : C'est comme avoir plusieurs experts qui analysent une phrase sous diff√©rents angles, puis combinent leurs analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration : diff√©rentes t√™tes peuvent capturer diff√©rentes relations\n",
    "phrase = [\"Le\", \"chat\", \"noir\", \"mange\", \"la\", \"souris\"]\n",
    "\n",
    "# T√™te 1 : Relations syntaxiques (sujet-verbe)\n",
    "attention_syntaxe = torch.tensor([\n",
    "    [0.3, 0.5, 0.1, 0.05, 0.03, 0.02],  # \"Le\" ‚Üí \"chat\"\n",
    "    [0.1, 0.3, 0.1, 0.4, 0.05, 0.05],   # \"chat\" ‚Üí \"mange\"\n",
    "    [0.1, 0.6, 0.2, 0.05, 0.03, 0.02],  # \"noir\" ‚Üí \"chat\"\n",
    "    [0.05, 0.5, 0.05, 0.2, 0.1, 0.1],   # \"mange\" ‚Üí \"chat\"\n",
    "    [0.02, 0.03, 0.02, 0.03, 0.3, 0.6], # \"la\" ‚Üí \"souris\"\n",
    "    [0.02, 0.1, 0.02, 0.4, 0.06, 0.4],  # \"souris\" ‚Üí \"mange\"\n",
    "])\n",
    "\n",
    "# T√™te 2 : Relations de proximit√©\n",
    "attention_proximite = torch.tensor([\n",
    "    [0.5, 0.4, 0.08, 0.01, 0.005, 0.005],\n",
    "    [0.3, 0.4, 0.25, 0.04, 0.005, 0.005],\n",
    "    [0.1, 0.35, 0.35, 0.15, 0.03, 0.02],\n",
    "    [0.02, 0.1, 0.3, 0.35, 0.18, 0.05],\n",
    "    [0.01, 0.02, 0.05, 0.25, 0.4, 0.27],\n",
    "    [0.005, 0.01, 0.02, 0.1, 0.35, 0.515],\n",
    "])\n",
    "\n",
    "# Visualisation c√¥te √† c√¥te\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (attn, title) in enumerate([\n",
    "    (attention_syntaxe, \"T√™te 1 : Relations syntaxiques\"),\n",
    "    (attention_proximite, \"T√™te 2 : Proximit√©\")\n",
    "]):\n",
    "    ax = axes[idx]\n",
    "    im = ax.imshow(attn, cmap='Blues')\n",
    "    ax.set_xticks(range(6))\n",
    "    ax.set_xticklabels(phrase, rotation=45)\n",
    "    ax.set_yticks(range(6))\n",
    "    ax.set_yticklabels(phrase)\n",
    "    ax.set_title(title)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Architecture Multi-Head Attention\n",
    "\n",
    "### Sch√©ma\n",
    "\n",
    "```\n",
    "Input (seq_len, embed_dim)\n",
    "        ‚Üì\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚Üì         ‚Üì        ‚Üì        ‚Üì\n",
    " Head 1   Head 2   Head 3   Head 4   (chaque t√™te a sa propre projection Q, K, V)\n",
    "   ‚Üì         ‚Üì        ‚Üì        ‚Üì\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚Üì\n",
    "    Concat\n",
    "        ‚Üì\n",
    "   Linear (projection de sortie)\n",
    "        ‚Üì\n",
    "Output (seq_len, embed_dim)\n",
    "```\n",
    "\n",
    "### Dimensions\n",
    "\n",
    "- **embed_dim** : Dimension des embeddings (ex: 512)\n",
    "- **num_heads** : Nombre de t√™tes (ex: 8)\n",
    "- **d_k = embed_dim / num_heads** : Dimension par t√™te (ex: 64)\n",
    "\n",
    "Chaque t√™te travaille avec une dimension r√©duite, puis on concat√®ne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Impl√©mentation √©tape par √©tape\n",
    "\n",
    "### √âtape 1 : Projections Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "embed_dim = 32\n",
    "num_heads = 4\n",
    "d_k = embed_dim // num_heads  # 32 / 4 = 8\n",
    "\n",
    "print(f\"embed_dim: {embed_dim}\")\n",
    "print(f\"num_heads: {num_heads}\")\n",
    "print(f\"d_k (dim par t√™te): {d_k}\")\n",
    "\n",
    "# Input\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "print(f\"\\nInput shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er les projections\n",
    "# On projette vers embed_dim (pas num_heads * d_k, c'est la m√™me chose)\n",
    "W_q = nn.Linear(embed_dim, embed_dim)\n",
    "W_k = nn.Linear(embed_dim, embed_dim)\n",
    "W_v = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "# Projeter\n",
    "Q = W_q(x)  # (batch, seq_len, embed_dim)\n",
    "K = W_k(x)\n",
    "V = W_v(x)\n",
    "\n",
    "print(f\"Q shape apr√®s projection: {Q.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 2 : Reshape pour s√©parer les t√™tes\n",
    "\n",
    "On doit transformer `(batch, seq_len, embed_dim)` en `(batch, num_heads, seq_len, d_k)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# EXERCICE 1 : Reshape pour multi-head\n# ============================================\n\n# Transformer (batch, seq_len, embed_dim) en (batch, num_heads, seq_len, d_k)\n# Indice : utilisez view() et transpose()\n\ndef split_heads(x, num_heads):\n    \"\"\"\n    Reshape (batch, seq_len, embed_dim) -> (batch, num_heads, seq_len, d_k)\n    \"\"\"\n    batch_size, seq_len, embed_dim = x.shape\n    d_k = embed_dim // num_heads\n    \n    # TODO: Impl√©menter le reshape en 2 √©tapes\n    # - S√©parer embed_dim en (num_heads, d_k)\n    # - R√©organiser pour avoir num_heads en position 1\n    \n    x = None  # √Ä compl√©ter\n    \n    return x\n\n# Test (d√©commentez apr√®s avoir compl√©t√© l'exercice)\n# Q_heads = split_heads(Q, num_heads)\n# print(f\"Q shape apr√®s split: {Q_heads.shape}\")  # Attendu: (2, 4, 6, 8)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 3 : Attention par t√™te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 4 : Concat√©ner les t√™tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# EXERCICE 2 : Concat des t√™tes\n# ============================================\n\ndef concat_heads(x):\n    \"\"\"\n    Reshape (batch, num_heads, seq_len, d_k) -> (batch, seq_len, embed_dim)\n    C'est l'inverse de split_heads\n    \"\"\"\n    batch_size, num_heads, seq_len, d_k = x.shape\n    embed_dim = num_heads * d_k\n    \n    # TODO: Impl√©menter le reshape inverse\n    # Indice : inverse des op√©rations de split_heads\n    # N'oubliez pas .contiguous() si n√©cessaire\n    \n    x = None  # √Ä compl√©ter\n    \n    return x\n\n# Test (d√©commentez apr√®s avoir compl√©t√© les exercices 1 et 2)\n# Q_heads = split_heads(Q, num_heads)\n# K_heads = split_heads(K, num_heads)\n# V_heads = split_heads(V, num_heads)\n# attn_output, attn_weights = scaled_dot_product_attention(Q_heads, K_heads, V_heads)\n# concat_output = concat_heads(attn_output)\n# print(f\"Output apr√®s concat: {concat_output.shape}\")  # Attendu: (2, 6, 32)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 5 : Projection de sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection finale\n",
    "W_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "final_output = W_o(concat_output)\n",
    "print(f\"Final output shape: {final_output.shape}\")  # (2, 6, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention.\n",
    "    \n",
    "    Args:\n",
    "        embed_dim: Dimension des embeddings\n",
    "        num_heads: Nombre de t√™tes d'attention\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim doit √™tre divisible par num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads\n",
    "        \n",
    "        # TODO: Cr√©er les 4 projections lin√©aires\n",
    "        self.W_q = None  # nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = None\n",
    "        self.W_v = None\n",
    "        self.W_o = None\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"(batch, seq_len, embed_dim) -> (batch, num_heads, seq_len, d_k)\"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        # TODO: Impl√©menter\n",
    "        return None\n",
    "    \n",
    "    def concat_heads(self, x):\n",
    "        \"\"\"(batch, num_heads, seq_len, d_k) -> (batch, seq_len, embed_dim)\"\"\"\n",
    "        batch_size, _, seq_len, _ = x.shape\n",
    "        # TODO: Impl√©menter\n",
    "        return None\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input, shape (batch, seq_len, embed_dim)\n",
    "            mask: Masque optionnel\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, embed_dim)\n",
    "            attention_weights: (batch, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # TODO: Impl√©menter le forward\n",
    "        # 1. Projeter x en Q, K, V\n",
    "        Q = None\n",
    "        K = None\n",
    "        V = None\n",
    "        \n",
    "        # 2. Split en t√™tes\n",
    "        Q = None  # self.split_heads(Q)\n",
    "        K = None\n",
    "        V = None\n",
    "        \n",
    "        # 3. Attention\n",
    "        attn_output, attn_weights = None, None  # scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 4. Concat\n",
    "        concat_output = None  # self.concat_heads(attn_output)\n",
    "        \n",
    "        # 5. Projection de sortie\n",
    "        output = None  # self.W_o(concat_output)\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre impl√©mentation\n",
    "mha = MultiHeadAttention(embed_dim=32, num_heads=4)\n",
    "\n",
    "x = torch.randn(2, 6, 32)  # batch=2, seq=6, embed=32\n",
    "output, weights = mha(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")  # Attendu: (2, 6, 32)\n",
    "print(f\"Weights shape: {weights.shape}\")  # Attendu: (2, 4, 6, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Visualisation des t√™tes sur un vrai mod√®le\n",
    "\n",
    "Voyons ce que les diff√©rentes t√™tes capturent sur un mod√®le **r√©ellement entra√Æn√©**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger DistilBERT (comme au TP 02)\n",
    "!pip install transformers -q\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "model.eval()\n",
    "\n",
    "# Phrase de test\n",
    "phrase = \"The cat sat on the mat because it was tired\"\n",
    "inputs = tokenizer(phrase, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extraire les attentions de la couche 5 (12 t√™tes)\n",
    "layer = 4  # Couche 5 (index 0-5)\n",
    "attention = outputs.attentions[layer][0]  # (num_heads, seq_len, seq_len)\n",
    "\n",
    "print(f\"Phrase: {phrase}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Shape attention couche {layer+1}: {attention.shape}\")  # (12, 11, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser 4 t√™tes diff√©rentes de la m√™me couche\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# S√©lectionner 4 t√™tes int√©ressantes\n",
    "heads_to_show = [1, 2, 5, 10]  # Diff√©rentes t√™tes\n",
    "\n",
    "for idx, head in enumerate(heads_to_show):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    w = attention[head].numpy()\n",
    "    \n",
    "    im = ax.imshow(w, cmap='Blues')\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_yticklabels(tokens, fontsize=9)\n",
    "    ax.set_title(f\"T√™te {head + 1}\", fontsize=12)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle(f\"Diff√©rentes t√™tes de la couche {layer+1} - Chaque t√™te capture des relations diff√©rentes\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyse sp√©cifique : que regarde \"it\" selon diff√©rentes t√™tes ?\n",
    "it_index = tokens.index(\"it\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Que regarde le pronom 'it' selon chaque t√™te ?\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for head in heads_to_show:\n",
    "    print(f\"\\n--- T√™te {head + 1} ---\")\n",
    "    weights = attention[head, it_index].numpy()\n",
    "    top_indices = weights.argsort()[-3:][::-1]  # Top 3\n",
    "    for i in top_indices:\n",
    "        bar = \"‚ñà\" * int(weights[i] * 20)\n",
    "        print(f\"  {tokens[i]:10} {weights[i]:.2f} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Chaque t√™te a appris √† capturer des **relations diff√©rentes** :\n",
    "- Certaines t√™tes se concentrent sur la **cor√©f√©rence** (\"it\" ‚Üí \"cat\")\n",
    "- D'autres sur la **proximit√©** (mots voisins)\n",
    "- D'autres sur la **syntaxe** (sujet-verbe)\n",
    "- Certaines regardent le token **[CLS]** (repr√©sentation globale)\n",
    "\n",
    "C'est exactement ce qu'on voulait ! Multi-head = **plusieurs experts** qui analysent la phrase sous diff√©rents angles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Comparaison avec PyTorch nn.MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch fournit une impl√©mentation optimis√©e\n",
    "mha_pytorch = nn.MultiheadAttention(embed_dim=32, num_heads=4, batch_first=True)\n",
    "\n",
    "x = torch.randn(2, 6, 32)\n",
    "\n",
    "# Pour nn.MultiheadAttention, on passe query, key, value s√©par√©ment\n",
    "# En self-attention, query = key = value = x\n",
    "output_pytorch, weights_pytorch = mha_pytorch(x, x, x)\n",
    "\n",
    "print(f\"Output shape (PyTorch): {output_pytorch.shape}\")\n",
    "print(f\"Weights shape (PyTorch): {weights_pytorch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Exercice de synth√®se : Nombre de param√®tres\n",
    "\n",
    "Calculons le nombre de param√®tres dans notre MultiHeadAttention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 4 : Calcul des param√®tres\n",
    "# ============================================\n",
    "\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "# Combien de param√®tres dans :\n",
    "# - W_q : Linear(embed_dim, embed_dim) = embed_dim * embed_dim + embed_dim (weights + bias)\n",
    "# - W_k : idem\n",
    "# - W_v : idem\n",
    "# - W_o : idem\n",
    "\n",
    "params_per_linear = None  # TODO: Calculer\n",
    "total_params = None  # TODO: 4 * params_per_linear\n",
    "\n",
    "print(f\"Param√®tres par couche lin√©aire: {params_per_linear:,}\")\n",
    "print(f\"Total param√®tres MHA: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification\n",
    "mha_test = MultiHeadAttention(embed_dim=512, num_heads=8)\n",
    "real_params = sum(p.numel() for p in mha_test.parameters())\n",
    "print(f\"V√©rification PyTorch: {real_params:,} param√®tres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ============================================\n# EXERCICE 4 : Calcul des param√®tres\n# ============================================\n\nembed_dim = 512\nnum_heads = 8\n\n# Combien de param√®tres dans notre MultiHeadAttention ?\n# Rappel : nn.Linear(in, out) a (in * out + out) param√®tres (weights + bias)\n#\n# Indice : Comptez les param√®tres de chaque couche lin√©aire (W_q, W_k, W_v, W_o)\n\nparams_per_linear = None  # TODO: Calculer pour une couche Linear(embed_dim, embed_dim)\ntotal_params = None  # TODO: Calculer le total\n\nprint(f\"Param√®tres par couche lin√©aire: {params_per_linear:,}\")\nprint(f\"Total param√®tres MHA: {total_params:,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Pour aller plus loin : Cross-Attention\n",
    "\n",
    "> **Note** : Le cross-attention sera abord√© en d√©tail dans les **projets** (Mini-GPT, RAG).\n",
    "\n",
    "**Self-attention** (ce qu'on a fait) : Q, K, V viennent de la **m√™me** source.\n",
    "\n",
    "**Cross-attention** : Q vient d'une source, K et V d'une **autre** source.\n",
    "- Utilis√© en **traduction** : le d√©codeur (fran√ßais) \"interroge\" l'encodeur (anglais)\n",
    "- Utilis√© dans les **mod√®les g√©n√©ratifs** avec contexte externe (RAG)\n",
    "\n",
    "```\n",
    "Self-attention:     x ‚îÄ‚îÄ‚ñ∫ Q, K, V     (m√™me source)\n",
    "Cross-attention:    x_dec ‚îÄ‚îÄ‚ñ∫ Q       (une source)\n",
    "                    x_enc ‚îÄ‚îÄ‚ñ∫ K, V    (autre source)\n",
    "```\n",
    "\n",
    "Dans les TP 2-4, nous utilisons uniquement la **self-attention** (encodeur). Le cross-attention sera explor√© dans les projets avec les architectures g√©n√©ratives."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 11. Mini-projet : Classifier les Pok√©mon par type\n\n> **üè† BONUS / DEVOIR MAISON**\n> \n> Cette section est **optionnelle** et ne fait pas partie du TP en session.\n> Elle est propos√©e pour les √©tudiants qui souhaitent approfondir √† la maison.\n\nDans ce projet, vous allez **fine-tuner CamemBERT** (un mod√®le BERT fran√ßais) pour classifier des articles Pok√©mon par type (Feu, Eau, Plante...), puis **analyser les t√™tes d'attention** pour comprendre ce que le mod√®le a appris.\n\n### Objectifs p√©dagogiques\n\n1. Comprendre le **fine-tuning** d'un mod√®le pr√©-entra√Æn√©\n2. Observer comment les **t√™tes d'attention changent** apr√®s entra√Ænement\n3. Analyser si certaines t√™tes se **sp√©cialisent** sur des patterns sp√©cifiques\n\n### Structure du projet\n\n| Partie | Contenu | Difficult√© |\n|--------|---------|------------|\n| **Partie 1** | Fine-tuning CamemBERT pour classification | ‚≠ê‚≠ê |\n| **Partie 2** | Visualisation de l'attention avant/apr√®s | ‚≠ê‚≠ê |\n| **Partie 3** | Analyse des t√™tes sp√©cialis√©es | ‚≠ê‚≠ê‚≠ê |\n\n---\n\n### Partie 1 : Fine-tuning CamemBERT\n\n#### 1.1 Chargement du dataset Pok√©mon",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}