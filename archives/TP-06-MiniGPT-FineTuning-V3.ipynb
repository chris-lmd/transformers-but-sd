{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 06 V3 : Fine-tuning GPT-2 - Version PÃ©dagogique\n",
    "\n",
    "**Objectif** : Fine-tuner GPT-2 franÃ§ais pour gÃ©nÃ©rer des descriptions de PokÃ©mon\n",
    "\n",
    "**Structure V3** :\n",
    "1. **Exploration** : DÃ©couvrir le tokenizer, le dataset, les techniques\n",
    "2. **Configuration** : Tous les paramÃ¨tres au mÃªme endroit (juste avant l'entraÃ®nement)\n",
    "3. **EntraÃ®nement** : Lancer le fine-tuning\n",
    "\n",
    "**DurÃ©e** : 2h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTIE 1 : EXPLORATION\n",
    "\n",
    "Dans cette partie, on explore les donnÃ©es et les techniques sans encore lancer l'entraÃ®nement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dÃ©pendances (Colab)\n",
    "!pip install -q transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, TrainerCallback\nfrom datasets import load_dataset, concatenate_datasets\nimport matplotlib.pyplot as plt\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\n# ReproductibilitÃ©\ntorch.manual_seed(42)\nrandom.seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.1 Explorer le tokenizer GPT-2 franÃ§ais\n",
    "\n",
    "Voyons comment GPT-2 tokenise du texte, notamment les noms de PokÃ©mon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le tokenizer pour exploration\n",
    "print(\"Chargement du tokenizer GPT-2 franÃ§ais...\")\n",
    "tokenizer_demo = AutoTokenizer.from_pretrained(\"asi/gpt-fr-cased-base\")\n",
    "print(f\"Vocabulaire : {len(tokenizer_demo):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment le tokenizer dÃ©coupe-t-il les noms de PokÃ©mon ?\n",
    "test_words = [\"Pikachu\", \"Dracaufeu\", \"SalamÃ¨che\", \"Bulbizarre\", \"Mewtwo\", \"Ã‰volution\"]\n",
    "\n",
    "print(\"â•\" * 50)\n",
    "print(\"Comment GPT-2 tokenise les noms de PokÃ©mon ?\")\n",
    "print(\"â•\" * 50)\n",
    "\n",
    "for word in test_words:\n",
    "    tokens = tokenizer_demo.tokenize(word)\n",
    "    ids = tokenizer_demo.encode(word, add_special_tokens=False)\n",
    "    status = \"âœ… Token unique\" if len(tokens) == 1 else f\"âŒ {len(tokens)} sous-tokens\"\n",
    "    print(f\"'{word}' â†’ {tokens} (ids: {ids}) - {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VÃ©rifier si un mot existe dans le vocabulaire\n",
    "def check_vocab(word, tokenizer):\n",
    "    \"\"\"VÃ©rifie si un mot est un token unique dans le vocabulaire.\"\"\"\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    # Tester avec et sans le prÃ©fixe Ä  (espace)\n",
    "    in_vocab = word in vocab or f\"Ä {word}\" in vocab\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    return {\n",
    "        \"word\": word,\n",
    "        \"in_vocab\": in_vocab,\n",
    "        \"tokens\": tokens,\n",
    "        \"is_single\": len(tokens) == 1\n",
    "    }\n",
    "\n",
    "# Test\n",
    "result = check_vocab(\"Pikachu\", tokenizer_demo)\n",
    "print(f\"Pikachu dans le vocabulaire : {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : La plupart des noms de PokÃ©mon sont dÃ©coupÃ©s en plusieurs sous-tokens par BPE.\n",
    "\n",
    "C'est pourquoi on peut vouloir les ajouter au vocabulaire (voir section Configuration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 Explorer le dataset PokÃ©mon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les datasets\n",
    "print(\"Chargement des datasets...\")\n",
    "\n",
    "# Dataset principal (articles Pokepedia)\n",
    "dataset_pokepedia = load_dataset(\"chris-lmd/pokepedia-fr\")\n",
    "print(f\"\\nğŸ“š Pokepedia : {len(dataset_pokepedia['train']):,} articles\")\n",
    "\n",
    "# Liste des noms de PokÃ©mon\n",
    "dataset_names = load_dataset(\"chris-lmd/pokemon-names-fr\")\n",
    "POKEMON_NAMES = [item[\"name\"] for item in dataset_names[\"train\"]]\n",
    "print(f\"ğŸ“‹ Noms de PokÃ©mon : {len(POKEMON_NAMES)}\")\n",
    "print(f\"   Exemples : {POKEMON_NAMES[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AperÃ§u d'un article\n",
    "sample = dataset_pokepedia['train'][0]\n",
    "print(f\"Titre: {sample.get('title', 'N/A')}\")\n",
    "print(f\"\\nContenu (500 premiers caractÃ¨res):\")\n",
    "print(sample['content'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse : combien d'articles parlent vraiment de PokÃ©mon ?\n",
    "pokemon_names_set = set(name.lower() for name in POKEMON_NAMES)\n",
    "\n",
    "def is_pokemon_article(example):\n",
    "    \"\"\"Article dont le titre est un nom de PokÃ©mon.\"\"\"\n",
    "    title = example.get('title', '').lower()\n",
    "    return title in pokemon_names_set\n",
    "\n",
    "pokemon_articles = dataset_pokepedia['train'].filter(is_pokemon_article)\n",
    "other_articles = dataset_pokepedia['train'].filter(lambda x: not is_pokemon_article(x))\n",
    "\n",
    "print(\"â•\" * 50)\n",
    "print(\"Analyse du dataset\")\n",
    "print(\"â•\" * 50)\n",
    "print(f\"Total articles : {len(dataset_pokepedia['train']):,}\")\n",
    "print(f\"Articles PokÃ©mon (titre = nom) : {len(pokemon_articles):,}\")\n",
    "print(f\"Autres articles : {len(other_articles):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemples d'articles PokÃ©mon vs autres\n",
    "print(\"\\nğŸ“ Exemples d'articles POKÃ‰MON :\")\n",
    "for i in range(min(5, len(pokemon_articles))):\n",
    "    print(f\"  - {pokemon_articles[i]['title']}\")\n",
    "\n",
    "print(\"\\nğŸ“ Exemples d'AUTRES articles :\")\n",
    "for i in range(min(5, len(other_articles))):\n",
    "    print(f\"  - {other_articles[i]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.3 Comprendre les techniques d'optimisation\n",
    "\n",
    "### Technique 1 : Ajouter des tokens au vocabulaire (Smart Initialization)\n",
    "\n",
    "**ProblÃ¨me** : \"Pikachu\" est dÃ©coupÃ© en sous-tokens â†’ le modÃ¨le doit apprendre Ã  reconstituer le concept.\n",
    "\n",
    "**Solution** : Ajouter \"Pikachu\" comme token unique, initialisÃ© avec l'embedding de \"PokÃ©mon\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combien de noms de PokÃ©mon ne sont PAS des tokens uniques ?\n",
    "tokens_to_add = []\n",
    "tokens_already_exist = []\n",
    "\n",
    "for name in POKEMON_NAMES:\n",
    "    tokens = tokenizer_demo.encode(name, add_special_tokens=False)\n",
    "    if len(tokens) > 1:\n",
    "        tokens_to_add.append(name)\n",
    "    else:\n",
    "        tokens_already_exist.append(name)\n",
    "\n",
    "print(f\"Noms dÃ©jÃ  dans le vocabulaire : {len(tokens_already_exist)}\")\n",
    "print(f\"Noms Ã  ajouter (multi-tokens) : {len(tokens_to_add)}\")\n",
    "print(f\"\\nExemples de noms Ã  ajouter : {tokens_to_add[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 2 : Figer les couches basses (Partial Freezing)\n",
    "\n",
    "**Principe** : Les couches basses capturent la grammaire gÃ©nÃ©rale (dÃ©jÃ  bien apprise). On ne fine-tune que les couches hautes (sÃ©mantique).\n",
    "\n",
    "**Avantages** : Plus rapide, moins d'overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'architecture GPT-2\n",
    "print(\"â•\" * 50)\n",
    "print(\"Architecture GPT-2\")\n",
    "print(\"â•\" * 50)\n",
    "print()\n",
    "print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"â”‚   Token Embeddings (wte)    â”‚ â† ReprÃ©sentation des mots\")\n",
    "print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "print(\"â”‚ Position Embeddings (wpe)   â”‚ â† Position dans la sÃ©quence\")\n",
    "print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "print(\"â”‚   Transformer Block 0      â”‚\")\n",
    "print(\"â”‚   Transformer Block 1      â”‚ â† Couches BASSES\")\n",
    "print(\"â”‚          ...               â”‚   (grammaire, syntaxe)\")\n",
    "print(\"â”‚   Transformer Block N/2    â”‚   â†’ Ã€ FIGER\")\n",
    "print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "print(\"â”‚   Transformer Block N/2+1  â”‚\")\n",
    "print(\"â”‚          ...               â”‚ â† Couches HAUTES\")\n",
    "print(\"â”‚   Transformer Block N-1    â”‚   (sÃ©mantique, style)\")\n",
    "print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â†’ Ã€ ENTRAÃNER\")\n",
    "print(\"â”‚      Layer Norm (ln_f)      â”‚\")\n",
    "print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "print(\"â”‚        LM Head             â”‚ â† PrÃ©diction du prochain token\")\n",
    "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print()\n",
    "print(\"ModÃ¨le small : 12 couches\")\n",
    "print(\"ModÃ¨le base  : 24 couches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTIE 2 : CONFIGURATION ET ENTRAÃNEMENT\n",
    "\n",
    "Maintenant que nous avons explorÃ© les donnÃ©es et les techniques, configurons et lanÃ§ons l'entraÃ®nement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.1 âš™ï¸ CONFIGURATION CENTRALE\n",
    "\n",
    "**Tous les paramÃ¨tres sont ici.** Modifiez cette cellule puis exÃ©cutez les suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘                    CONFIGURATION CENTRALE                        â•‘\n",
    "# â•‘         Modifiez ces paramÃ¨tres puis exÃ©cutez la suite          â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# MODÃˆLE\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MODEL_SIZE = \"base\"      # \"small\" (12 couches, rapide) ou \"base\" (24 couches, meilleur)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# TECHNIQUES D'OPTIMISATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ADD_POKEMON_TOKENS = True     # Ajouter les noms de PokÃ©mon au vocabulaire\n",
    "FREEZE_LOWER_LAYERS = True    # Figer les couches basses\n",
    "FREEZE_RATIO = 0.5            # Proportion de couches Ã  figer (0.5 = 50%)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# DATASET\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MAX_SAMPLES = 5000       # Nombre d'articles pour l'entraÃ®nement\n",
    "MAX_LENGTH = 256         # Longueur max des sÃ©quences (tokens)\n",
    "PRIORITIZE_POKEMON = True     # Prioriser les vrais articles PokÃ©mon\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ENTRAÃNEMENT\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "NUM_EPOCHS = 3           # Nombre d'epochs\n",
    "LEARNING_RATE = 5e-5     # Taux d'apprentissage\n",
    "BATCH_SIZE = None        # None = automatique selon le modÃ¨le\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Affichage de la configuration\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"â•”\" + \"â•\" * 58 + \"â•—\")\n",
    "print(\"â•‘\" + \" CONFIGURATION ACTIVE \".center(58) + \"â•‘\")\n",
    "print(\"â• \" + \"â•\" * 58 + \"â•£\")\n",
    "print(f\"â•‘  ModÃ¨le          : {MODEL_SIZE:<37} â•‘\")\n",
    "print(f\"â•‘  Tokens PokÃ©mon  : {str(ADD_POKEMON_TOKENS):<37} â•‘\")\n",
    "print(f\"â•‘  Freeze layers   : {str(FREEZE_LOWER_LAYERS):<37} â•‘\")\n",
    "print(f\"â•‘  Freeze ratio    : {FREEZE_RATIO:<37} â•‘\")\n",
    "print(f\"â•‘  Max samples     : {MAX_SAMPLES:<37} â•‘\")\n",
    "print(f\"â•‘  Max length      : {MAX_LENGTH:<37} â•‘\")\n",
    "print(f\"â•‘  Epochs          : {NUM_EPOCHS:<37} â•‘\")\n",
    "print(f\"â•‘  Learning rate   : {LEARNING_RATE:<37} â•‘\")\n",
    "print(\"â•š\" + \"â•\" * 58 + \"â•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2 Chargement du modÃ¨le et tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modÃ¨le selon la configuration\n",
    "model_name = f\"asi/gpt-fr-cased-{MODEL_SIZE}\"\n",
    "\n",
    "print(f\"Chargement du modÃ¨le : {model_name}\")\n",
    "print(\"Chargement du tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Chargement du modÃ¨le...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ajouter un token de padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"\\nâœ… ModÃ¨le chargÃ© !\")\n",
    "print(f\"   ParamÃ¨tres : {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Vocabulaire : {len(tokenizer):,} tokens\")\n",
    "print(f\"   Couches : {model.config.n_layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.3 Application des techniques d'optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# TECHNIQUE 1 : Ajouter les tokens PokÃ©mon\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def add_pokemon_tokens_to_vocab(tokenizer, model, pokemon_names):\n",
    "    \"\"\"Ajoute les noms de PokÃ©mon au vocabulaire avec smart initialization.\"\"\"\n",
    "    \n",
    "    # Trouver le token de rÃ©fÃ©rence\n",
    "    reference_tokens = [\"PokÃ©mon\", \"Pokemon\", \"animal\", \"crÃ©ature\"]\n",
    "    reference_id = None\n",
    "    reference_word = None\n",
    "    \n",
    "    for ref in reference_tokens:\n",
    "        tokens = tokenizer.encode(ref, add_special_tokens=False)\n",
    "        if len(tokens) == 1:\n",
    "            reference_id = tokens[0]\n",
    "            reference_word = ref\n",
    "            break\n",
    "    \n",
    "    if reference_id is None:\n",
    "        reference_id = tokenizer.encode(\"PokÃ©mon\", add_special_tokens=False)[0]\n",
    "        reference_word = \"PokÃ©mon (premier sous-token)\"\n",
    "    \n",
    "    print(f\"   Token de rÃ©fÃ©rence : '{reference_word}' (id={reference_id})\")\n",
    "    \n",
    "    # Filtrer les noms qui ne sont pas dÃ©jÃ  des tokens uniques\n",
    "    new_tokens = [name for name in pokemon_names \n",
    "                  if len(tokenizer.encode(name, add_special_tokens=False)) > 1]\n",
    "    \n",
    "    print(f\"   Tokens Ã  ajouter : {len(new_tokens)}\")\n",
    "    \n",
    "    if len(new_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Sauvegarder l'embedding de rÃ©fÃ©rence\n",
    "    with torch.no_grad():\n",
    "        reference_embedding = model.transformer.wte.weight[reference_id].clone()\n",
    "    \n",
    "    # Ajouter les tokens\n",
    "    num_added = tokenizer.add_tokens(new_tokens)\n",
    "    old_size = model.transformer.wte.weight.shape[0]\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Initialiser avec l'embedding de rÃ©fÃ©rence + bruit\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_added):\n",
    "            new_token_id = old_size + i\n",
    "            noise = torch.randn_like(reference_embedding) * 0.01\n",
    "            model.transformer.wte.weight[new_token_id] = reference_embedding + noise\n",
    "    \n",
    "    return num_added\n",
    "\n",
    "\n",
    "if ADD_POKEMON_TOKENS:\n",
    "    print(\"â•\" * 50)\n",
    "    print(\"TECHNIQUE 1 : Ajout des tokens PokÃ©mon\")\n",
    "    print(\"â•\" * 50)\n",
    "    \n",
    "    # Test AVANT\n",
    "    tokens_before = tokenizer.tokenize(\"Pikachu\")\n",
    "    print(f\"\\n   AVANT : 'Pikachu' â†’ {tokens_before}\")\n",
    "    \n",
    "    # Ajout des tokens\n",
    "    num_added = add_pokemon_tokens_to_vocab(tokenizer, model, POKEMON_NAMES)\n",
    "    \n",
    "    # Test APRÃˆS\n",
    "    tokens_after = tokenizer.tokenize(\"Pikachu\")\n",
    "    print(f\"   APRÃˆS : 'Pikachu' â†’ {tokens_after}\")\n",
    "    print(f\"\\n   âœ… {num_added} tokens ajoutÃ©s au vocabulaire\")\n",
    "else:\n",
    "    print(\"âŒ Option ADD_POKEMON_TOKENS dÃ©sactivÃ©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# TECHNIQUE 2 : Figer les couches basses\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def freeze_lower_layers(model, freeze_ratio):\n",
    "    \"\"\"Fige les embeddings et les N premiÃ¨res couches.\"\"\"\n",
    "    total_layers = model.config.n_layer\n",
    "    num_to_freeze = int(total_layers * freeze_ratio)\n",
    "    \n",
    "    # Figer les embeddings\n",
    "    for param in model.transformer.wte.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.transformer.wpe.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Figer les N premiÃ¨res couches\n",
    "    for i in range(num_to_freeze):\n",
    "        for param in model.transformer.h[i].parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Stats\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    frozen = total_params - trainable\n",
    "    \n",
    "    return num_to_freeze, trainable, frozen, total_params\n",
    "\n",
    "\n",
    "if FREEZE_LOWER_LAYERS:\n",
    "    print(\"\\n\" + \"â•\" * 50)\n",
    "    print(\"TECHNIQUE 2 : Freezing des couches basses\")\n",
    "    print(\"â•\" * 50)\n",
    "    \n",
    "    num_frozen, trainable, frozen, total = freeze_lower_layers(model, FREEZE_RATIO)\n",
    "    \n",
    "    print(f\"\\n   Couches figÃ©es : {num_frozen} / {model.config.n_layer}\")\n",
    "    print(f\"   ParamÃ¨tres figÃ©s : {frozen:,} ({100*frozen/total:.1f}%)\")\n",
    "    print(f\"   ParamÃ¨tres entraÃ®nables : {trainable:,} ({100*trainable/total:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\nâŒ Option FREEZE_LOWER_LAYERS dÃ©sactivÃ©e\")\n",
    "    print(\"   Tous les paramÃ¨tres seront entraÃ®nÃ©s (full fine-tuning)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DÃ©placer le modÃ¨le sur GPU\n",
    "model = model.to(device)\n",
    "print(f\"\\nâœ… ModÃ¨le dÃ©placÃ© sur : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.4 Test du modÃ¨le AVANT fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100, temperature=0.8):\n",
    "    \"\"\"GÃ©nÃ¨re du texte Ã  partir d'un prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=50,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test\n",
    "prompt = \"Pikachu est un PokÃ©mon de type\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nGÃ©nÃ©ration AVANT fine-tuning:\")\n",
    "print(generate_text(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.5 PrÃ©paration du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constitution du dataset d'entraÃ®nement\n",
    "if PRIORITIZE_POKEMON:\n",
    "    # Prioriser les vrais articles PokÃ©mon\n",
    "    num_pokemon = len(pokemon_articles)\n",
    "    \n",
    "    if num_pokemon >= MAX_SAMPLES:\n",
    "        train_dataset = pokemon_articles.shuffle(seed=42).select(range(MAX_SAMPLES))\n",
    "        print(f\"âœ… Dataset : {MAX_SAMPLES} vrais PokÃ©mon\")\n",
    "    else:\n",
    "        # ComplÃ©ter avec d'autres articles\n",
    "        remaining = MAX_SAMPLES - num_pokemon\n",
    "        other_sample = other_articles.shuffle(seed=42).select(range(min(remaining, len(other_articles))))\n",
    "        train_dataset = concatenate_datasets([pokemon_articles, other_sample]).shuffle(seed=42)\n",
    "        print(f\"âœ… Dataset : {len(train_dataset)} articles\")\n",
    "        print(f\"   - Vrais PokÃ©mon : {num_pokemon}\")\n",
    "        print(f\"   - Autres : {len(other_sample)}\")\n",
    "else:\n",
    "    # Ã‰chantillon alÃ©atoire\n",
    "    train_dataset = dataset_pokepedia['train'].shuffle(seed=42).select(range(MAX_SAMPLES))\n",
    "    print(f\"âœ… Dataset : {MAX_SAMPLES} articles (alÃ©atoire)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['content'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "print(\"Tokenization en cours...\")\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "# Ajouter les labels\n",
    "def add_labels(examples):\n",
    "    examples['labels'] = examples['input_ids'].copy()\n",
    "    return examples\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)\n",
    "print(f\"âœ… Tokenization terminÃ©e !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.6 Lancement de l'entraÃ®nement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Callback pour enregistrer la loss Ã  chaque step\nclass LossHistoryCallback(TrainerCallback):\n    def __init__(self):\n        self.loss_history = []\n        self.steps = []\n    \n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs is not None and \"loss\" in logs:\n            self.loss_history.append(logs[\"loss\"])\n            self.steps.append(state.global_step)\n\n# CrÃ©er le callback\nloss_callback = LossHistoryCallback()\n\n# Batch size adaptÃ© au modÃ¨le\nif BATCH_SIZE is None:\n    batch_size = 2 if MODEL_SIZE == \"base\" else 4\n    grad_accum = 8 if MODEL_SIZE == \"base\" else 4\nelse:\n    batch_size = BATCH_SIZE\n    grad_accum = 4\n\ntraining_args = TrainingArguments(\n    output_dir=\"./gpt2-pokemon-v3\",\n    overwrite_output_dir=True,\n    \n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=grad_accum,\n    \n    learning_rate=LEARNING_RATE,\n    warmup_steps=100,\n    weight_decay=0.01,\n    \n    logging_steps=10,  # Log plus frÃ©quent pour la courbe\n    save_steps=500,\n    save_total_limit=2,\n    \n    fp16=torch.cuda.is_available(),\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    callbacks=[loss_callback],  # Ajouter le callback\n)\n\nprint(\"âœ… Trainer crÃ©Ã© avec monitoring de la loss !\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RÃ©sumÃ© final avant entraÃ®nement\n",
    "print(\"â•”\" + \"â•\" * 58 + \"â•—\")\n",
    "print(\"â•‘\" + \" RÃ‰SUMÃ‰ - PRÃŠT POUR L'ENTRAÃNEMENT \".center(58) + \"â•‘\")\n",
    "print(\"â• \" + \"â•\" * 58 + \"â•£\")\n",
    "print(f\"â•‘  ğŸ¤– ModÃ¨le : {model_name:<43} â•‘\")\n",
    "print(f\"â•‘  ğŸ“Š Dataset : {len(train_dataset):,} articles{' ':<33} â•‘\")\n",
    "print(f\"â•‘  ğŸ“ Max length : {MAX_LENGTH} tokens{' ':<32} â•‘\")\n",
    "print(f\"â•‘  ğŸ”„ Epochs : {NUM_EPOCHS}{' ':<44} â•‘\")\n",
    "print(f\"â•‘  ğŸ“¦ Batch : {batch_size} x {grad_accum} = {batch_size * grad_accum} effectif{' ':<25} â•‘\")\n",
    "print(\"â• \" + \"â•\" * 58 + \"â•£\")\n",
    "print(f\"â•‘  âœ¨ Tokens PokÃ©mon : {str(ADD_POKEMON_TOKENS):<36} â•‘\")\n",
    "frozen_layers = int(model.config.n_layer * FREEZE_RATIO) if FREEZE_LOWER_LAYERS else 0\n",
    "print(f\"â•‘  ğŸ§Š Couches figÃ©es : {frozen_layers}/{model.config.n_layer}{' ':<35} â•‘\")\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"â•‘  ğŸ§  Params entraÃ®nables : {trainable:,} ({100*trainable/total:.0f}%){' ':<14} â•‘\")\n",
    "print(\"â•š\" + \"â•\" * 58 + \"â•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ LANCEMENT DE L'ENTRAÃNEMENT\n",
    "print(\"ğŸš€ Fine-tuning en cours...\")\n",
    "print()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nâœ… Fine-tuning terminÃ© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### ğŸ“Š Analyse de la loss\n\n**Comment interprÃ©ter la courbe :**\n- **Descente rÃ©guliÃ¨re** â†’ Bon apprentissage\n- **Plateau** â†’ Le modÃ¨le a convergÃ©, plus d'epochs inutiles\n- **RemontÃ©e** â†’ Overfitting ! ArrÃªter l'entraÃ®nement\n- **Oscillations fortes** â†’ Learning rate trop Ã©levÃ©",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ğŸ“Š Courbe de la loss\nplt.figure(figsize=(12, 5))\n\n# Subplot 1 : Loss complÃ¨te\nplt.subplot(1, 2, 1)\nplt.plot(loss_callback.steps, loss_callback.loss_history, 'b-', alpha=0.7, linewidth=1)\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.grid(True, alpha=0.3)\n\n# Subplot 2 : Loss lissÃ©e (moyenne mobile)\nplt.subplot(1, 2, 2)\nwindow = min(10, len(loss_callback.loss_history) // 5) if len(loss_callback.loss_history) > 10 else 1\nif window > 1:\n    smoothed = []\n    for i in range(len(loss_callback.loss_history)):\n        start = max(0, i - window)\n        smoothed.append(sum(loss_callback.loss_history[start:i+1]) / (i - start + 1))\n    plt.plot(loss_callback.steps, smoothed, 'r-', linewidth=2, label=f'Moyenne mobile (window={window})')\n    plt.plot(loss_callback.steps, loss_callback.loss_history, 'b-', alpha=0.3, linewidth=1, label='Loss brute')\n    plt.legend()\nelse:\n    plt.plot(loss_callback.steps, loss_callback.loss_history, 'b-', linewidth=2)\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.title('Training Loss (lissÃ©e)')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Stats\nprint(f\"\\nğŸ“ˆ Statistiques :\")\nprint(f\"   Loss initiale : {loss_callback.loss_history[0]:.4f}\")\nprint(f\"   Loss finale   : {loss_callback.loss_history[-1]:.4f}\")\nprint(f\"   RÃ©duction     : {(1 - loss_callback.loss_history[-1]/loss_callback.loss_history[0])*100:.1f}%\")\nprint(f\"   Loss min      : {min(loss_callback.loss_history):.4f} (step {loss_callback.steps[loss_callback.loss_history.index(min(loss_callback.loss_history))]})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modÃ¨le\n",
    "trainer.save_model(\"./gpt2-pokemon-v3-final\")\n",
    "tokenizer.save_pretrained(\"./gpt2-pokemon-v3-final\")\n",
    "print(\"âœ… ModÃ¨le sauvegardÃ© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTIE 3 : GÃ‰NÃ‰RATION ET TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(prompt, max_length=150, temperature=0.8):\n",
    "    \"\"\"GÃ©nÃ¨re une description de PokÃ©mon.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test : Pikachu\n",
    "prompt = \"Pikachu est un PokÃ©mon de type\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nGÃ©nÃ©ration APRÃˆS fine-tuning:\")\n",
    "print(generate_description(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test : PokÃ©mon inventÃ©\n",
    "prompt = \"Flamador est un PokÃ©mon de type Feu. Il\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nGÃ©nÃ©ration:\")\n",
    "print(generate_description(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test : Ã‰volution\n",
    "prompt = \"Dracaufeu Ã©volue Ã  partir de\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nGÃ©nÃ©ration:\")\n",
    "print(generate_description(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RÃ©capitulatif\n",
    "\n",
    "### Configurations suggÃ©rÃ©es\n",
    "\n",
    "| ScÃ©nario | MODEL_SIZE | ADD_TOKENS | FREEZE | RÃ©sultat |\n",
    "|----------|-----------|------------|--------|----------|\n",
    "| Test rapide | small | False | False | ~10 min |\n",
    "| Ã‰quilibrÃ© | base | True | True | ~20 min, bon rÃ©sultat |\n",
    "| QualitÃ© max | base | True | False | ~40 min, meilleur |\n",
    "\n",
    "### Techniques apprises\n",
    "\n",
    "1. **Smart Token Initialization** : Ajouter des tokens spÃ©cialisÃ©s\n",
    "2. **Partial Freezing** : Figer les couches basses pour un entraÃ®nement efficace\n",
    "3. **Dataset curation** : Prioriser les donnÃ©es pertinentes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}