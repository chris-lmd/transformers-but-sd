{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet - Mini-GPT : Génération de Texte\n",
    "\n",
    "**Module** : Réseaux de Neurones Approfondissement  \n",
    "**Durée** : 2h  \n",
    "**Prérequis** : TPs 1-4 (Fondamentaux NLP, Attention, Multi-Head, Transformer)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs du projet\n",
    "\n",
    "Dans ce projet, vous allez :\n",
    "\n",
    "**Partie 1 - From Scratch (1h)**\n",
    "1. Comprendre la différence Encodeur vs Décodeur\n",
    "2. Implémenter le masque causal\n",
    "3. Construire un décodeur Transformer\n",
    "4. Générer des noms fantasy de manière autoregressive\n",
    "\n",
    "**Partie 2 - Fine-tuning GPT-2 (1h)**\n",
    "5. Utiliser un modèle pré-entraîné (GPT-2)\n",
    "6. Comparer les approches\n",
    "7. Explorer les paramètres de génération"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTIE 1 : Décodeur From Scratch\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers matplotlib numpy tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Télécharger le dataset de noms fantasy/celtes\n",
    "!wget -q https://raw.githubusercontent.com/chris-lmd/transformers-but-sd/main/data/fantasy_names.txt\n",
    "print(\"Dataset téléchargé !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Théorie : Encodeur vs Décodeur\n",
    "\n",
    "### BERT vs GPT\n",
    "\n",
    "| Modèle | Architecture | Attention | Tâche |\n",
    "|--------|-------------|-----------|-------|\n",
    "| **BERT** | Encodeur | Bidirectionnelle | Classification, NER, QA |\n",
    "| **GPT** | Décodeur | Causale (gauche→droite) | Génération de texte |\n",
    "\n",
    "### Pourquoi un masque causal ?\n",
    "\n",
    "Lors de l'entraînement de GPT, on veut prédire le prochain token. Mais l'attention standard permet à chaque position de \"voir\" toutes les autres positions, y compris les futures !\n",
    "\n",
    "**Le masque causal** empêche chaque position de regarder les positions futures. C'est comme cacher la réponse pendant un examen.\n",
    "\n",
    "```\n",
    "Sans masque (BERT) :     Avec masque (GPT) :\n",
    "Je  [voit tout]          Je  [voit: Je]\n",
    "suis [voit tout]         suis [voit: Je, suis]\n",
    "un  [voit tout]          un   [voit: Je, suis, un]\n",
    "chat [voit tout]         chat [voit: Je, suis, un, chat]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Le Masque Causal\n",
    "\n",
    "### Visualisation sans masque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulons une matrice d'attention sans masque\n",
    "seq_len = 5\n",
    "attn_scores = torch.randn(seq_len, seq_len)\n",
    "attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(attn_weights.numpy(), cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Clé (K)')\n",
    "plt.ylabel('Requête (Q)')\n",
    "plt.title('Attention SANS masque (Encodeur)')\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        plt.text(j, i, f'{attn_weights[i, j]:.2f}', ha='center', va='center', fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "print(\"Chaque position peut voir TOUTES les autres positions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1 : Créer le masque causal\n",
    "\n",
    "Le masque causal est une matrice triangulaire supérieure remplie de `-inf` :\n",
    "\n",
    "```\n",
    "[[0, -inf, -inf, -inf],\n",
    " [0,  0,   -inf, -inf],\n",
    " [0,  0,    0,   -inf],\n",
    " [0,  0,    0,    0  ]]\n",
    "```\n",
    "\n",
    "Après softmax, les `-inf` deviennent 0, empêchant l'attention sur les positions futures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Crée un masque causal de taille (seq_len, seq_len).\n",
    "    Les positions futures (triangulaire supérieure) sont masquées avec -inf.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Longueur de la séquence\n",
    "    \n",
    "    Returns:\n",
    "        Tensor de shape (seq_len, seq_len) avec 0 pour les positions autorisées\n",
    "        et -inf pour les positions masquées\n",
    "    \"\"\"\n",
    "    # TODO: Créer le masque causal\n",
    "    # Indice: Utilisez torch.triu() pour créer une matrice triangulaire supérieure\n",
    "    # et torch.ones() pour créer la matrice de base\n",
    "    \n",
    "    mask = None  # À COMPLÉTER\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du masque\n",
    "mask = create_causal_mask(5)\n",
    "print(\"Masque causal:\")\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation avec masque\n",
    "attn_scores_masked = attn_scores + mask\n",
    "attn_weights_masked = F.softmax(attn_scores_masked, dim=-1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Sans masque\n",
    "axes[0].imshow(attn_weights.numpy(), cmap='Blues')\n",
    "axes[0].set_title('SANS masque (Encodeur/BERT)')\n",
    "axes[0].set_xlabel('Clé (K)')\n",
    "axes[0].set_ylabel('Requête (Q)')\n",
    "\n",
    "# Avec masque\n",
    "axes[1].imshow(attn_weights_masked.numpy(), cmap='Blues')\n",
    "axes[1].set_title('AVEC masque causal (Décodeur/GPT)')\n",
    "axes[1].set_xlabel('Clé (K)')\n",
    "axes[1].set_ylabel('Requête (Q)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRemarquez que la partie supérieure droite est maintenant à 0 !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Pourquoi utilise-t-on `-inf` plutôt que `0` pour masquer ?\n",
    "\n",
    "*Réponse* : ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Dataset : Noms Fantasy\n",
    "\n",
    "Nous allons entraîner notre Mini-GPT à générer des noms fantasy et celtes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les noms\n",
    "with open('fantasy_names.txt', 'r', encoding='utf-8') as f:\n",
    "    names = [line.strip() for line in f if line.strip() and not line.startswith('#')]\n",
    "\n",
    "print(f\"Nombre de noms: {len(names)}\")\n",
    "print(f\"\\nExemples:\")\n",
    "for name in names[::500][:10]:  # Un nom tous les 500\n",
    "    print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le vocabulaire (caractères uniques)\n",
    "# On ajoute des tokens spéciaux\n",
    "PAD_TOKEN = '<PAD>'\n",
    "SOS_TOKEN = '<SOS>'  # Start of sequence\n",
    "EOS_TOKEN = '<EOS>'  # End of sequence\n",
    "\n",
    "# Collecter tous les caractères uniques\n",
    "chars = set()\n",
    "for name in names:\n",
    "    chars.update(name)\n",
    "\n",
    "chars = sorted(list(chars))\n",
    "vocab = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN] + chars\n",
    "\n",
    "# Créer les mappings\n",
    "char2idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "idx2char = {i: ch for ch, i in char2idx.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Taille du vocabulaire: {vocab_size}\")\n",
    "print(f\"Caractères: {vocab[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques sur les noms\n",
    "lengths = [len(name) for name in names]\n",
    "print(f\"Longueur min: {min(lengths)}\")\n",
    "print(f\"Longueur max: {max(lengths)}\")\n",
    "print(f\"Longueur moyenne: {np.mean(lengths):.1f}\")\n",
    "\n",
    "plt.hist(lengths, bins=30, edgecolor='black')\n",
    "plt.xlabel('Longueur du nom')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Distribution des longueurs de noms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset PyTorch pour la génération char-level\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, names, char2idx, max_len=25):\n",
    "        self.names = names\n",
    "        self.char2idx = char2idx\n",
    "        self.max_len = max_len\n",
    "        self.pad_idx = char2idx[PAD_TOKEN]\n",
    "        self.sos_idx = char2idx[SOS_TOKEN]\n",
    "        self.eos_idx = char2idx[EOS_TOKEN]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        name = self.names[idx]\n",
    "        \n",
    "        # Encoder: SOS + nom + EOS\n",
    "        encoded = [self.sos_idx] + [self.char2idx[c] for c in name] + [self.eos_idx]\n",
    "        \n",
    "        # Padding\n",
    "        if len(encoded) < self.max_len + 2:  # +2 pour SOS et EOS\n",
    "            encoded += [self.pad_idx] * (self.max_len + 2 - len(encoded))\n",
    "        else:\n",
    "            encoded = encoded[:self.max_len + 2]\n",
    "        \n",
    "        # Input: tout sauf le dernier token\n",
    "        # Target: tout sauf le premier token (décalé de 1)\n",
    "        input_ids = torch.tensor(encoded[:-1], dtype=torch.long)\n",
    "        target_ids = torch.tensor(encoded[1:], dtype=torch.long)\n",
    "        \n",
    "        return input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le dataset\n",
    "MAX_LEN = 25\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = NameDataset(names, char2idx, MAX_LEN)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Vérification\n",
    "input_ids, target_ids = dataset[0]\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Target shape: {target_ids.shape}\")\n",
    "\n",
    "# Décoder pour vérifier\n",
    "input_decoded = ''.join([idx2char[i.item()] for i in input_ids if i.item() != char2idx[PAD_TOKEN]])\n",
    "target_decoded = ''.join([idx2char[i.item()] for i in target_ids if i.item() != char2idx[PAD_TOKEN]])\n",
    "print(f\"\\nInput: {input_decoded}\")\n",
    "print(f\"Target: {target_decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Le Décodeur Transformer\n",
    "\n",
    "### Composants réutilisés des TPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Composants du Transformer (TPs 2-4) ===\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention avec support du masque causal.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        B, S, _ = x.shape\n",
    "        \n",
    "        Q = self.W_q(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Appliquer le masque causal si fourni\n",
    "        if mask is not None:\n",
    "            scores = scores + mask\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = (attn @ V).transpose(1, 2).contiguous().view(B, S, self.embed_dim)\n",
    "        \n",
    "        return self.W_o(out)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-Forward Network.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, ff_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        ff_dim = ff_dim or 4 * embed_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional Encoding sinusoïdal.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, :x.size(1)])\n",
    "\n",
    "\n",
    "print(\"Composants chargés !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : Le Décodeur Transformer\n",
    "\n",
    "Complétez la classe `TransformerDecoder` qui utilise :\n",
    "- Une couche d'embedding\n",
    "- Le positional encoding\n",
    "- Plusieurs blocs de décodeur (avec masque causal)\n",
    "- Une tête de prédiction (Linear → vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Un bloc du décodeur Transformer.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ff = FeedForward(embed_dim, ff_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm architecture\n",
    "        attn_out = self.attn(self.norm1(x), mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Mini-GPT : Décodeur Transformer pour la génération de texte.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, \n",
    "                 max_len=100, ff_dim=None, dropout=0.1, pad_idx=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        # TODO: Créer les couches\n",
    "        # 1. Embedding de taille (vocab_size, embed_dim)\n",
    "        # 2. Positional Encoding\n",
    "        # 3. Liste de DecoderBlock (num_layers blocs)\n",
    "        # 4. LayerNorm finale\n",
    "        # 5. Tête de prédiction (Linear: embed_dim → vocab_size)\n",
    "        \n",
    "        self.embedding = None  # À COMPLÉTER\n",
    "        self.pos_encoding = None  # À COMPLÉTER\n",
    "        self.layers = None  # À COMPLÉTER\n",
    "        self.norm = None  # À COMPLÉTER\n",
    "        self.head = None  # À COMPLÉTER\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, S = x.shape\n",
    "        \n",
    "        # TODO: Implémenter le forward pass\n",
    "        # 1. Embedding + scaling par sqrt(embed_dim)\n",
    "        # 2. Positional encoding\n",
    "        # 3. Créer le masque causal\n",
    "        # 4. Passer par tous les DecoderBlock\n",
    "        # 5. LayerNorm\n",
    "        # 6. Tête de prédiction\n",
    "        \n",
    "        logits = None  # À COMPLÉTER\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le modèle\n",
    "EMBED_DIM = 64\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 3\n",
    "FF_DIM = 256\n",
    "\n",
    "model = TransformerDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    max_len=MAX_LEN + 2,\n",
    "    ff_dim=FF_DIM,\n",
    "    dropout=0.1,\n",
    "    pad_idx=char2idx[PAD_TOKEN]\n",
    ").to(device)\n",
    "\n",
    "# Compter les paramètres\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Paramètres: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du modèle\n",
    "test_input = torch.randint(0, vocab_size, (2, 10)).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "print(f\"Expected: (2, 10, {vocab_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for input_ids, target_ids in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Loss (reshape pour CrossEntropy)\n",
    "        loss = criterion(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            target_ids.view(-1)\n",
    "        )\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EPOCHS = 50\n",
    "LR = 1e-3\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=char2idx[PAD_TOKEN])\n",
    "\n",
    "# Historique\n",
    "history = []\n",
    "\n",
    "print(\"Début de l'entraînement...\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train_epoch(model, dataloader, optimizer, criterion, device)\n",
    "    history.append(loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"\\nEntraînement terminé !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Courbe d\\'entraînement')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Génération Autoregressive\n",
    "\n",
    "### Exercice 3 : Implémenter la fonction de génération\n",
    "\n",
    "La génération autoregressive fonctionne ainsi :\n",
    "1. Commencer avec le token `<SOS>`\n",
    "2. Prédire le prochain token\n",
    "3. Ajouter ce token à la séquence\n",
    "4. Répéter jusqu'à `<EOS>` ou longueur max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, prompt=\"\", max_len=25, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Génère un nom de manière autoregressive.\n",
    "    \n",
    "    Args:\n",
    "        model: Le modèle TransformerDecoder\n",
    "        prompt: Début du nom (optionnel)\n",
    "        max_len: Longueur maximale\n",
    "        temperature: Contrôle la \"créativité\" (1.0 = normal)\n",
    "    \n",
    "    Returns:\n",
    "        Le nom généré (string)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialiser avec SOS + prompt\n",
    "    tokens = [char2idx[SOS_TOKEN]]\n",
    "    for c in prompt:\n",
    "        if c in char2idx:\n",
    "            tokens.append(char2idx[c])\n",
    "    \n",
    "    # TODO: Génération autoregressive\n",
    "    # Boucle jusqu'à max_len ou EOS:\n",
    "    #   1. Convertir tokens en tensor\n",
    "    #   2. Passer dans le modèle\n",
    "    #   3. Prendre les logits du dernier token\n",
    "    #   4. Appliquer temperature: logits = logits / temperature\n",
    "    #   5. Softmax pour obtenir les probabilités\n",
    "    #   6. Échantillonner le prochain token (torch.multinomial)\n",
    "    #   7. Si EOS, arrêter\n",
    "    #   8. Sinon, ajouter à tokens\n",
    "    \n",
    "    generated = \"\"  # À COMPLÉTER\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester la génération\n",
    "print(\"=\" * 50)\n",
    "print(\"GÉNÉRATION DE NOMS FANTASY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sans prompt\n",
    "print(\"\\nSans prompt (température=1.0):\")\n",
    "for _ in range(10):\n",
    "    name = generate(model, prompt=\"\", temperature=1.0)\n",
    "    print(f\"  {name}\")\n",
    "\n",
    "# Avec prompt\n",
    "print(\"\\nAvec prompt 'Gwen':\")\n",
    "for _ in range(5):\n",
    "    name = generate(model, prompt=\"Gwen\", temperature=1.0)\n",
    "    print(f\"  {name}\")\n",
    "\n",
    "print(\"\\nAvec prompt 'Mael':\")\n",
    "for _ in range(5):\n",
    "    name = generate(model, prompt=\"Mael\", temperature=1.0)\n",
    "    print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Température et Créativité\n",
    "\n",
    "### Exercice 4 : Explorer l'effet de la température\n",
    "\n",
    "La température contrôle la \"créativité\" de la génération :\n",
    "- **T < 1** : Plus conservateur, plus prévisible\n",
    "- **T = 1** : Distribution normale\n",
    "- **T > 1** : Plus créatif, plus aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer différentes températures\n",
    "temperatures = [0.3, 0.7, 1.0, 1.5, 2.0]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EFFET DE LA TEMPÉRATURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTempérature = {temp}:\")\n",
    "    for _ in range(5):\n",
    "        name = generate(model, prompt=\"\", temperature=temp)\n",
    "        print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Répondez aux questions suivantes\n",
    "\n",
    "# 1. Quelle température donne les noms les plus \"réalistes\" ?\n",
    "# Réponse: ...\n",
    "\n",
    "# 2. Quelle température donne les noms les plus \"originaux\" ?\n",
    "# Réponse: ...\n",
    "\n",
    "# 3. Pourquoi une température trop basse peut poser problème ?\n",
    "# Réponse: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTIE 2 : Fine-tuning GPT-2\n",
    "\n",
    "---\n",
    "\n",
    "Maintenant, comparons avec un \"vrai\" modèle de langage pré-entraîné."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Charger GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger GPT-2 (version petite)\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token\n",
    "\n",
    "model_gpt2 = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "num_params_gpt2 = sum(p.numel() for p in model_gpt2.parameters())\n",
    "print(f\"Modèle: {model_name}\")\n",
    "print(f\"Paramètres GPT-2: {num_params_gpt2:,}\")\n",
    "print(f\"Paramètres Mini-GPT: {num_params:,}\")\n",
    "print(f\"\\nRatio: GPT-2 est {num_params_gpt2/num_params:.0f}x plus grand !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Génération sans fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Créer un pipeline de génération\n",
    "generator = pipeline(\n",
    "    'text-generation',\n",
    "    model=model_gpt2,\n",
    "    tokenizer=tokenizer_gpt2,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Tester\n",
    "print(\"Génération GPT-2 (sans fine-tuning):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "prompts = [\"The wizard named\", \"In the realm of dragons,\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    result = generator(prompt, max_length=50, num_return_sequences=1, temperature=0.8)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Généré: {result[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Fine-tuning sur notre corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer le corpus pour GPT-2\n",
    "# On crée un fichier texte avec un nom par ligne\n",
    "with open('names_for_gpt2.txt', 'w') as f:\n",
    "    for name in names:\n",
    "        f.write(name + '\\n')\n",
    "\n",
    "print(f\"Corpus créé: {len(names)} noms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le dataset pour fine-tuning\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(file_path, tokenizer, block_size=128):\n",
    "    return TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size\n",
    "    )\n",
    "\n",
    "train_dataset = load_dataset('names_for_gpt2.txt', tokenizer_gpt2, block_size=32)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer_gpt2,\n",
    "    mlm=False  # GPT-2 n'utilise pas le MLM\n",
    ")\n",
    "\n",
    "print(f\"Dataset créé: {len(train_dataset)} exemples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-fantasy\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_gpt2,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "print(\"Fine-tuning GPT-2...\")\n",
    "trainer.train()\n",
    "print(\"\\nFine-tuning terminé !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester après fine-tuning\n",
    "print(\"Génération GPT-2 (APRÈS fine-tuning):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "generator_finetuned = pipeline(\n",
    "    'text-generation',\n",
    "    model=model_gpt2,\n",
    "    tokenizer=tokenizer_gpt2,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Générer des noms\n",
    "for _ in range(10):\n",
    "    result = generator_finetuned(\n",
    "        \"\", \n",
    "        max_length=15, \n",
    "        num_return_sequences=1, \n",
    "        temperature=0.8,\n",
    "        pad_token_id=tokenizer_gpt2.eos_token_id\n",
    "    )\n",
    "    print(result[0]['generated_text'].strip().split('\\n')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Paramètres de génération\n",
    "\n",
    "### Exercice 5 : Explorer les paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Expérimentez avec différents paramètres\n",
    "# - temperature: 0.5, 1.0, 1.5\n",
    "# - top_k: 10, 50, 100\n",
    "# - top_p: 0.9, 0.95, 1.0\n",
    "# - repetition_penalty: 1.0, 1.2, 1.5\n",
    "\n",
    "def generate_with_params(prompt=\"\", temperature=1.0, top_k=50, top_p=0.95, repetition_penalty=1.0):\n",
    "    result = generator_finetuned(\n",
    "        prompt,\n",
    "        max_length=20,\n",
    "        num_return_sequences=5,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        pad_token_id=tokenizer_gpt2.eos_token_id,\n",
    "        do_sample=True\n",
    "    )\n",
    "    return [r['generated_text'].strip().split('\\n')[0] for r in result]\n",
    "\n",
    "# Test\n",
    "print(\"Temperature basse (0.5):\")\n",
    "for name in generate_with_params(temperature=0.5):\n",
    "    print(f\"  {name}\")\n",
    "\n",
    "print(\"\\nTemperature haute (1.5):\")\n",
    "for name in generate_with_params(temperature=1.5):\n",
    "    print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Conclusion\n",
    "\n",
    "### Comparaison des approches\n",
    "\n",
    "| Aspect | Mini-GPT (From Scratch) | GPT-2 (Fine-tuning) |\n",
    "|--------|------------------------|--------------------|\n",
    "| Paramètres | ~50K | ~124M |\n",
    "| Temps d'entraînement | ~2 min | ~5 min |\n",
    "| Qualité | Bonne pour noms simples | Excellente |\n",
    "| Compréhension | ✅ Totale | ⚠️ Boîte noire |\n",
    "| Flexibilité | Limité aux noms | Texte libre |\n",
    "\n",
    "### Ce que vous avez appris\n",
    "\n",
    "1. **Masque causal** : Comment empêcher le modèle de \"tricher\" en regardant le futur\n",
    "2. **Décodeur Transformer** : L'architecture derrière GPT\n",
    "3. **Génération autoregressive** : Prédire token par token\n",
    "4. **Température** : Contrôler la créativité\n",
    "5. **Fine-tuning** : Adapter un modèle pré-entraîné à un domaine spécifique\n",
    "\n",
    "### Lien avec ChatGPT\n",
    "\n",
    "ChatGPT utilise la même architecture de base, mais avec :\n",
    "- **Beaucoup plus de paramètres** (~175B pour GPT-3)\n",
    "- **RLHF** (Reinforcement Learning from Human Feedback)\n",
    "- **Instruction tuning** pour suivre les consignes\n",
    "\n",
    "### Limites\n",
    "\n",
    "- Les modèles génèrent du texte **statistiquement probable**, pas **vrai**\n",
    "- Risque d'**hallucinations** (inventer des faits)\n",
    "- **Biais** présents dans les données d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récapitulatif final\n",
    "print(\"=\" * 60)\n",
    "print(\"RÉCAPITULATIF\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nMini-GPT (From Scratch):\")\n",
    "print(f\"  - Paramètres: {num_params:,}\")\n",
    "print(f\"  - Vocabulaire: {vocab_size} caractères\")\n",
    "print(f\"  - Dataset: {len(names)} noms fantasy/celtes\")\n",
    "\n",
    "print(f\"\\nGPT-2 (Fine-tuned):\")\n",
    "print(f\"  - Paramètres: {num_params_gpt2:,}\")\n",
    "print(f\"  - Ratio: {num_params_gpt2/num_params:.0f}x plus grand\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Bravo ! Vous avez construit votre propre Mini-GPT !\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
