{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 01 - Le MÃ©canisme d'Attention\n",
    "\n",
    "**Module** : RÃ©seaux de Neurones Approfondissement  \n",
    "**DurÃ©e** : 2h  \n",
    "**Objectif** : Comprendre et implÃ©menter le mÃ©canisme d'attention, brique fondamentale des Transformers\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs pÃ©dagogiques\n",
    "\n",
    "Ã€ la fin de ce TP, vous serez capable de :\n",
    "1. Expliquer intuitivement ce qu'est l'attention\n",
    "2. Comprendre les concepts de Query, Key, Value\n",
    "3. ImplÃ©menter le Scaled Dot-Product Attention\n",
    "4. Visualiser et interprÃ©ter les poids d'attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports\n",
    "\n",
    "ExÃ©cutez cette cellule pour installer les dÃ©pendances nÃ©cessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dÃ©pendances (Google Colab)\n",
    "!pip install torch matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 1. Introduction : Pourquoi l'attention ?\n\n> **Note pÃ©dagogique** : Dans les sessions 1 Ã  3, on se concentre sur le **fonctionnement** de l'architecture (infÃ©rence/forward pass). L'**entraÃ®nement** (backpropagation, optimisation) sera abordÃ© en session 4.\n\n### 1.1 Les architectures sÃ©quentielles (RNN / LSTM)\n\nLes **rÃ©seaux rÃ©currents (RNN)** traitent les sÃ©quences **mot par mot** :\n\n```\n        â”Œâ”€â”€â”€â”    â”Œâ”€â”€â”€â”    â”Œâ”€â”€â”€â”    â”Œâ”€â”€â”€â”    â”Œâ”€â”€â”€â”\n  xâ‚ â”€â”€â–¶â”‚ h â”œâ”€â”€â”€â–¶â”‚ h â”œâ”€â”€â”€â–¶â”‚ h â”œâ”€â”€â”€â–¶â”‚ h â”œâ”€â”€â”€â–¶â”‚ h â”œâ”€â”€â–¶ sortie\n  Le    â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜\n           â”‚        â”‚        â”‚        â”‚        â”‚\n          xâ‚‚       xâ‚ƒ       xâ‚„       xâ‚…       xâ‚†\n         chat     dort      sur      le     canapÃ©\n```\n\n**ProblÃ¨me** : L'information passe de cellule en cellule. Pour relier \"Le chat\" Ã  \"canapÃ©\", il faut traverser toute la chaÃ®ne â†’ l'info se dÃ©grade (gradient Ã©vanescent).\n\nLes **LSTM** ajoutent des \"portes\" pour mieux contrÃ´ler la mÃ©moire :\n\n```\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚      CELLULE LSTM       â”‚\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n        â”‚  Porte    â”‚  Porte    â”‚    Porte    â”‚\n        â”‚  Oubli    â”‚  EntrÃ©e   â”‚   Sortie    â”‚\n        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n              â”‚           â”‚            â”‚\n          Effacer?    Ajouter?    Utiliser?\n```\n\n**AmÃ©lioration** : Les LSTM retiennent mieux les infos longue distance.\n**Mais** : Toujours sÃ©quentiel (lent) et limitÃ© sur les trÃ¨s longues sÃ©quences.\n\n### 1.2 L'architecture Transformer\n\nLe **Transformer** (2017) abandonne la rÃ©currence. Chaque mot peut regarder **tous les autres directement** :\n\n```\nEntrÃ©e: \"Le chat dort sur le canapÃ©\" (6 tokens)\n         â”‚    â”‚    â”‚    â”‚    â”‚    â”‚\n         â–¼    â–¼    â–¼    â–¼    â–¼    â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚          EMBEDDINGS (6 vecteurs)            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚    â”‚    â”‚    â”‚    â”‚    â”‚\n         â–¼    â–¼    â–¼    â–¼    â–¼    â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚             SELF-ATTENTION                  â”‚\nâ”‚   Chaque vecteur regarde les 5 autres       â”‚\nâ”‚   â†’ Enrichit chaque mot avec le CONTEXTE    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚    â”‚    â”‚    â”‚    â”‚    â”‚\n         â–¼    â–¼    â–¼    â–¼    â–¼    â–¼\n       (6 vecteurs enrichis)\n         â”‚    â”‚    â”‚    â”‚    â”‚    â”‚\n         â–¼    â–¼    â–¼    â–¼    â–¼    â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚        FEED-FORWARD (par position)          â”‚\nâ”‚   Exploite le contexte enrichi              â”‚\nâ”‚   (comme un rÃ©seau de neurones classique)   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚    â”‚    â”‚    â”‚    â”‚    â”‚\n         â–¼    â–¼    â–¼    â–¼    â–¼    â–¼\n      Sortie: 6 vecteurs transformÃ©s\n```\n\n**Points clÃ©s** :\n- **EntrÃ©e = Sortie** : Si tu entres 6 mots â†’ tu obtiens 6 vecteurs enrichis\n- **Taille variable** : Tu peux entrer 5, 50, ou 500 mots (jusqu'Ã  une limite : 512 pour BERT, 128K pour GPT-4)\n- **Self-Attention** : Donne du contexte Ã  chaque mot\n- **Feed-Forward** : Exploite ce contexte (transformation non-linÃ©aire)\n\n**Que sort le Transformer ?**\n\nLe Transformer produit des **vecteurs enrichis** (reprÃ©sentations). Une couche de sortie (ajoutÃ©e selon la tÃ¢che) les transforme en rÃ©sultat :\n- **Classification** â†’ probabilitÃ© par classe (ex: 70% positif, 30% nÃ©gatif)\n- **GÃ©nÃ©ration** â†’ probabilitÃ© du prochain mot\n- **Traduction** â†’ phrase dans l'autre langue\n\n### Comment les mots entrent dans le Transformer ?\n\nChaque mot passe par **deux Ã©tapes** avant d'entrer :\n\n```\nMot \"chat\" (position 1)\n        â”‚\n        â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Token Embedding (fixe pour chaque token)        â”‚\nâ”‚ \"chat\" â†’ [0.8, 0.1, 0.3, ...]                   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        â”‚\n        + (addition)\n        â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Positional Encoding (fixe pour chaque position) â”‚\nâ”‚ position 1 â†’ [0.0, 0.1, 0.0, ...]               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        â”‚\n        â–¼\nVecteur d'entrÃ©e = [0.8, 0.2, 0.3, ...]\n```\n\n**Deux composants distincts, tous deux figÃ©s aprÃ¨s entraÃ®nement :**\n\n| Composant | Taille | RÃ´le |\n|-----------|--------|------|\n| Token embeddings | ~50k Ã— dim | \"Qui suis-je ?\" (sens du mot) |\n| Positional encodings | max_len Ã— dim | \"OÃ¹ suis-je ?\" (position dans la phrase) |\n\n**Pourquoi c'est important ?** Sans le positional encoding, le modÃ¨le ne distinguerait pas :\n- *\"Le chat mange la souris\"*\n- *\"La souris mange le chat\"*\n\n(MÃªmes tokens, ordre diffÃ©rent â†’ sens opposÃ© !)\n\n### 1.3 Empilement des blocs\n\nCes blocs (Attention + FFN) sont **empilÃ©s** : la sortie de l'un devient l'entrÃ©e du suivant.\n\n```\nEntrÃ©e (6 vecteurs)\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Attention 1   â”‚\nâ”‚        â†“        â”‚  Bloc 1\nâ”‚     FFN 1       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Attention 2   â”‚\nâ”‚        â†“        â”‚  Bloc 2\nâ”‚     FFN 2       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n        ...\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Attention N   â”‚\nâ”‚        â†“        â”‚  Bloc N (ex: N=12 pour BERT)\nâ”‚     FFN N       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nSortie (6 vecteurs trÃ¨s enrichis)\n```\n\nChaque passage enrichit les reprÃ©sentations. AprÃ¨s N blocs, chaque mot \"comprend\" toute la phrase.\n\n### 1.4 Ce qu'on va construire\n\n```\n    TRANSFORMER\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚  Embedding + Positional    â”‚\n    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n    â”‚ â”‚   SELF-ATTENTION  â—€â”€â”€â”€â”€â”¼â”€â”¼â”€â”€â”€ Sessions 1-2\n    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n    â”‚ â”‚     FEED-FORWARD       â”‚ â”‚\n    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n    â”‚         Ã— N blocs          â”‚\n    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    â”‚     Couche de sortie       â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Plan du cours** :\n- **Session 1** : MÃ©canisme d'attention (ce TP)\n- **Session 2** : Multi-Head Attention\n- **Session 3** : Assembler le Transformer complet\n- **Sessions 4-5** : EntraÃ®nement et projets\n\n### 1.5 L'idÃ©e clÃ© de l'attention\n\nL'attention rÃ©pond Ã  la question : **\"Pour comprendre ce mot, quels autres mots dois-je regarder ?\"**\n\n**Exemple** : *\"Le chat qui dormait sur le canapÃ© a sautÃ©\"*\n- Pour comprendre **\"a sautÃ©\"** â†’ regarder **\"chat\"** (le sujet, pas \"canapÃ©\")\n\n### Analogie : La bibliothÃ¨que\n\n- **Query (Q)** : Votre question (\"Je cherche un livre sur les chats\")\n- **Key (K)** : Les mots-clÃ©s de chaque livre\n- **Value (V)** : Le contenu des livres\n\nL'attention compare votre **question** aux **mots-clÃ©s**, puis retourne un mÃ©lange pondÃ©rÃ© des **contenus** les plus pertinents.\n\n---\n\n### ğŸ“š Pour approfondir RNN/LSTM (optionnel)\n\n**VidÃ©os en franÃ§ais** :\n- [Machine Learnia - Les RNN expliquÃ©s](https://www.youtube.com/watch?v=EL439RMv3Xc) (~20 min)\n- [Science4All - Comprendre les LSTM](https://www.youtube.com/watch?v=WCUNPb-5EYI) (~15 min)\n\n**Articles en franÃ§ais** :\n- [PensÃ©e Artificielle - Introduction aux RNN](https://www.penseeartificielle.fr/comprendre-reseaux-neurones-recurrents-rnn/)\n- [DataScientest - LSTM expliquÃ© simplement](https://datascientest.com/lstm-tout-savoir)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Visualisation intuitive\n",
    "\n",
    "Avant de coder, visualisons ce que fait l'attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple simple : attention dans une phrase\n",
    "phrase = [\"Le\", \"chat\", \"mange\", \"la\", \"souris\"]\n",
    "\n",
    "# Matrice d'attention simulÃ©e (quels mots regardent quels mots ?)\n",
    "# Chaque ligne = un mot qui \"regarde\" les autres\n",
    "attention_simulee = torch.tensor([\n",
    "    [0.8, 0.1, 0.05, 0.03, 0.02],  # \"Le\" regarde surtout lui-mÃªme\n",
    "    [0.1, 0.7, 0.1, 0.05, 0.05],   # \"chat\" regarde surtout lui-mÃªme\n",
    "    [0.05, 0.4, 0.4, 0.05, 0.1],   # \"mange\" regarde \"chat\" et lui-mÃªme\n",
    "    [0.02, 0.03, 0.05, 0.8, 0.1],  # \"la\" regarde surtout lui-mÃªme\n",
    "    [0.02, 0.1, 0.2, 0.08, 0.6],   # \"souris\" regarde \"mange\" et elle-mÃªme\n",
    "])\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention_simulee, cmap='Blues')\n",
    "plt.xticks(range(5), phrase)\n",
    "plt.yticks(range(5), phrase)\n",
    "plt.xlabel(\"Mots regardÃ©s (Keys)\")\n",
    "plt.ylabel(\"Mots qui regardent (Queries)\")\n",
    "plt.title(\"Qui regarde qui ? (Matrice d'attention)\")\n",
    "plt.colorbar(label=\"Poids d'attention\")\n",
    "\n",
    "# Afficher les valeurs\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        plt.text(j, i, f'{attention_simulee[i,j]:.2f}', \n",
    "                ha='center', va='center',\n",
    "                color='white' if attention_simulee[i,j] > 0.5 else 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Dans cette matrice, quel mot le verbe \"mange\" regarde-t-il le plus ? Pourquoi est-ce logique ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. Scaled Dot-Product Attention\n\nLe **Scaled Dot-Product Attention** est l'opÃ©ration qui calcule la **matrice d'attention** (les poids \"qui regarde qui\") et produit les vecteurs enrichis en sortie.\n\n**Rappel des 3 vecteurs :**\n\n| Vecteur | RÃ´le | Sert Ã ... |\n|---------|------|-----------|\n| **Q** (Query) | Ce que je cherche | Calculer les poids (avec K) |\n| **K** (Key) | Mon identitÃ© / Ã©tiquette | Calculer les poids (avec Q) |\n| **V** (Value) | Mon contenu / l'info que je transmets | ÃŠtre rÃ©cupÃ©rÃ© selon les poids |\n\n**ConcrÃ¨tement** : La matrice d'attention dit \"Ã  quel point chaque mot m'intÃ©resse\" (calculÃ©e avec Q et K). Ensuite on rÃ©cupÃ¨re l'**information** (V) de ces mots, pondÃ©rÃ©e par ces poids.\n\n**Exemple** : Pour \"dort\" dans [\"Le\", \"chat\", \"dort\"], si les poids sont [0.26, 0.42, 0.32] :\n- On rÃ©cupÃ¨re 26% du **contenu** (V) de \"Le\"\n- On rÃ©cupÃ¨re 42% du **contenu** (V) de \"chat\"\n- On rÃ©cupÃ¨re 32% du **contenu** (V) de \"dort\"\n\n**Attention au vocabulaire** :\n- `softmax(QK^T/âˆšd_k)` = **matrice d'attention** (les poids)\n- `Attention(Q,K,V)` = matrice d'attention Ã— V = **sortie** (vecteurs enrichis)\n\n### La formule\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\nOÃ¹ :\n- $Q$ (Query) : Ce que je cherche - shape `(seq_len, d_k)`\n- $K$ (Key) : Les Ã©tiquettes de ce qui est disponible - shape `(seq_len, d_k)`\n- $V$ (Value) : Le contenu disponible - shape `(seq_len, d_v)`\n- $d_k$ : Dimension des clÃ©s (pour normaliser)\n\n> **Note** : Q, K, V sont obtenus Ã  partir des embeddings via des matrices de poids apprenables. Cela permet Ã  chaque mot d'avoir une reprÃ©sentation adaptÃ©e Ã  son rÃ´le (chercher, s'identifier, transmettre).\n\n### Exemple concret\n\nPrenons la phrase **[\"Le\", \"chat\", \"dort\"]** avec des embeddings de dimension 4.\n\nSupposons qu'aprÃ¨s transformation, on obtienne :\n\n```\n         Q (Queries)         K (Keys)           V (Values)\nLe    â†’ [0.1, 0.2, 0.1, 0.0]  [0.9, 0.1, 0.0, 0.2]  [1.0, 0.0, 0.0, 0.0]\nchat  â†’ [0.2, 0.8, 0.1, 0.3]  [0.2, 0.9, 0.2, 0.1]  [0.0, 1.0, 0.0, 0.0]\ndort  â†’ [0.3, 0.7, 0.2, 0.1]  [0.1, 0.3, 0.8, 0.1]  [0.0, 0.0, 1.0, 0.0]\n```\n\n**Calculons l'attention pour \"dort\"** (quelle info rÃ©cupÃ¨re-t-il des autres mots ?) :\n\n**Ã‰tape 1 - Scores (QÂ·Káµ€)** : On compare la Query de \"dort\" aux Keys de tous les mots\n```\nQ_dort Â· K_Le   = 0.3Ã—0.9 + 0.7Ã—0.1 + 0.2Ã—0.0 + 0.1Ã—0.2 = 0.36\nQ_dort Â· K_chat = 0.3Ã—0.2 + 0.7Ã—0.9 + 0.2Ã—0.2 + 0.1Ã—0.1 = 0.74  â† score Ã©levÃ© !\nQ_dort Â· K_dort = 0.3Ã—0.1 + 0.7Ã—0.3 + 0.2Ã—0.8 + 0.1Ã—0.1 = 0.41\n\nScores = [0.36, 0.74, 0.41]\n```\n\n**Ã‰tape 2 - Scaling (Ã·âˆšd_k)** : On divise par âˆš4 = 2\n```\nScaled = [0.18, 0.37, 0.205]\n```\n\n**Ã‰tape 3 - Softmax** : On transforme en probabilitÃ©s\n```\nPoids = [0.26, 0.42, 0.32]  (somme = 1)\n```\n\n**Ã‰tape 4 - Output (poids Ã— V)** : Moyenne pondÃ©rÃ©e des Values\n```\nOutput_dort = 0.26 Ã— V_Le + 0.42 Ã— V_chat + 0.32 Ã— V_dort\n            = 0.26 Ã— [1,0,0,0] + 0.42 Ã— [0,1,0,0] + 0.32 Ã— [0,0,1,0]\n            = [0.26, 0.42, 0.32, 0.0]\n```\n\n**InterprÃ©tation** : La nouvelle reprÃ©sentation de \"dort\" contient **42% d'info de \"chat\"** (le sujet), **32% de lui-mÃªme** (le verbe), et **26% de \"Le\"** (le dÃ©terminant). Le modÃ¨le a appris que pour comprendre un verbe, il faut surtout regarder son sujet.\n\n### DÃ©composition Ã©tape par Ã©tape\n\n1. **Scores** : $QK^T$ - Mesure la similaritÃ© entre queries et keys\n2. **Scaling** : Division par $\\sqrt{d_k}$ - Ã‰vite des valeurs trop grandes\n3. **Softmax** : Transforme en probabilitÃ©s (somme = 1)\n4. **Output** : Multiplication par $V$ - Moyenne pondÃ©rÃ©e des values\n\n### Pourquoi softmax ? Pourquoi normaliser ?\n\n**Le softmax** transforme des scores quelconques en **probabilitÃ©s** :\n\n$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n\n```\nScores bruts :  [0.36, 0.74, 0.41]  (peuvent Ãªtre nÃ©gatifs, grands, etc.)\n                        â†“ softmax\nProbabilitÃ©s :  [0.26, 0.42, 0.32]  (entre 0 et 1, somme = 1)\n```\n\n**PropriÃ©tÃ©s utiles** :\n- Toutes les valeurs sont positives et somment Ã  1 â†’ interprÃ©tables comme \"pourcentage d'attention\"\n- Amplifie les diffÃ©rences : le score le plus Ã©levÃ© \"gagne\" plus de poids\n\n**La normalisation (Ã·âˆšd_k)** Ã©vite un problÃ¨me quand la dimension est grande :\n\n```\nSans normalisation (d_k = 512) :\n  Scores QÂ·K â†’ valeurs entre -50 et +50\n  Softmax â†’ [0.0001, 0.9998, 0.0001]  â† trop \"peaked\" !\n  \nAvec normalisation (Ã·âˆš512 â‰ˆ 22.6) :\n  Scores â†’ valeurs entre -2 et +2\n  Softmax â†’ [0.20, 0.45, 0.35]  â† distribution plus douce\n```\n\nUne distribution trop \"peaked\" pose problÃ¨me : gradients trÃ¨s faibles â†’ apprentissage difficile."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1 : Calcul manuel des scores\n",
    "\n",
    "CommenÃ§ons par calculer les scores d'attention manuellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple simple avec 3 mots et dimension 4\n",
    "seq_len = 3\n",
    "d_k = 4\n",
    "\n",
    "# CrÃ©ons des Query, Key, Value alÃ©atoires\n",
    "Q = torch.randn(seq_len, d_k)\n",
    "K = torch.randn(seq_len, d_k)\n",
    "V = torch.randn(seq_len, d_k)\n",
    "\n",
    "print(\"Q (Queries):\")\n",
    "print(Q)\n",
    "print(f\"\\nShape Q: {Q.shape}\")\n",
    "print(f\"Shape K: {K.shape}\")\n",
    "print(f\"Shape V: {V.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Calculez les scores d'attention\n",
    "# ============================================\n",
    "\n",
    "# Ã‰tape 1 : Calculer QK^T (produit matriciel)\n",
    "# La transposÃ©e de K se note K.T\n",
    "\n",
    "scores = None  # TODO: Calculer QK^T\n",
    "\n",
    "print(\"Scores (QK^T):\")\n",
    "print(scores)\n",
    "print(f\"Shape: {scores.shape}\")  # Devrait Ãªtre (3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 2 : Appliquez le scaling\n",
    "# ============================================\n",
    "\n",
    "# Diviser par la racine de la dimension des vecteurs pour Ã©viter des valeurs trop grandes\n",
    "\n",
    "import math\n",
    "\n",
    "scaled_scores = None  # TODO: scores / sqrt(d_k)\n",
    "\n",
    "print(\"Scaled scores:\")\n",
    "print(scaled_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 3 : Appliquez le softmax\n",
    "# ============================================\n",
    "\n",
    "# Le softmax transforme les scores en probabilitÃ©s\n",
    "# Chaque ligne doit sommer Ã  1\n",
    "# Indice : F.softmax(tensor, dim=i) applique softmax sur la dimension i\n",
    "\n",
    "attention_weights = None  # TODO: Appliquer softmax sur scaled_scores\n",
    "\n",
    "print(\"Poids d'attention (aprÃ¨s softmax):\")\n",
    "print(attention_weights)\n",
    "print(f\"\\nVÃ©rification - Somme par ligne: {attention_weights.sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 4 : Calculez la sortie finale\n",
    "# ============================================\n",
    "\n",
    "# Multiplier les poids d'attention par V\n",
    "# C'est une moyenne pondÃ©rÃ©e des values\n",
    "\n",
    "output = None  # TODO: attention_weights @ V\n",
    "\n",
    "print(\"Output:\")\n",
    "print(output)\n",
    "print(f\"Shape: {output.shape}\")  # Devrait Ãªtre (3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. ImplÃ©mentation complÃ¨te\n",
    "\n",
    "### Exercice 5 : Fonction d'attention\n",
    "\n",
    "Maintenant, regroupez tout dans une fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Calcule le Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries, shape (seq_len, d_k) ou (batch, seq_len, d_k)\n",
    "        K: Keys, shape (seq_len, d_k) ou (batch, seq_len, d_k)\n",
    "        V: Values, shape (seq_len, d_v) ou (batch, seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: RÃ©sultat de l'attention, shape (seq_len, d_v)\n",
    "        attention_weights: Poids d'attention, shape (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # TODO: RÃ©cupÃ©rer d_k (derniÃ¨re dimension de K)\n",
    "    d_k=None\n",
    "    # TODO: ImplÃ©menter les 4 Ã©tapes\n",
    "    # 1. Calculer les scores : QK^T\n",
    "    scores = None\n",
    "    \n",
    "    # 2. Scaling : diviser par sqrt(d_k)\n",
    "    scaled_scores = None\n",
    "    \n",
    "    # 3. Softmax pour obtenir les poids\n",
    "    attention_weights = None\n",
    "    \n",
    "    # 4. Moyenne pondÃ©rÃ©e : weights @ V\n",
    "    output = None\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre fonction\n",
    "Q_test = torch.randn(4, 8)  # 4 tokens, dimension 8\n",
    "K_test = torch.randn(4, 8)\n",
    "V_test = torch.randn(4, 8)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q_test, K_test, V_test)\n",
    "\n",
    "print(f\"Structure de sortie: {output.shape}\")  # Attendu: (4, 8)\n",
    "print(f\"Structure des poids: {weights.shape}\")  # Attendu: (4, 4)\n",
    "print(f\"Somme des poids par ligne: {weights.sum(dim=1)}\")  # Attendu: [1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Pourquoi diviser par sqrt(d_k) ?\n",
    "\n",
    "C'est une question importante ! Voyons l'effet du scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison avec et sans scaling\n",
    "d_k_grand = 512  # Dimension typique dans un Transformer\n",
    "\n",
    "Q_grand = torch.randn(10, d_k_grand)\n",
    "K_grand = torch.randn(10, d_k_grand)\n",
    "\n",
    "# Scores sans scaling\n",
    "scores_sans_scaling = Q_grand @ K_grand.T\n",
    "attention_sans_scaling = F.softmax(scores_sans_scaling, dim=-1)\n",
    "\n",
    "# Scores avec scaling\n",
    "scores_avec_scaling = (Q_grand @ K_grand.T) / math.sqrt(d_k_grand)\n",
    "attention_avec_scaling = F.softmax(scores_avec_scaling, dim=-1)\n",
    "\n",
    " # Fonction pour calculer l'entropie (avec epsilon pour Ã©viter log(0))\n",
    "def entropy(p, eps=1e-9):\n",
    "  p_safe = p.clamp(min=eps)\n",
    "  return -(p * p_safe.log()).sum(dim=-1).mean()\n",
    "\n",
    "\n",
    "print(\"=== SANS SCALING ===\")\n",
    "print(f\"Scores - min: {scores_sans_scaling.min():.2f}, max: {scores_sans_scaling.max():.2f}\")\n",
    "print(f\"Attention max par ligne: {attention_sans_scaling.max(dim=-1).values}\")\n",
    "print(f\"Entropie moyenne: {entropy(attention_sans_scaling):.4f}\")\n",
    "\n",
    "print(\"\\n=== AVEC SCALING ===\")\n",
    "print(f\"Scores - min: {scores_avec_scaling.min():.2f}, max: {scores_avec_scaling.max():.2f}\")\n",
    "print(f\"Attention max par ligne: {attention_avec_scaling.max(dim=-1).values}\")\n",
    "print(f\"Entropie moyenne: {entropy(attention_avec_scaling):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Observation** : Sans scaling, le softmax devient trÃ¨s \"peaked\" (une valeur proche de 1, les autres proches de 0). Le scaling permet une distribution plus douce et des gradients plus stables.\n\n**Comment lire l'entropie ?**\n- **Entropie haute** (~2.3 pour 10 tokens) â†’ attention rÃ©partie sur plusieurs mots\n- **Entropie basse** (~0) â†’ attention concentrÃ©e sur un seul mot\n\n**Nuance importante** : Une attention concentrÃ©e n'est pas toujours mauvaise ! Par exemple, dans *\"Le chat dort, il ronfle\"*, le mot \"il\" DOIT regarder \"chat\" Ã  95%.\n\nLe problÃ¨me c'est quand l'attention est peaked **par dÃ©faut** (artefact numÃ©rique du softmax saturÃ©) plutÃ´t que **par apprentissage**. Le scaling permet au modÃ¨le de **choisir** entre attention concentrÃ©e ou distribuÃ©e selon ce qui est pertinent."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 6. Module nn.Module : Self-Attention\n\n### Self-Attention vs Cross-Attention\n\nJusqu'ici, on a manipulÃ© Q, K, V comme des tenseurs indÃ©pendants. Mais d'oÃ¹ viennent-ils ?\n\n**Self-Attention** (ce qu'on fait ici) :\n- Q, K, V sont tous calculÃ©s Ã  partir du **mÃªme** input `x`\n- Chaque mot de la phrase regarde les autres mots **de la mÃªme phrase**\n- C'est le cas dans BERT, GPT, et la plupart des Transformers\n\n```\nx (embeddings) â”€â”€â”¬â”€â”€â–º W_q â”€â”€â–º Q\n                 â”œâ”€â”€â–º W_k â”€â”€â–º K    (mÃªme source x)\n                 â””â”€â”€â–º W_v â”€â”€â–º V\n```\n\n**Cross-Attention** (on verra plus tard) :\n- Q vient d'une source, K et V d'une **autre** source\n- Exemple : en traduction, le dÃ©codeur (franÃ§ais) \"interroge\" l'encodeur (anglais)\n- UtilisÃ© dans les architectures encodeur-dÃ©codeur\n\n```\nx_decoder â”€â”€â–º W_q â”€â”€â–º Q\nx_encoder â”€â”€â”¬â”€â”€â–º W_k â”€â”€â–º K    (sources diffÃ©rentes)\n            â””â”€â”€â–º W_v â”€â”€â–º V\n```\n\n> **Dans ce TP**, on implÃ©mente la **self-attention** : la sÃ©quence \"s'attentionne elle-mÃªme\".\n\n### D'oÃ¹ viennent Q, K, V ?\n\nDans les exercices 1-5, on a utilisÃ© des tenseurs alÃ©atoires (`torch.randn`) pour comprendre le mÃ©canisme d'attention. Mais en pratique, **Q, K, V sont calculÃ©s Ã  partir des embeddings de la phrase**.\n\n**Le point clÃ©** : Un mÃªme mot a besoin de **3 reprÃ©sentations diffÃ©rentes** selon son rÃ´le :\n\n| RÃ´le | ReprÃ©sentation | Question posÃ©e |\n|------|----------------|----------------|\n| **Query** | `Q = x @ W_q` | \"Qu'est-ce que je cherche ?\" |\n| **Key** | `K = x @ W_k` | \"Comment les autres me voient ?\" |\n| **Value** | `V = x @ W_v` | \"Quelle info je transmets ?\" |\n\n**Exemple concret** :\n\n```\nPhrase : \"Le chat dort\"\n\nx = embeddings de la phrase (3 mots Ã— embed_dim)\n\nQ = x @ W_q  â†’  chaque mot \"formule sa question\"\nK = x @ W_k  â†’  chaque mot \"affiche son identitÃ©\"\nV = x @ W_v  â†’  chaque mot \"prÃ©pare son contenu Ã  transmettre\"\n```\n\n**Pourquoi 3 matrices diffÃ©rentes ?**\n\nSi on faisait simplement `Q = K = V = x`, le modÃ¨le serait limitÃ©. Les matrices W_q, W_k, W_v sont **apprises** pendant l'entraÃ®nement : le modÃ¨le dÃ©couvre quelles \"facettes\" de chaque mot sont utiles pour chaque rÃ´le.\n\n> **C'est ce qu'on implÃ©mente dans l'exercice 6** : une classe qui projette `x` vers Q, K, V, puis applique l'attention.\n\n### Exercice 6 : Classe SelfAttention en PyTorch\n\nCrÃ©ons une classe PyTorch rÃ©utilisable qui :\n1. Projette l'input `x` vers Q, K, V avec des matrices apprenables\n2. Applique la fonction `scaled_dot_product_attention` de l'exercice 5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SelfAttention(nn.Module):\n    \"\"\"\n    Module de Self-Attention.\n    \n    Projette l'input x vers Q, K, V puis applique l'attention.\n    \"\"\"\n\n    def __init__(self, embed_dim):\n        \"\"\"\n        Args:\n            embed_dim: Dimension des embeddings d'entrÃ©e\n        \"\"\"\n        super().__init__()\n        self.embed_dim = embed_dim\n\n        # TODO: CrÃ©er 3 couches linÃ©aires pour projeter vers Q, K, V\n        # Chaque couche : embed_dim -> embed_dim (utiliser nn.Linear)\n        self.W_q = None\n        self.W_k = None\n        self.W_v = None\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Embeddings, shape (batch, seq_len, embed_dim)\n\n        Returns:\n            output: RÃ©sultat de l'attention\n            attention_weights: Poids d'attention\n        \"\"\"\n        # TODO: Projeter x vers Q, K, V en utilisant les couches linÃ©aires\n        Q = None\n        K = None\n        V = None\n\n        # TODO: RÃ©utiliser la fonction scaled_dot_product_attention de l'exercice 5\n        # (elle fonctionne aussi avec des tenseurs 3D grÃ¢ce Ã  .transpose(-2, -1))\n        output, attention_weights = None, None\n\n        return output, attention_weights"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du module\n",
    "embed_dim = 32\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "attention_layer = SelfAttention(embed_dim)\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "output, weights = attention_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")  # Attendu: (2, 5, 32)\n",
    "print(f\"Weights shape: {weights.shape}\")  # Attendu: (2, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 7. Visualiser l'attention d'un vrai modÃ¨le\n\nMaintenant qu'on a compris et implÃ©mentÃ© le mÃ©canisme, regardons ce que Ã§a donne sur un modÃ¨le **rÃ©ellement entraÃ®nÃ©**.\n\nOn va utiliser **DistilBERT**, une version lÃ©gÃ¨re de BERT, pour observer les patterns d'attention appris.\n\n### Tokens spÃ©ciaux : [CLS] et [SEP]\n\nAvant de visualiser, il faut comprendre les **tokens spÃ©ciaux** ajoutÃ©s par BERT :\n\n| Token | Signification | RÃ´le |\n|-------|---------------|------|\n| **[CLS]** | \"Classification\" | AjoutÃ© au dÃ©but de chaque phrase. Son vecteur final reprÃ©sente toute la phrase (utilisÃ© pour la classification). |\n| **[SEP]** | \"Separator\" | AjoutÃ© Ã  la fin. SÃ©pare les phrases (utile quand on en a deux, ex: question/rÃ©ponse). |\n\n**Exemple** :\n```\nPhrase originale :  \"The cat sat\"\nAprÃ¨s tokenization : [CLS] The cat sat [SEP]\n```\n\n**Pourquoi [CLS] reÃ§oit beaucoup d'attention ?**\n\nLe token [CLS] est entraÃ®nÃ© pour \"rÃ©sumer\" la phrase. Il regarde tous les autres mots et les autres mots le regardent aussi â†’ c'est normal de voir des poids Ã©levÃ©s vers [CLS].\n\n> **Dans nos visualisations** : Ne vous Ã©tonnez pas si [CLS] et [SEP] ont des patterns particuliers !"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Installation de la librairie transformers\n!pip install transformers -q"
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoModel, AutoTokenizer\nimport torch\n\n# Charger un petit modÃ¨le prÃ©-entraÃ®nÃ©\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, output_attentions=True)\nmodel.eval()\n\n# Phrase de test (en anglais pour ce modÃ¨le)\nphrase = \"The cat sat on the mat because it was tired\"\n\n# Tokenizer la phrase\ninputs = tokenizer(phrase, return_tensors=\"pt\")\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n\n# Forward pass (sans calculer les gradients)\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Extraire les attentions\nattentions = outputs.attentions\n\nprint(f\"Phrase: {phrase}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Nombre de couches: {len(attentions)}\")\nprint(f\"Nombre de tÃªtes par couche: {attentions[0].shape[1]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualiser l'attention d'une tÃªte spÃ©cifique\n",
    "# Couche 5, TÃªte 2 : capture bien la corÃ©fÃ©rence \"it\" â†’ \"cat\"\n",
    "layer = 4   # Couche 5 (index 0-5)\n",
    "head = 1    # TÃªte 2 (index 0-11)\n",
    "\n",
    "attention_matrix = attentions[layer][0, head].numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_matrix, cmap='Blues')\n",
    "plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "plt.yticks(range(len(tokens)), tokens)\n",
    "plt.xlabel(\"Tokens regardÃ©s (Keys)\")\n",
    "plt.ylabel(\"Tokens qui regardent (Queries)\")\n",
    "plt.title(f\"Attention rÃ©elle - Couche {layer+1}, TÃªte {head+1}\")\n",
    "plt.colorbar(label=\"Poids d'attention\")\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens)):\n",
    "        val = attention_matrix[i, j]\n",
    "        plt.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                color='white' if val > 0.3 else 'black', fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Visualiser l'attention d'une tÃªte spÃ©cifique\nlayer = 0   # PremiÃ¨re couche (0 Ã  5)\nhead = 0    # PremiÃ¨re tÃªte (0 Ã  11)\n\nattention_matrix = attentions[layer][0, head].numpy()\n\nplt.figure(figsize=(10, 8))\nplt.imshow(attention_matrix, cmap='Blues')\nplt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\nplt.yticks(range(len(tokens)), tokens)\nplt.xlabel(\"Tokens regardÃ©s (Keys)\")\nplt.ylabel(\"Tokens qui regardent (Queries)\")\nplt.title(f\"Attention rÃ©elle - Couche {layer+1}, TÃªte {head+1}\")\nplt.colorbar(label=\"Poids d'attention\")\n\nfor i in range(len(tokens)):\n    for j in range(len(tokens)):\n        val = attention_matrix[i, j]\n        plt.text(j, i, f'{val:.2f}', ha='center', va='center',\n                color='white' if val > 0.3 else 'black', fontsize=7)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Question : Que regarde le pronom \"it\" ?\n# Trouvons son index et regardons sa ligne d'attention\n\nit_index = tokens.index(\"it\")\nprint(f\"'it' est Ã  l'index {it_index}\")\nprint(f\"\\nAttention de 'it' vers les autres tokens (couche {layer+1}, tÃªte {head+1}):\")\nprint(\"-\" * 40)\n\nfor i, (token, weight) in enumerate(zip(tokens, attention_matrix[it_index])):\n    bar = \"â–ˆ\" * int(weight * 30)\n    marker = \" â† ?\" if token in [\"cat\", \"mat\"] else \"\"\n    print(f\"  {token:10} {weight:.2f} {bar}{marker}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Trouver les tÃªtes oÃ¹ \"it\" regarde le plus \"cat\"\nit_index = tokens.index(\"it\")\ncat_index = tokens.index(\"cat\")\n\nprint(\"Top 5 des tÃªtes oÃ¹ 'it' regarde le plus 'cat' :\\n\")\n\nresults = []\nfor layer_idx in range(len(attentions)):\n    for head_idx in range(attentions[layer_idx].shape[1]):\n        attention_matrix = attentions[layer_idx][0, head_idx].numpy()\n        attention_to_cat = attention_matrix[it_index, cat_index]\n        results.append((layer_idx + 1, head_idx + 1, attention_to_cat))\n\n# Trier par attention dÃ©croissante\nresults.sort(key=lambda x: x[2], reverse=True)\n\nprint(f\"{'Couche':<8} {'TÃªte':<8} {'Attention â†’ cat'}\")\nprint(\"-\" * 30)\nfor layer, head, attn in results[:5]:\n    print(f\"{layer:<8} {head:<8} {attn:.3f}\")\n\n# Visualiser la meilleure tÃªte\nbest_layer, best_head, _ = results[0]\nprint(f\"\\nâ†’ Visualisons la couche {best_layer}, tÃªte {best_head} :\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Trouver les tÃªtes oÃ¹ \"it\" regarde le plus \"cat\"\nprint(\"Top 5 des tÃªtes oÃ¹ 'it' regarde le plus 'cat' :\\n\")\n\nresults = []\nfor layer_idx in range(len(attentions)):\n    for head_idx in range(attentions[layer_idx].shape[1]):\n        attention_matrix = attentions[layer_idx][0, head_idx].numpy()\n        attention_to_cat = attention_matrix[it_index, cat_index]\n        results.append((layer_idx + 1, head_idx + 1, attention_to_cat))\n\n# Trier par attention dÃ©croissante\nresults.sort(key=lambda x: x[2], reverse=True)\n\nprint(f\"{'Couche':<8} {'TÃªte':<8} {'Attention â†’ cat'}\")\nprint(\"-\" * 30)\nfor layer, head, attn in results[:5]:\n    print(f\"{layer:<8} {head:<8} {attn:.3f}\")\n\n# Visualiser la meilleure tÃªte\nbest_layer, best_head, _ = results[0]\nprint(f\"\\nâ†’ Visualisons la couche {best_layer}, tÃªte {best_head} :\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualiser l'attention de \"it\" sur TOUTES les couches (moyenne des tÃªtes)\nit_index = tokens.index(\"it\")\ncat_index = tokens.index(\"cat\")\nmat_index = tokens.index(\"mat\")\n\nprint(f\"Attention de 'it' vers 'cat' et 'mat' par couche :\\n\")\nprint(f\"{'Couche':<10} {'â†’ cat':<12} {'â†’ mat':<12} {'Meilleur ?'}\")\nprint(\"-\" * 45)\n\nfor layer_idx in range(len(attentions)):\n    # Moyenne sur toutes les tÃªtes de cette couche\n    layer_attention = attentions[layer_idx][0].mean(dim=0).numpy()  # (seq_len, seq_len)\n    \n    attention_to_cat = layer_attention[it_index, cat_index]\n    attention_to_mat = layer_attention[it_index, mat_index]\n    \n    winner = \"â† cat âœ“\" if attention_to_cat > attention_to_mat else \"â† mat\"\n    print(f\"Couche {layer_idx + 1:<3} {attention_to_cat:.3f}        {attention_to_mat:.3f}        {winner}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Explorer plusieurs couches et tÃªtes\n\nL'attention de \"it\" peut sembler contre-intuitive sur une seule tÃªte. Mais DistilBERT a **6 couches Ã— 12 tÃªtes = 72 patterns d'attention** diffÃ©rents !\n\nCertaines tÃªtes capturent :\n- Les relations syntaxiques (sujet-verbe)\n- Les corÃ©fÃ©rences (pronom â†’ antÃ©cÃ©dent)\n- Les dÃ©pendances locales (mots adjacents)\n- Les patterns positionnels (regarder [CLS] ou [SEP])\n\nExplorons plusieurs couches pour trouver oÃ¹ se cache la corÃ©fÃ©rence \"it\" â†’ \"cat\".",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "**Questions d'analyse** :\n\n1. Le pronom \"it\" regarde-t-il principalement \"cat\" ou \"mat\" ? Pourquoi est-ce logique grammaticalement ?\n\n2. Changez `layer` et `head` dans les cellules ci-dessus. Que remarquez-vous ? (Indice : diffÃ©rentes tÃªtes capturent diffÃ©rentes relations)\n\n3. Essayez d'autres phrases, par exemple :\n   - `\"The dog chased the cat because it was fast\"` (qui est \"it\" ici ?)\n   - `\"The trophy didn't fit in the suitcase because it was too big\"` (cas ambigu !)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8. RÃ©capitulatif\n\n### Ce que nous avons appris\n\n1. **L'attention** permet Ã  chaque Ã©lÃ©ment de \"regarder\" tous les autres\n2. **Q, K, V** : Query (ce que je cherche), Key (les Ã©tiquettes), Value (le contenu)\n3. **Formule** : $\\text{softmax}(QK^T / \\sqrt{d_k}) \\cdot V$\n4. **Scaling** : Essentiel pour la stabilitÃ© des gradients\n\n### Points clÃ©s\n\n| Concept | RÃ´le |\n|---------|------|\n| Dot product $QK^T$ | Mesure la similaritÃ© |\n| Softmax | Transforme en probabilitÃ©s |\n| Scaling $\\sqrt{d_k}$ | Stabilise les gradients |\n| Self-attention | Q = K = V (chaque mot regarde tous les autres) |\n\n### Prochaine session\n\nNous verrons le **Multi-Head Attention** : plusieurs \"tÃªtes\" d'attention qui regardent sous diffÃ©rents angles."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 9. Pour aller plus loin (optionnel)\n\n### Comment entraÃ®ne-t-on un Transformer ?\n\nIl existe deux grandes approches selon l'usage du modÃ¨le :\n\n### Approche 1 : PrÃ©dire le mot suivant (GPT)\n\nPour les modÃ¨les **gÃ©nÃ©ratifs** (GPT, LLaMA, etc.), on entraÃ®ne le modÃ¨le Ã  prÃ©dire le prochain mot.\n\n**Objectif** : EntraÃ®ner efficacement sur des phrases entiÃ¨res en un seul forward pass.\n\n```\nPhrase : \"Le chat dort sur\"\n\nSans masque (inefficace) :\n  Forward 1 : \"Le\"           â†’ apprend Ã  prÃ©dire \"chat\"\n  Forward 2 : \"Le chat\"      â†’ apprend Ã  prÃ©dire \"dort\"\n  Forward 3 : \"Le chat dort\" â†’ apprend Ã  prÃ©dire \"sur\"\n  â†’ 3 forward passes pour une phrase !\n\nAvec masque causal (efficace) :\n  Forward unique : \"Le chat dort sur\"\n    Position 1 (voit \"Le\")           â†’ apprend Ã  prÃ©dire \"chat\"\n    Position 2 (voit \"Le chat\")      â†’ apprend Ã  prÃ©dire \"dort\"\n    Position 3 (voit \"Le chat dort\") â†’ apprend Ã  prÃ©dire \"sur\"\n  â†’ 1 seul forward pass, tout en parallÃ¨le !\n```\n\n**Le masque causal** permet Ã  chaque position de ne voir que les mots prÃ©cÃ©dents :\n\n```\n              Le   chat  dort  sur\n      Le    [  âœ“     âœ—     âœ—    âœ—  ]\n     chat   [  âœ“     âœ“     âœ—    âœ—  ]\n     dort   [  âœ“     âœ“     âœ“    âœ—  ]\n      sur   [  âœ“     âœ“     âœ“    âœ“  ]\n```\n\n**ImplÃ©mentation** : On met `-âˆ` aux positions masquÃ©es â†’ `softmax(-âˆ) = 0`\n\n### Approche 2 : Remplir les trous (BERT)\n\nPour les modÃ¨les de **comprÃ©hension** (BERT, RoBERTa, etc.) :\n\n```\nEntrÃ©e :    \"Le [MASK] dort sur le [MASK]\"\nObjectif :   PrÃ©dire \"chat\" et \"canapÃ©\"\n```\n\nLe modÃ¨le peut voir tout le contexte (gauche ET droite) pour deviner les mots masquÃ©s â†’ pas besoin de masque causal.\n\n### Comparaison\n\n| | GPT (gÃ©nÃ©ratif) | BERT (comprÃ©hension) |\n|--|-----------------|---------------------|\n| **EntraÃ®nement** | PrÃ©dire le mot suivant | PrÃ©dire les mots masquÃ©s |\n| **Contexte** | PassÃ© uniquement | Tout (bidirectionnel) |\n| **Masque causal** | âœ… Oui | âŒ Non |\n| **Usage** | GÃ©nÃ©ration de texte | Classification, QA, NER |\n\n### Exercice bonus : ImplÃ©menter le masque causal"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_with_mask(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Attention avec masking optionnel.\n",
    "    \n",
    "    Args:\n",
    "        Q, K, V: Query, Key, Value\n",
    "        mask: Tensor boolÃ©en, True = position Ã  masquer\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    "    \n",
    "    # Appliquer le masque (mettre -inf pour les positions masquÃ©es)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# CrÃ©er un masque causal (triangulaire)\n",
    "seq_len = 5\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "print(\"Masque causal (True = masquÃ©):\")\n",
    "print(causal_mask.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec masque\n",
    "Q = torch.randn(seq_len, 8)\n",
    "K = torch.randn(seq_len, 8)\n",
    "V = torch.randn(seq_len, 8)\n",
    "\n",
    "output_masked, weights_masked = scaled_dot_product_attention_with_mask(Q, K, V, causal_mask)\n",
    "\n",
    "print(\"Poids d'attention avec masque causal:\")\n",
    "print(weights_masked.round(decimals=2))\n",
    "print(\"\\nObservation: chaque ligne ne peut voir que les positions prÃ©cÃ©dentes (et elle-mÃªme)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
