{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 01 - Le Mécanisme d'Attention\n",
    "\n",
    "**Module** : Réseaux de Neurones Approfondissement  \n",
    "**Durée** : 2h  \n",
    "**Objectif** : Comprendre et implémenter le mécanisme d'attention, brique fondamentale des Transformers\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs pédagogiques\n",
    "\n",
    "À la fin de ce TP, vous serez capable de :\n",
    "1. Expliquer intuitivement ce qu'est l'attention\n",
    "2. Comprendre les concepts de Query, Key, Value\n",
    "3. Implémenter le Scaled Dot-Product Attention\n",
    "4. Visualiser et interpréter les poids d'attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports\n",
    "\n",
    "Exécutez cette cellule pour installer les dépendances nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances (Google Colab)\n",
    "!pip install torch matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction : Pourquoi l'attention ?\n",
    "\n",
    "### Le problème des RNN\n",
    "\n",
    "Les réseaux récurrents (RNN, LSTM) traitent les séquences **mot par mot**, ce qui pose deux problèmes :\n",
    "1. **Dépendances longues** : difficile de relier des mots éloignés\n",
    "2. **Pas de parallélisation** : calcul séquentiel = lent\n",
    "\n",
    "### L'idée de l'attention\n",
    "\n",
    "L'attention permet à chaque élément d'une séquence de \"regarder\" tous les autres éléments et de décider lesquels sont importants.\n",
    "\n",
    "**Exemple** : Dans la phrase *\"Le chat qui était sur le toit a sauté\"*, pour comprendre *\"a sauté\"*, le modèle doit se concentrer sur *\"chat\"* (le sujet) plutôt que sur *\"toit\"*.\n",
    "\n",
    "### Analogie : La recherche dans une base de données\n",
    "\n",
    "Imaginez une bibliothèque :\n",
    "- **Query (Q)** : Votre question (\"Je cherche un livre sur les chats\")\n",
    "- **Key (K)** : Les mots-clés de chaque livre\n",
    "- **Value (V)** : Le contenu des livres\n",
    "\n",
    "L'attention compare votre **question** aux **mots-clés**, puis retourne un mélange pondéré des **contenus** les plus pertinents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Visualisation intuitive\n",
    "\n",
    "Avant de coder, visualisons ce que fait l'attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple simple : attention dans une phrase\n",
    "phrase = [\"Le\", \"chat\", \"mange\", \"la\", \"souris\"]\n",
    "\n",
    "# Matrice d'attention simulée (quels mots regardent quels mots ?)\n",
    "# Chaque ligne = un mot qui \"regarde\" les autres\n",
    "attention_simulee = torch.tensor([\n",
    "    [0.8, 0.1, 0.05, 0.03, 0.02],  # \"Le\" regarde surtout lui-même\n",
    "    [0.1, 0.7, 0.1, 0.05, 0.05],   # \"chat\" regarde surtout lui-même\n",
    "    [0.05, 0.4, 0.4, 0.05, 0.1],   # \"mange\" regarde \"chat\" et lui-même\n",
    "    [0.02, 0.03, 0.05, 0.8, 0.1],  # \"la\" regarde surtout lui-même\n",
    "    [0.02, 0.1, 0.2, 0.08, 0.6],   # \"souris\" regarde \"mange\" et elle-même\n",
    "])\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention_simulee, cmap='Blues')\n",
    "plt.xticks(range(5), phrase)\n",
    "plt.yticks(range(5), phrase)\n",
    "plt.xlabel(\"Mots regardés (Keys)\")\n",
    "plt.ylabel(\"Mots qui regardent (Queries)\")\n",
    "plt.title(\"Qui regarde qui ? (Matrice d'attention)\")\n",
    "plt.colorbar(label=\"Poids d'attention\")\n",
    "\n",
    "# Afficher les valeurs\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        plt.text(j, i, f'{attention_simulee[i,j]:.2f}', \n",
    "                ha='center', va='center',\n",
    "                color='white' if attention_simulee[i,j] > 0.5 else 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Dans cette matrice, quel mot le verbe \"mange\" regarde-t-il le plus ? Pourquoi est-ce logique ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Scaled Dot-Product Attention\n",
    "\n",
    "### La formule\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Où :\n",
    "- $Q$ (Query) : Ce que je cherche - shape `(seq_len, d_k)`\n",
    "- $K$ (Key) : Les étiquettes de ce qui est disponible - shape `(seq_len, d_k)`\n",
    "- $V$ (Value) : Le contenu disponible - shape `(seq_len, d_v)`\n",
    "- $d_k$ : Dimension des clés (pour normaliser)\n",
    "\n",
    "### Décomposition étape par étape\n",
    "\n",
    "1. **Scores** : $QK^T$ - Mesure la similarité entre queries et keys\n",
    "2. **Scaling** : Division par $\\sqrt{d_k}$ - Évite des valeurs trop grandes\n",
    "3. **Softmax** : Transforme en probabilités (somme = 1)\n",
    "4. **Output** : Multiplication par $V$ - Moyenne pondérée des values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1 : Calcul manuel des scores\n",
    "\n",
    "Commençons par calculer les scores d'attention manuellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple simple avec 3 mots et dimension 4\n",
    "seq_len = 3\n",
    "d_k = 4\n",
    "\n",
    "# Créons des Query, Key, Value aléatoires\n",
    "Q = torch.randn(seq_len, d_k)\n",
    "K = torch.randn(seq_len, d_k)\n",
    "V = torch.randn(seq_len, d_k)\n",
    "\n",
    "print(\"Q (Queries):\")\n",
    "print(Q)\n",
    "print(f\"\\nShape Q: {Q.shape}\")\n",
    "print(f\"Shape K: {K.shape}\")\n",
    "print(f\"Shape V: {V.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Calculez les scores d'attention\n",
    "# ============================================\n",
    "\n",
    "# Étape 1 : Calculer Q @ K^T (produit matriciel)\n",
    "# La transposée de K se note K.T ou K.transpose(-2, -1)\n",
    "\n",
    "scores = None  # TODO: Calculer Q @ K^T\n",
    "\n",
    "print(\"Scores (Q @ K^T):\")\n",
    "print(scores)\n",
    "print(f\"Shape: {scores.shape}\")  # Devrait être (3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 2 : Appliquez le scaling\n",
    "# ============================================\n",
    "\n",
    "# Diviser par sqrt(d_k) pour éviter des valeurs trop grandes\n",
    "# Indice : utilisez d_k ** 0.5 ou torch.sqrt(torch.tensor(d_k))\n",
    "\n",
    "import math\n",
    "\n",
    "scaled_scores = None  # TODO: scores / sqrt(d_k)\n",
    "\n",
    "print(\"Scaled scores:\")\n",
    "print(scaled_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 3 : Appliquez le softmax\n",
    "# ============================================\n",
    "\n",
    "# Le softmax transforme les scores en probabilités\n",
    "# Chaque ligne doit sommer à 1\n",
    "# Indice : F.softmax(tensor, dim=-1) applique softmax sur la dernière dimension\n",
    "\n",
    "attention_weights = None  # TODO: Appliquer softmax sur scaled_scores\n",
    "\n",
    "print(\"Poids d'attention (après softmax):\")\n",
    "print(attention_weights)\n",
    "print(f\"\\nVérification - Somme par ligne: {attention_weights.sum(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 4 : Calculez la sortie finale\n",
    "# ============================================\n",
    "\n",
    "# Multiplier les poids d'attention par V\n",
    "# C'est une moyenne pondérée des values\n",
    "\n",
    "output = None  # TODO: attention_weights @ V\n",
    "\n",
    "print(\"Output:\")\n",
    "print(output)\n",
    "print(f\"Shape: {output.shape}\")  # Devrait être (3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implémentation complète\n",
    "\n",
    "### Exercice 5 : Fonction d'attention\n",
    "\n",
    "Maintenant, regroupez tout dans une fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Calcule le Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries, shape (seq_len, d_k) ou (batch, seq_len, d_k)\n",
    "        K: Keys, shape (seq_len, d_k) ou (batch, seq_len, d_k)\n",
    "        V: Values, shape (seq_len, d_v) ou (batch, seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: Résultat de l'attention, shape (seq_len, d_v)\n",
    "        attention_weights: Poids d'attention, shape (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # Récupérer d_k (dernière dimension de K)\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # TODO: Implémenter les 4 étapes\n",
    "    # 1. Calculer les scores : Q @ K^T\n",
    "    scores = None\n",
    "    \n",
    "    # 2. Scaling : diviser par sqrt(d_k)\n",
    "    scaled_scores = None\n",
    "    \n",
    "    # 3. Softmax pour obtenir les poids\n",
    "    attention_weights = None\n",
    "    \n",
    "    # 4. Moyenne pondérée : weights @ V\n",
    "    output = None\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre fonction\n",
    "Q_test = torch.randn(4, 8)  # 4 tokens, dimension 8\n",
    "K_test = torch.randn(4, 8)\n",
    "V_test = torch.randn(4, 8)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q_test, K_test, V_test)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")  # Attendu: (4, 8)\n",
    "print(f\"Weights shape: {weights.shape}\")  # Attendu: (4, 4)\n",
    "print(f\"Weights sum per row: {weights.sum(dim=-1)}\")  # Attendu: [1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Pourquoi diviser par sqrt(d_k) ?\n",
    "\n",
    "C'est une question importante ! Voyons l'effet du scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison avec et sans scaling\n",
    "d_k_grand = 512  # Dimension typique dans un Transformer\n",
    "\n",
    "Q_grand = torch.randn(10, d_k_grand)\n",
    "K_grand = torch.randn(10, d_k_grand)\n",
    "\n",
    "# Scores sans scaling\n",
    "scores_sans_scaling = Q_grand @ K_grand.T\n",
    "attention_sans_scaling = F.softmax(scores_sans_scaling, dim=-1)\n",
    "\n",
    "# Scores avec scaling\n",
    "scores_avec_scaling = (Q_grand @ K_grand.T) / math.sqrt(d_k_grand)\n",
    "attention_avec_scaling = F.softmax(scores_avec_scaling, dim=-1)\n",
    "\n",
    "print(\"=== SANS SCALING ===\")\n",
    "print(f\"Scores - min: {scores_sans_scaling.min():.2f}, max: {scores_sans_scaling.max():.2f}\")\n",
    "print(f\"Attention max par ligne: {attention_sans_scaling.max(dim=-1).values[:3]}\")\n",
    "print(f\"Entropie moyenne: {-(attention_sans_scaling * attention_sans_scaling.log()).sum(dim=-1).mean():.4f}\")\n",
    "\n",
    "print(\"\\n=== AVEC SCALING ===\")\n",
    "print(f\"Scores - min: {scores_avec_scaling.min():.2f}, max: {scores_avec_scaling.max():.2f}\")\n",
    "print(f\"Attention max par ligne: {attention_avec_scaling.max(dim=-1).values[:3]}\")\n",
    "print(f\"Entropie moyenne: {-(attention_avec_scaling * attention_avec_scaling.log()).sum(dim=-1).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Sans scaling, le softmax devient très \"peaked\" (une valeur proche de 1, les autres proches de 0). Le scaling permet une distribution plus douce et des gradients plus stables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Application : Self-Attention sur une phrase\n",
    "\n",
    "Appliquons l'attention à une vraie phrase et visualisons les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase exemple\n",
    "phrase = [\"Le\", \"chat\", \"noir\", \"dort\", \"sur\", \"le\", \"canapé\"]\n",
    "seq_len = len(phrase)\n",
    "embed_dim = 16  # Dimension des embeddings\n",
    "\n",
    "# Simulons des embeddings (en vrai, ils seraient appris)\n",
    "torch.manual_seed(42)\n",
    "embeddings = torch.randn(seq_len, embed_dim)\n",
    "\n",
    "print(f\"Phrase: {phrase}\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En self-attention, Q = K = V = embeddings\n",
    "# (chaque mot se compare à tous les autres)\n",
    "\n",
    "output, attention_weights = scaled_dot_product_attention(\n",
    "    Q=embeddings,\n",
    "    K=embeddings,\n",
    "    V=embeddings\n",
    ")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_weights.detach().numpy(), cmap='Blues')\n",
    "plt.xticks(range(seq_len), phrase, rotation=45)\n",
    "plt.yticks(range(seq_len), phrase)\n",
    "plt.xlabel(\"Mots regardés (Keys)\")\n",
    "plt.ylabel(\"Mots qui regardent (Queries)\")\n",
    "plt.title(\"Self-Attention : Qui regarde qui ?\")\n",
    "plt.colorbar(label=\"Poids d'attention\")\n",
    "\n",
    "# Afficher les valeurs\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        val = attention_weights[i, j].item()\n",
    "        plt.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                color='white' if val > 0.3 else 'black', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Module nn.Module\n",
    "\n",
    "### Exercice 6 : Classe Attention en PyTorch\n",
    "\n",
    "Créons une classe PyTorch réutilisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Module de Self-Attention.\n",
    "    \n",
    "    En self-attention, on projette les embeddings en Q, K, V\n",
    "    avec des matrices de poids apprenables.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: Dimension des embeddings d'entrée\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # TODO: Créer 3 couches linéaires pour projeter vers Q, K, V\n",
    "        # Chaque couche : embed_dim -> embed_dim\n",
    "        self.W_q = None  # nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = None  # nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = None  # nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Embeddings, shape (batch, seq_len, embed_dim)\n",
    "        \n",
    "        Returns:\n",
    "            output: Résultat de l'attention\n",
    "            attention_weights: Poids d'attention\n",
    "        \"\"\"\n",
    "        # TODO: Projeter x vers Q, K, V\n",
    "        Q = None  # self.W_q(x)\n",
    "        K = None  # self.W_k(x)\n",
    "        V = None  # self.W_v(x)\n",
    "        \n",
    "        # TODO: Appliquer l'attention\n",
    "        # Attention: pour les batches, K.transpose(-2, -1) au lieu de K.T\n",
    "        d_k = self.embed_dim\n",
    "        \n",
    "        scores = None  # Q @ K.transpose(-2, -1)\n",
    "        scaled_scores = None  # scores / sqrt(d_k)\n",
    "        attention_weights = None  # softmax\n",
    "        output = None  # attention_weights @ V\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du module\n",
    "embed_dim = 32\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "attention_layer = SelfAttention(embed_dim)\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "output, weights = attention_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")  # Attendu: (2, 5, 32)\n",
    "print(f\"Weights shape: {weights.shape}\")  # Attendu: (2, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Exercice de synthèse\n",
    "\n",
    "Analysez les poids d'attention appris sur un exemple concret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase de test\n",
    "phrase_test = [\"Le\", \"programmeur\", \"écrit\", \"du\", \"code\", \"Python\"]\n",
    "seq_len = len(phrase_test)\n",
    "\n",
    "# Embeddings simulés\n",
    "torch.manual_seed(123)\n",
    "x = torch.randn(1, seq_len, embed_dim)  # Batch size = 1\n",
    "\n",
    "# Appliquer l'attention\n",
    "attention_layer = SelfAttention(embed_dim)\n",
    "output, weights = attention_layer(x)\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(weights[0].detach().numpy(), cmap='Blues')\n",
    "plt.xticks(range(seq_len), phrase_test, rotation=45)\n",
    "plt.yticks(range(seq_len), phrase_test)\n",
    "plt.xlabel(\"Keys\")\n",
    "plt.ylabel(\"Queries\")\n",
    "plt.title(\"Self-Attention (poids aléatoires non entraînés)\")\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        val = weights[0, i, j].item()\n",
    "        plt.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                color='white' if val > 0.3 else 'black', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Récapitulatif\n",
    "\n",
    "### Ce que nous avons appris\n",
    "\n",
    "1. **L'attention** permet à chaque élément de \"regarder\" tous les autres\n",
    "2. **Q, K, V** : Query (ce que je cherche), Key (les étiquettes), Value (le contenu)\n",
    "3. **Formule** : $\\text{softmax}(QK^T / \\sqrt{d_k}) \\cdot V$\n",
    "4. **Scaling** : Essentiel pour la stabilité des gradients\n",
    "\n",
    "### Points clés\n",
    "\n",
    "| Concept | Rôle |\n",
    "|---------|------|\n",
    "| Dot product $QK^T$ | Mesure la similarité |\n",
    "| Softmax | Transforme en probabilités |\n",
    "| Scaling $\\sqrt{d_k}$ | Stabilise les gradients |\n",
    "| Self-attention | Q = K = V (chaque mot regarde tous les autres) |\n",
    "\n",
    "### Prochaine session\n",
    "\n",
    "Nous verrons le **Multi-Head Attention** : plusieurs \"têtes\" d'attention qui regardent sous différents angles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Pour aller plus loin (optionnel)\n",
    "\n",
    "### Exercice bonus : Masking\n",
    "\n",
    "Dans certains cas (génération de texte), on veut empêcher un mot de \"voir\" les mots futurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_with_mask(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Attention avec masking optionnel.\n",
    "    \n",
    "    Args:\n",
    "        Q, K, V: Query, Key, Value\n",
    "        mask: Tensor booléen, True = position à masquer\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    "    \n",
    "    # Appliquer le masque (mettre -inf pour les positions masquées)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Créer un masque causal (triangulaire)\n",
    "seq_len = 5\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "print(\"Masque causal (True = masqué):\")\n",
    "print(causal_mask.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec masque\n",
    "Q = torch.randn(seq_len, 8)\n",
    "K = torch.randn(seq_len, 8)\n",
    "V = torch.randn(seq_len, 8)\n",
    "\n",
    "output_masked, weights_masked = scaled_dot_product_attention_with_mask(Q, K, V, causal_mask)\n",
    "\n",
    "print(\"Poids d'attention avec masque causal:\")\n",
    "print(weights_masked.round(decimals=2))\n",
    "print(\"\\nObservation: chaque ligne ne peut voir que les positions précédentes (et elle-même)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
