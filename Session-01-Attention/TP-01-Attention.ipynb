{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 01 - Le M√©canisme d'Attention\n",
    "\n",
    "**Module** : R√©seaux de Neurones Approfondissement  \n",
    "**Dur√©e** : 2h  \n",
    "**Objectif** : Comprendre et impl√©menter le m√©canisme d'attention, brique fondamentale des Transformers\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs p√©dagogiques\n",
    "\n",
    "√Ä la fin de ce TP, vous serez capable de :\n",
    "1. Expliquer intuitivement ce qu'est l'attention\n",
    "2. Comprendre les concepts de Query, Key, Value\n",
    "3. Impl√©menter le Scaled Dot-Product Attention\n",
    "4. Visualiser et interpr√©ter les poids d'attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports\n",
    "\n",
    "Ex√©cutez cette cellule pour installer les d√©pendances n√©cessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab)\n",
    "!pip install torch matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 1. Introduction : Pourquoi l'attention ?\n\n> **Note p√©dagogique** : Dans les sessions 1 √† 3, on se concentre sur le **fonctionnement** de l'architecture (inf√©rence/forward pass). L'**entra√Ænement** (backpropagation, optimisation) sera abord√© en session 4.\n\n### 1.1 Les architectures s√©quentielles (RNN / LSTM)\n\nLes **r√©seaux r√©currents (RNN)** traitent les s√©quences **mot par mot** :\n\n```\n        ‚îå‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îê\n  x‚ÇÅ ‚îÄ‚îÄ‚ñ∂‚îÇ h ‚îú‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ h ‚îú‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ h ‚îú‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ h ‚îú‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ h ‚îú‚îÄ‚îÄ‚ñ∂ sortie\n  Le    ‚îî‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îò\n           ‚îÇ        ‚îÇ        ‚îÇ        ‚îÇ        ‚îÇ\n          x‚ÇÇ       x‚ÇÉ       x‚ÇÑ       x‚ÇÖ       x‚ÇÜ\n         chat     dort      sur      le     canap√©\n```\n\n**Probl√®me** : L'information passe de cellule en cellule. Pour relier \"Le chat\" √† \"canap√©\", il faut traverser toute la cha√Æne ‚Üí l'info se d√©grade (gradient √©vanescent).\n\nLes **LSTM** ajoutent des \"portes\" pour mieux contr√¥ler la m√©moire :\n\n```\n                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                    ‚îÇ      CELLULE LSTM       ‚îÇ\n        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n        ‚îÇ  Porte    ‚îÇ  Porte    ‚îÇ    Porte    ‚îÇ\n        ‚îÇ  Oubli    ‚îÇ  Entr√©e   ‚îÇ   Sortie    ‚îÇ\n        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n              ‚îÇ           ‚îÇ            ‚îÇ\n          Effacer?    Ajouter?    Utiliser?\n```\n\n**Am√©lioration** : Les LSTM retiennent mieux les infos longue distance.\n**Mais** : Toujours s√©quentiel (lent) et limit√© sur les tr√®s longues s√©quences.\n\n### 1.2 L'architecture Transformer\n\nLe **Transformer** (2017) abandonne la r√©currence. Chaque mot peut regarder **tous les autres directement** :\n\n```\nEntr√©e: \"Le chat dort sur le canap√©\" (6 tokens)\n         ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ\n         ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ          EMBEDDINGS (6 vecteurs)            ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ\n         ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ             SELF-ATTENTION                  ‚îÇ\n‚îÇ   Chaque vecteur regarde les 5 autres       ‚îÇ\n‚îÇ   ‚Üí Enrichit chaque mot avec le CONTEXTE    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ\n         ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº\n       (6 vecteurs enrichis)\n         ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ\n         ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ        FEED-FORWARD (par position)          ‚îÇ\n‚îÇ   Exploite le contexte enrichi              ‚îÇ\n‚îÇ   (comme un r√©seau de neurones classique)   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ\n         ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº\n      Sortie: 6 vecteurs transform√©s\n```\n\n**Points cl√©s** :\n- **Entr√©e = Sortie** : Si tu entres 6 mots ‚Üí tu obtiens 6 vecteurs enrichis\n- **Taille variable** : Tu peux entrer 5, 50, ou 500 mots (jusqu'√† une limite : 512 pour BERT, 128K pour GPT-4)\n- **Self-Attention** : Donne du contexte √† chaque mot\n- **Feed-Forward** : Exploite ce contexte (transformation non-lin√©aire)\n\n**Que sort le Transformer ?**\n\nLe Transformer produit des **vecteurs enrichis** (repr√©sentations). Une couche de sortie (ajout√©e selon la t√¢che) les transforme en r√©sultat :\n- **Classification** ‚Üí probabilit√© par classe (ex: 70% positif, 30% n√©gatif)\n- **G√©n√©ration** ‚Üí probabilit√© du prochain mot\n- **Traduction** ‚Üí phrase dans l'autre langue\n\n### Comment les mots entrent dans le Transformer ?\n\nChaque mot passe par **deux √©tapes** avant d'entrer :\n\n```\nMot \"chat\" (position 1)\n        ‚îÇ\n        ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Token Embedding (fixe pour chaque token)        ‚îÇ\n‚îÇ \"chat\" ‚Üí [0.8, 0.1, 0.3, ...]                   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n        ‚îÇ\n        + (addition)\n        ‚îÇ\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Positional Encoding (fixe pour chaque position) ‚îÇ\n‚îÇ position 1 ‚Üí [0.0, 0.1, 0.0, ...]               ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n        ‚îÇ\n        ‚ñº\nVecteur d'entr√©e = [0.8, 0.2, 0.3, ...]\n```\n\n**Deux composants distincts, tous deux fig√©s apr√®s entra√Ænement :**\n\n| Composant | Taille | R√¥le |\n|-----------|--------|------|\n| Token embeddings | ~50k √ó dim | \"Qui suis-je ?\" (sens du mot) |\n| Positional encodings | max_len √ó dim | \"O√π suis-je ?\" (position dans la phrase) |\n\n**Pourquoi c'est important ?** Sans le positional encoding, le mod√®le ne distinguerait pas :\n- *\"Le chat mange la souris\"*\n- *\"La souris mange le chat\"*\n\n(M√™mes tokens, ordre diff√©rent ‚Üí sens oppos√© !)\n\n### 1.3 Empilement des blocs\n\nCes blocs (Attention + FFN) sont **empil√©s** : la sortie de l'un devient l'entr√©e du suivant.\n\n```\nEntr√©e (6 vecteurs)\n         ‚îÇ\n         ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Attention 1   ‚îÇ\n‚îÇ        ‚Üì        ‚îÇ  Bloc 1\n‚îÇ     FFN 1       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n         ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Attention 2   ‚îÇ\n‚îÇ        ‚Üì        ‚îÇ  Bloc 2\n‚îÇ     FFN 2       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n        ...\n         ‚îÇ\n         ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Attention N   ‚îÇ\n‚îÇ        ‚Üì        ‚îÇ  Bloc N (ex: N=12 pour BERT)\n‚îÇ     FFN N       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n         ‚ñº\nSortie (6 vecteurs tr√®s enrichis)\n```\n\nChaque passage enrichit les repr√©sentations. Apr√®s N blocs, chaque mot \"comprend\" toute la phrase.\n\n### 1.4 Ce qu'on va construire\n\n```\n    TRANSFORMER\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚îÇ  Embedding + Positional    ‚îÇ\n    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n    ‚îÇ ‚îÇ   SELF-ATTENTION  ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îº‚îÄ‚îÄ‚îÄ Sessions 1-2\n    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n    ‚îÇ ‚îÇ     FEED-FORWARD       ‚îÇ ‚îÇ\n    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n    ‚îÇ         √ó N blocs          ‚îÇ\n    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n    ‚îÇ     Couche de sortie       ‚îÇ\n    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Plan du cours** :\n- **Session 1** : M√©canisme d'attention (ce TP)\n- **Session 2** : Multi-Head Attention\n- **Session 3** : Assembler le Transformer complet\n- **Sessions 4-5** : Entra√Ænement et projets\n\n### 1.5 L'id√©e cl√© de l'attention\n\nL'attention r√©pond √† la question : **\"Pour comprendre ce mot, quels autres mots dois-je regarder ?\"**\n\n**Exemple** : *\"Le chat qui dormait sur le canap√© a saut√©\"*\n- Pour comprendre **\"a saut√©\"** ‚Üí regarder **\"chat\"** (le sujet, pas \"canap√©\")\n\n### Analogie : La biblioth√®que\n\n- **Query (Q)** : Votre question (\"Je cherche un livre sur les chats\")\n- **Key (K)** : Les mots-cl√©s de chaque livre\n- **Value (V)** : Le contenu des livres\n\nL'attention compare votre **question** aux **mots-cl√©s**, puis retourne un m√©lange pond√©r√© des **contenus** les plus pertinents.\n\n---\n\n### üìö Pour approfondir RNN/LSTM (optionnel)\n\n**Vid√©os en fran√ßais** :\n- [Machine Learnia - Les RNN expliqu√©s](https://www.youtube.com/watch?v=EL439RMv3Xc) (~20 min)\n- [Science4All - Comprendre les LSTM](https://www.youtube.com/watch?v=WCUNPb-5EYI) (~15 min)\n\n**Articles en fran√ßais** :\n- [Pens√©e Artificielle - Introduction aux RNN](https://www.penseeartificielle.fr/comprendre-reseaux-neurones-recurrents-rnn/)\n- [DataScientest - LSTM expliqu√© simplement](https://datascientest.com/lstm-tout-savoir)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Visualisation intuitive\n",
    "\n",
    "Avant de coder, visualisons ce que fait l'attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple simple : attention dans une phrase\n",
    "phrase = [\"Le\", \"chat\", \"mange\", \"la\", \"souris\"]\n",
    "\n",
    "# Matrice d'attention simul√©e (quels mots regardent quels mots ?)\n",
    "# Chaque ligne = un mot qui \"regarde\" les autres\n",
    "attention_simulee = torch.tensor([\n",
    "    [0.8, 0.1, 0.05, 0.03, 0.02],  # \"Le\" regarde surtout lui-m√™me\n",
    "    [0.1, 0.7, 0.1, 0.05, 0.05],   # \"chat\" regarde surtout lui-m√™me\n",
    "    [0.05, 0.4, 0.4, 0.05, 0.1],   # \"mange\" regarde \"chat\" et lui-m√™me\n",
    "    [0.02, 0.03, 0.05, 0.8, 0.1],  # \"la\" regarde surtout lui-m√™me\n",
    "    [0.02, 0.1, 0.2, 0.08, 0.6],   # \"souris\" regarde \"mange\" et elle-m√™me\n",
    "])\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention_simulee, cmap='Blues')\n",
    "plt.xticks(range(5), phrase)\n",
    "plt.yticks(range(5), phrase)\n",
    "plt.xlabel(\"Mots regard√©s (Keys)\")\n",
    "plt.ylabel(\"Mots qui regardent (Queries)\")\n",
    "plt.title(\"Qui regarde qui ? (Matrice d'attention)\")\n",
    "plt.colorbar(label=\"Poids d'attention\")\n",
    "\n",
    "# Afficher les valeurs\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        plt.text(j, i, f'{attention_simulee[i,j]:.2f}', \n",
    "                ha='center', va='center',\n",
    "                color='white' if attention_simulee[i,j] > 0.5 else 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Dans cette matrice, quel mot le verbe \"mange\" regarde-t-il le plus ? Pourquoi est-ce logique ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. Scaled Dot-Product Attention\n\nLe **Scaled Dot-Product Attention** est l'op√©ration qui calcule la **matrice d'attention** (les poids \"qui regarde qui\") et produit les vecteurs enrichis en sortie.\n\n**Rappel des 3 vecteurs :**\n\n| Vecteur | R√¥le | Sert √†... |\n|---------|------|-----------|\n| **Q** (Query) | Ce que je cherche | Calculer les poids (avec K) |\n| **K** (Key) | Mon identit√© / √©tiquette | Calculer les poids (avec Q) |\n| **V** (Value) | Mon contenu / l'info que je transmets | √ätre r√©cup√©r√© selon les poids |\n\n**Concr√®tement** : La matrice d'attention dit \"√† quel point chaque mot m'int√©resse\" (calcul√©e avec Q et K). Ensuite on r√©cup√®re l'**information** (V) de ces mots, pond√©r√©e par ces poids.\n\n**Exemple** : Pour \"dort\" dans [\"Le\", \"chat\", \"dort\"], si les poids sont [0.26, 0.42, 0.32] :\n- On r√©cup√®re 26% du **contenu** (V) de \"Le\"\n- On r√©cup√®re 42% du **contenu** (V) de \"chat\"\n- On r√©cup√®re 32% du **contenu** (V) de \"dort\"\n\n**Attention au vocabulaire** :\n- `softmax(QK^T/‚àöd_k)` = **matrice d'attention** (les poids)\n- `Attention(Q,K,V)` = matrice d'attention √ó V = **sortie** (vecteurs enrichis)\n\n### La formule\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\nO√π :\n- $Q$ (Query) : Ce que je cherche - shape `(seq_len, d_k)`\n- $K$ (Key) : Les √©tiquettes de ce qui est disponible - shape `(seq_len, d_k)`\n- $V$ (Value) : Le contenu disponible - shape `(seq_len, d_v)`\n- $d_k$ : Dimension des cl√©s (pour normaliser)\n\n> **Note** : Q, K, V sont obtenus √† partir des embeddings via des matrices de poids apprenables. Cela permet √† chaque mot d'avoir une repr√©sentation adapt√©e √† son r√¥le (chercher, s'identifier, transmettre).\n\n### Exemple concret\n\nPrenons la phrase **[\"Le\", \"chat\", \"dort\"]** avec des embeddings de dimension 4.\n\nSupposons qu'apr√®s transformation, on obtienne :\n\n```\n         Q (Queries)         K (Keys)           V (Values)\nLe    ‚Üí [0.1, 0.2, 0.1, 0.0]  [0.9, 0.1, 0.0, 0.2]  [1.0, 0.0, 0.0, 0.0]\nchat  ‚Üí [0.2, 0.8, 0.1, 0.3]  [0.2, 0.9, 0.2, 0.1]  [0.0, 1.0, 0.0, 0.0]\ndort  ‚Üí [0.3, 0.7, 0.2, 0.1]  [0.1, 0.3, 0.8, 0.1]  [0.0, 0.0, 1.0, 0.0]\n```\n\n**Calculons l'attention pour \"dort\"** (quelle info r√©cup√®re-t-il des autres mots ?) :\n\n**√âtape 1 - Scores (Q¬∑K·µÄ)** : On compare la Query de \"dort\" aux Keys de tous les mots\n```\nQ_dort ¬∑ K_Le   = 0.3√ó0.9 + 0.7√ó0.1 + 0.2√ó0.0 + 0.1√ó0.2 = 0.36\nQ_dort ¬∑ K_chat = 0.3√ó0.2 + 0.7√ó0.9 + 0.2√ó0.2 + 0.1√ó0.1 = 0.74  ‚Üê score √©lev√© !\nQ_dort ¬∑ K_dort = 0.3√ó0.1 + 0.7√ó0.3 + 0.2√ó0.8 + 0.1√ó0.1 = 0.41\n\nScores = [0.36, 0.74, 0.41]\n```\n\n**√âtape 2 - Scaling (√∑‚àöd_k)** : On divise par ‚àö4 = 2\n```\nScaled = [0.18, 0.37, 0.205]\n```\n\n**√âtape 3 - Softmax** : On transforme en probabilit√©s\n```\nPoids = [0.26, 0.42, 0.32]  (somme = 1)\n```\n\n**√âtape 4 - Output (poids √ó V)** : Moyenne pond√©r√©e des Values\n```\nOutput_dort = 0.26 √ó V_Le + 0.42 √ó V_chat + 0.32 √ó V_dort\n            = 0.26 √ó [1,0,0,0] + 0.42 √ó [0,1,0,0] + 0.32 √ó [0,0,1,0]\n            = [0.26, 0.42, 0.32, 0.0]\n```\n\n**Interpr√©tation** : La nouvelle repr√©sentation de \"dort\" contient **42% d'info de \"chat\"** (le sujet), **32% de lui-m√™me** (le verbe), et **26% de \"Le\"** (le d√©terminant). Le mod√®le a appris que pour comprendre un verbe, il faut surtout regarder son sujet.\n\n### D√©composition √©tape par √©tape\n\n1. **Scores** : $QK^T$ - Mesure la similarit√© entre queries et keys\n2. **Scaling** : Division par $\\sqrt{d_k}$ - √âvite des valeurs trop grandes\n3. **Softmax** : Transforme en probabilit√©s (somme = 1)\n4. **Output** : Multiplication par $V$ - Moyenne pond√©r√©e des values\n\n### Pourquoi softmax ? Pourquoi normaliser ?\n\n**Le softmax** transforme des scores quelconques en **probabilit√©s** :\n\n$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n\n```\nScores bruts :  [0.36, 0.74, 0.41]  (peuvent √™tre n√©gatifs, grands, etc.)\n                        ‚Üì softmax\nProbabilit√©s :  [0.26, 0.42, 0.32]  (entre 0 et 1, somme = 1)\n```\n\n**Propri√©t√©s utiles** :\n- Toutes les valeurs sont positives et somment √† 1 ‚Üí interpr√©tables comme \"pourcentage d'attention\"\n- Amplifie les diff√©rences : le score le plus √©lev√© \"gagne\" plus de poids\n\n**La normalisation (√∑‚àöd_k)** √©vite un probl√®me quand la dimension est grande :\n\n```\nSans normalisation (d_k = 512) :\n  Scores Q¬∑K ‚Üí valeurs entre -50 et +50\n  Softmax ‚Üí [0.0001, 0.9998, 0.0001]  ‚Üê trop \"peaked\" !\n  \nAvec normalisation (√∑‚àö512 ‚âà 22.6) :\n  Scores ‚Üí valeurs entre -2 et +2\n  Softmax ‚Üí [0.20, 0.45, 0.35]  ‚Üê distribution plus douce\n```\n\nUne distribution trop \"peaked\" pose probl√®me : gradients tr√®s faibles ‚Üí apprentissage difficile."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1 : Calcul manuel des scores\n",
    "\n",
    "Commen√ßons par calculer les scores d'attention manuellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple simple avec 3 mots et dimension 4\n",
    "seq_len = 3\n",
    "d_k = 4\n",
    "\n",
    "# Cr√©ons des Query, Key, Value al√©atoires\n",
    "Q = torch.randn(seq_len, d_k)\n",
    "K = torch.randn(seq_len, d_k)\n",
    "V = torch.randn(seq_len, d_k)\n",
    "\n",
    "print(\"Q (Queries):\")\n",
    "print(Q)\n",
    "print(f\"\\nShape Q: {Q.shape}\")\n",
    "print(f\"Shape K: {K.shape}\")\n",
    "print(f\"Shape V: {V.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Calculez les scores d'attention\n",
    "# ============================================\n",
    "\n",
    "# √âtape 1 : Calculer QK^T (produit matriciel)\n",
    "# La transpos√©e de K se note K.T\n",
    "\n",
    "scores = None  # TODO: Calculer QK^T\n",
    "\n",
    "print(\"Scores (QK^T):\")\n",
    "print(scores)\n",
    "print(f\"Shape: {scores.shape}\")  # Devrait √™tre (3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 2 : Appliquez le scaling\n",
    "# ============================================\n",
    "\n",
    "# Diviser par la racine de la dimension des vecteurs pour √©viter des valeurs trop grandes\n",
    "\n",
    "import math\n",
    "\n",
    "scaled_scores = None  # TODO: scores / sqrt(d_k)\n",
    "\n",
    "print(\"Scaled scores:\")\n",
    "print(scaled_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 3 : Appliquez le softmax\n",
    "# ============================================\n",
    "\n",
    "# Le softmax transforme les scores en probabilit√©s\n",
    "# Chaque ligne doit sommer √† 1\n",
    "# Indice : F.softmax(tensor, dim=i) applique softmax sur la dimension i\n",
    "\n",
    "attention_weights = None  # TODO: Appliquer softmax sur scaled_scores\n",
    "\n",
    "print(\"Poids d'attention (apr√®s softmax):\")\n",
    "print(attention_weights)\n",
    "print(f\"\\nV√©rification - Somme par ligne: {attention_weights.sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 4 : Calculez la sortie finale\n",
    "# ============================================\n",
    "\n",
    "# Multiplier les poids d'attention par V\n",
    "# C'est une moyenne pond√©r√©e des values\n",
    "\n",
    "output = None  # TODO: attention_weights @ V\n",
    "\n",
    "print(\"Output:\")\n",
    "print(output)\n",
    "print(f\"Shape: {output.shape}\")  # Devrait √™tre (3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Impl√©mentation compl√®te\n",
    "\n",
    "### Exercice 5 : Fonction d'attention\n",
    "\n",
    "Maintenant, regroupez tout dans une fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Calcule le Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries, shape (seq_len, d_k) ou (batch, seq_len, d_k)\n",
    "        K: Keys, shape (seq_len, d_k) ou (batch, seq_len, d_k)\n",
    "        V: Values, shape (seq_len, d_v) ou (batch, seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: R√©sultat de l'attention, shape (seq_len, d_v)\n",
    "        attention_weights: Poids d'attention, shape (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # TODO: R√©cup√©rer d_k (derni√®re dimension de K)\n",
    "    d_k=None\n",
    "    # TODO: Impl√©menter les 4 √©tapes\n",
    "    # 1. Calculer les scores : QK^T\n",
    "    scores = None\n",
    "    \n",
    "    # 2. Scaling : diviser par sqrt(d_k)\n",
    "    scaled_scores = None\n",
    "    \n",
    "    # 3. Softmax pour obtenir les poids\n",
    "    attention_weights = None\n",
    "    \n",
    "    # 4. Moyenne pond√©r√©e : weights @ V\n",
    "    output = None\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre fonction\n",
    "Q_test = torch.randn(4, 8)  # 4 tokens, dimension 8\n",
    "K_test = torch.randn(4, 8)\n",
    "V_test = torch.randn(4, 8)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q_test, K_test, V_test)\n",
    "\n",
    "print(f\"Structure de sortie: {output.shape}\")  # Attendu: (4, 8)\n",
    "print(f\"Structure des poids: {weights.shape}\")  # Attendu: (4, 4)\n",
    "print(f\"Somme des poids par ligne: {weights.sum(dim=1)}\")  # Attendu: [1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Pourquoi diviser par sqrt(d_k) ?\n",
    "\n",
    "C'est une question importante ! Voyons l'effet du scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison avec et sans scaling\n",
    "d_k_grand = 512  # Dimension typique dans un Transformer\n",
    "\n",
    "Q_grand = torch.randn(10, d_k_grand)\n",
    "K_grand = torch.randn(10, d_k_grand)\n",
    "\n",
    "# Scores sans scaling\n",
    "scores_sans_scaling = Q_grand @ K_grand.T\n",
    "attention_sans_scaling = F.softmax(scores_sans_scaling, dim=-1)\n",
    "\n",
    "# Scores avec scaling\n",
    "scores_avec_scaling = (Q_grand @ K_grand.T) / math.sqrt(d_k_grand)\n",
    "attention_avec_scaling = F.softmax(scores_avec_scaling, dim=-1)\n",
    "\n",
    " # Fonction pour calculer l'entropie (avec epsilon pour √©viter log(0))\n",
    "def entropy(p, eps=1e-9):\n",
    "  p_safe = p.clamp(min=eps)\n",
    "  return -(p * p_safe.log()).sum(dim=-1).mean()\n",
    "\n",
    "\n",
    "print(\"=== SANS SCALING ===\")\n",
    "print(f\"Scores - min: {scores_sans_scaling.min():.2f}, max: {scores_sans_scaling.max():.2f}\")\n",
    "print(f\"Attention max par ligne: {attention_sans_scaling.max(dim=-1).values[:3]}\")\n",
    "print(f\"Entropie moyenne: {entropy(attention_sans_scaling):.4f}\")\n",
    "\n",
    "print(\"\\n=== AVEC SCALING ===\")\n",
    "print(f\"Scores - min: {scores_avec_scaling.min():.2f}, max: {scores_avec_scaling.max():.2f}\")\n",
    "print(f\"Attention max par ligne: {attention_avec_scaling.max(dim=-1).values[:3]}\")\n",
    "print(f\"Entropie moyenne: {entropy(attention_avec_scaling):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Observation** : Sans scaling, le softmax devient tr√®s \"peaked\" (une valeur proche de 1, les autres proches de 0). Le scaling permet une distribution plus douce et des gradients plus stables.\n\n**Comment lire l'entropie ?**\n- **Entropie haute** (~2.3 pour 10 tokens) ‚Üí attention r√©partie sur plusieurs mots\n- **Entropie basse** (~0) ‚Üí attention concentr√©e sur un seul mot\n\n**Nuance importante** : Une attention concentr√©e n'est pas toujours mauvaise ! Par exemple, dans *\"Le chat dort, il ronfle\"*, le mot \"il\" DOIT regarder \"chat\" √† 95%.\n\nLe probl√®me c'est quand l'attention est peaked **par d√©faut** (artefact num√©rique du softmax satur√©) plut√¥t que **par apprentissage**. Le scaling permet au mod√®le de **choisir** entre attention concentr√©e ou distribu√©e selon ce qui est pertinent."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Application : Self-Attention sur une phrase\n",
    "\n",
    "Appliquons l'attention √† une vraie phrase et visualisons les r√©sultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase exemple\n",
    "phrase = [\"Le\", \"chat\", \"noir\", \"dort\", \"sur\", \"le\", \"canap√©\"]\n",
    "seq_len = len(phrase)\n",
    "embed_dim = 16  # Dimension des embeddings\n",
    "\n",
    "# Simulons des embeddings (en vrai, ils seraient appris)\n",
    "torch.manual_seed(42)\n",
    "embeddings = torch.randn(seq_len, embed_dim)\n",
    "\n",
    "print(f\"Phrase: {phrase}\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En self-attention, Q = K = V = embeddings\n",
    "# (chaque mot se compare √† tous les autres)\n",
    "\n",
    "output, attention_weights = scaled_dot_product_attention(\n",
    "    Q=embeddings,\n",
    "    K=embeddings,\n",
    "    V=embeddings\n",
    ")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_weights.detach().numpy(), cmap='Blues')\n",
    "plt.xticks(range(seq_len), phrase, rotation=45)\n",
    "plt.yticks(range(seq_len), phrase)\n",
    "plt.xlabel(\"Mots regard√©s (Keys)\")\n",
    "plt.ylabel(\"Mots qui regardent (Queries)\")\n",
    "plt.title(\"Self-Attention : Qui regarde qui ?\")\n",
    "plt.colorbar(label=\"Poids d'attention\")\n",
    "\n",
    "# Afficher les valeurs\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        val = attention_weights[i, j].item()\n",
    "        plt.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                color='white' if val > 0.3 else 'black', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Module nn.Module\n",
    "\n",
    "### Exercice 6 : Classe Attention en PyTorch\n",
    "\n",
    "Cr√©ons une classe PyTorch r√©utilisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Module de Self-Attention.\n",
    "    \n",
    "    En self-attention, on projette les embeddings en Q, K, V\n",
    "    avec des matrices de poids apprenables.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: Dimension des embeddings d'entr√©e\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # TODO: Cr√©er 3 couches lin√©aires pour projeter vers Q, K, V\n",
    "        # Chaque couche : embed_dim -> embed_dim\n",
    "        self.W_q = None  # nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = None  # nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = None  # nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Embeddings, shape (batch, seq_len, embed_dim)\n",
    "        \n",
    "        Returns:\n",
    "            output: R√©sultat de l'attention\n",
    "            attention_weights: Poids d'attention\n",
    "        \"\"\"\n",
    "        # TODO: Projeter x vers Q, K, V\n",
    "        Q = None  # self.W_q(x)\n",
    "        K = None  # self.W_k(x)\n",
    "        V = None  # self.W_v(x)\n",
    "        \n",
    "        # TODO: Appliquer l'attention\n",
    "        # Attention: pour les batches, K.transpose(-2, -1) au lieu de K.T\n",
    "        d_k = self.embed_dim\n",
    "        \n",
    "        scores = None  # Q @ K.transpose(-2, -1)\n",
    "        scaled_scores = None  # scores / sqrt(d_k)\n",
    "        attention_weights = None  # softmax\n",
    "        output = None  # attention_weights @ V\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du module\n",
    "embed_dim = 32\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "attention_layer = SelfAttention(embed_dim)\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "output, weights = attention_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")  # Attendu: (2, 5, 32)\n",
    "print(f\"Weights shape: {weights.shape}\")  # Attendu: (2, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 8. Visualiser l'attention d'un vrai mod√®le\n\nMaintenant qu'on a compris et impl√©ment√© le m√©canisme, regardons ce que √ßa donne sur un mod√®le **r√©ellement entra√Æn√©**.\n\nOn va utiliser **DistilBERT**, une version l√©g√®re de BERT, pour observer les patterns d'attention appris."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Installation de la librairie transformers\n!pip install transformers -q"
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoModel, AutoTokenizer\nimport torch\n\n# Charger un petit mod√®le pr√©-entra√Æn√©\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, output_attentions=True)\nmodel.eval()\n\n# Phrase de test (en anglais pour ce mod√®le)\nphrase = \"The cat sat on the mat because it was tired\"\n\n# Tokenizer la phrase\ninputs = tokenizer(phrase, return_tensors=\"pt\")\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n\n# Forward pass (sans calculer les gradients)\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Extraire les attentions\nattentions = outputs.attentions\n\nprint(f\"Phrase: {phrase}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Nombre de couches: {len(attentions)}\")\nprint(f\"Nombre de t√™tes par couche: {attentions[0].shape[1]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualiser l'attention d'une t√™te sp√©cifique\nlayer = 0   # Premi√®re couche (0 √† 5)\nhead = 0    # Premi√®re t√™te (0 √† 11)\n\nattention_matrix = attentions[layer][0, head].numpy()\n\nplt.figure(figsize=(10, 8))\nplt.imshow(attention_matrix, cmap='Blues')\nplt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\nplt.yticks(range(len(tokens)), tokens)\nplt.xlabel(\"Tokens regard√©s (Keys)\")\nplt.ylabel(\"Tokens qui regardent (Queries)\")\nplt.title(f\"Attention r√©elle - Couche {layer+1}, T√™te {head+1}\")\nplt.colorbar(label=\"Poids d'attention\")\n\nfor i in range(len(tokens)):\n    for j in range(len(tokens)):\n        val = attention_matrix[i, j]\n        plt.text(j, i, f'{val:.2f}', ha='center', va='center',\n                color='white' if val > 0.3 else 'black', fontsize=7)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Question : Que regarde le pronom \"it\" ?\n# Trouvons son index et regardons sa ligne d'attention\n\nit_index = tokens.index(\"it\")\nprint(f\"'it' est √† l'index {it_index}\")\nprint(f\"\\nAttention de 'it' vers les autres tokens (couche {layer+1}, t√™te {head+1}):\")\nprint(\"-\" * 40)\n\nfor i, (token, weight) in enumerate(zip(tokens, attention_matrix[it_index])):\n    bar = \"‚ñà\" * int(weight * 30)\n    marker = \" ‚Üê ?\" if token in [\"cat\", \"mat\"] else \"\"\n    print(f\"  {token:10} {weight:.2f} {bar}{marker}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Questions d'analyse** :\n\n1. Le pronom \"it\" regarde-t-il principalement \"cat\" ou \"mat\" ? Pourquoi est-ce logique grammaticalement ?\n\n2. Changez `layer` et `head` dans les cellules ci-dessus. Que remarquez-vous ? (Indice : diff√©rentes t√™tes capturent diff√©rentes relations)\n\n3. Essayez d'autres phrases, par exemple :\n   - `\"The dog chased the cat because it was fast\"` (qui est \"it\" ici ?)\n   - `\"The trophy didn't fit in the suitcase because it was too big\"` (cas ambigu !)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. R√©capitulatif\n",
    "\n",
    "### Ce que nous avons appris\n",
    "\n",
    "1. **L'attention** permet √† chaque √©l√©ment de \"regarder\" tous les autres\n",
    "2. **Q, K, V** : Query (ce que je cherche), Key (les √©tiquettes), Value (le contenu)\n",
    "3. **Formule** : $\\text{softmax}(QK^T / \\sqrt{d_k}) \\cdot V$\n",
    "4. **Scaling** : Essentiel pour la stabilit√© des gradients\n",
    "\n",
    "### Points cl√©s\n",
    "\n",
    "| Concept | R√¥le |\n",
    "|---------|------|\n",
    "| Dot product $QK^T$ | Mesure la similarit√© |\n",
    "| Softmax | Transforme en probabilit√©s |\n",
    "| Scaling $\\sqrt{d_k}$ | Stabilise les gradients |\n",
    "| Self-attention | Q = K = V (chaque mot regarde tous les autres) |\n",
    "\n",
    "### Prochaine session\n",
    "\n",
    "Nous verrons le **Multi-Head Attention** : plusieurs \"t√™tes\" d'attention qui regardent sous diff√©rents angles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Pour aller plus loin (optionnel)\n",
    "\n",
    "### Exercice bonus : Masking\n",
    "\n",
    "Dans certains cas (g√©n√©ration de texte), on veut emp√™cher un mot de \"voir\" les mots futurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_with_mask(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Attention avec masking optionnel.\n",
    "    \n",
    "    Args:\n",
    "        Q, K, V: Query, Key, Value\n",
    "        mask: Tensor bool√©en, True = position √† masquer\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    "    \n",
    "    # Appliquer le masque (mettre -inf pour les positions masqu√©es)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Cr√©er un masque causal (triangulaire)\n",
    "seq_len = 5\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "print(\"Masque causal (True = masqu√©):\")\n",
    "print(causal_mask.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec masque\n",
    "Q = torch.randn(seq_len, 8)\n",
    "K = torch.randn(seq_len, 8)\n",
    "V = torch.randn(seq_len, 8)\n",
    "\n",
    "output_masked, weights_masked = scaled_dot_product_attention_with_mask(Q, K, V, causal_mask)\n",
    "\n",
    "print(\"Poids d'attention avec masque causal:\")\n",
    "print(weights_masked.round(decimals=2))\n",
    "print(\"\\nObservation: chaque ligne ne peut voir que les positions pr√©c√©dentes (et elle-m√™me)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}