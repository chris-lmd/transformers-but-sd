{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet C - Mini-GPT : Génération de Texte\n",
    "\n",
    "**Module** : Réseaux de Neurones Approfondissement  \n",
    "**Durée** : 2h  \n",
    "**Objectif** : Construire et entraîner un petit modèle génératif type GPT\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs du projet\n",
    "\n",
    "Dans ce projet, vous allez :\n",
    "1. Comprendre la différence entre Encodeur (BERT) et Décodeur (GPT)\n",
    "2. Implémenter le masque causal pour la génération\n",
    "3. Entraîner un mini-GPT sur un corpus français\n",
    "4. Générer du texte de manière autoregressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib numpy tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Encodeur vs Décodeur : Quelle différence ?\n",
    "\n",
    "### BERT (Encodeur) - Ce qu'on a fait jusqu'ici\n",
    "\n",
    "```\n",
    "Entrée:  \"Le chat [MASK] sur le canapé\"\n",
    "                    ↓\n",
    "         Chaque mot voit TOUS les autres\n",
    "                    ↓\n",
    "Sortie:  Prédire [MASK] = \"dort\"\n",
    "```\n",
    "\n",
    "- **Bidirectionnel** : chaque token voit le passé ET le futur\n",
    "- **Usage** : Classification, NER, Question-Answering extractif\n",
    "\n",
    "### GPT (Décodeur) - Ce qu'on fait maintenant\n",
    "\n",
    "```\n",
    "Entrée:  \"Le chat dort sur le\"\n",
    "                    ↓\n",
    "         Chaque mot voit SEULEMENT les précédents\n",
    "                    ↓\n",
    "Sortie:  Prédire le mot suivant = \"canapé\"\n",
    "```\n",
    "\n",
    "- **Unidirectionnel (causal)** : chaque token ne voit que le passé\n",
    "- **Usage** : Génération de texte, chatbots, complétion\n",
    "\n",
    "### Le masque causal\n",
    "\n",
    "Pour empêcher un token de \"tricher\" en regardant le futur, on utilise un **masque causal** :\n",
    "\n",
    "```\n",
    "              Le   chat  dort  sur   le\n",
    "      Le    [  1     0     0     0    0  ]   ← \"Le\" ne voit que lui-même\n",
    "     chat   [  1     1     0     0    0  ]   ← \"chat\" voit \"Le\" et lui-même\n",
    "     dort   [  1     1     1     0    0  ]   ← \"dort\" voit les 3 premiers\n",
    "      sur   [  1     1     1     1    0  ]   ← etc.\n",
    "       le   [  1     1     1     1    1  ]   ← dernier voit tout le passé\n",
    "```\n",
    "\n",
    "Les 0 deviennent `-∞` avant le softmax → attention = 0 sur ces positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Implémentation du Masque Causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Créer le masque causal\n",
    "# ============================================\n",
    "\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Crée un masque causal (triangulaire inférieur).\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Longueur de la séquence\n",
    "    \n",
    "    Returns:\n",
    "        mask: Tensor booléen (seq_len, seq_len)\n",
    "              True = position à masquer (ne pas regarder)\n",
    "              False = position visible\n",
    "    \"\"\"\n",
    "    # TODO: Créer une matrice triangulaire supérieure de True\n",
    "    # Indice: torch.triu(torch.ones(...), diagonal=1)\n",
    "    # diagonal=1 pour que la diagonale soit visible (un token se voit lui-même)\n",
    "    \n",
    "    mask = None  # À compléter\n",
    "    \n",
    "    return mask.bool()\n",
    "\n",
    "# Test\n",
    "mask = create_causal_mask(5)\n",
    "print(\"Masque causal (True = masqué) :\")\n",
    "print(mask.int())  # Affiche 0 et 1 pour plus de lisibilité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(~mask, cmap='Blues')  # ~ inverse pour afficher \"visible\" en bleu\n",
    "plt.xticks(range(5), ['pos 0', 'pos 1', 'pos 2', 'pos 3', 'pos 4'])\n",
    "plt.yticks(range(5), ['pos 0', 'pos 1', 'pos 2', 'pos 3', 'pos 4'])\n",
    "plt.xlabel(\"Positions regardées (Keys)\")\n",
    "plt.ylabel(\"Positions qui regardent (Queries)\")\n",
    "plt.title(\"Masque Causal (bleu = visible)\")\n",
    "plt.colorbar(label=\"Visible\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Architecture Mini-GPT\n",
    "\n",
    "On reprend notre Transformer et on ajoute le masque causal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Self-Attention avec masque causal (style GPT).\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Pré-calculer le masque causal (enregistré comme buffer)\n",
    "        mask = torch.triu(torch.ones(max_len, max_len), diagonal=1).bool()\n",
    "        self.register_buffer('mask', mask)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, S, _ = x.shape\n",
    "        \n",
    "        # Projections Q, K, V\n",
    "        Q = self.W_q(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Appliquer le masque causal\n",
    "        scores = scores.masked_fill(self.mask[:S, :S], float('-inf'))\n",
    "        \n",
    "        # Softmax et dropout\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Output\n",
    "        out = (attn @ V).transpose(1, 2).contiguous().view(B, S, self.embed_dim)\n",
    "        return self.W_o(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    \"\"\"Un bloc de GPT (attention causale + FFN).\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(embed_dim, num_heads, max_len, dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attn(self.norm1(x)))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 2 : Compléter le Mini-GPT\n",
    "# ============================================\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Mini-GPT pour la génération de texte caractère par caractère.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Taille du vocabulaire (nombre de caractères uniques)\n",
    "        embed_dim: Dimension des embeddings\n",
    "        num_heads: Nombre de têtes d'attention\n",
    "        num_layers: Nombre de blocs GPT\n",
    "        max_len: Longueur maximale des séquences\n",
    "        dropout: Taux de dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_layers=4, \n",
    "                 max_len=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Position embedding (apprise, pas sinusoïdale)\n",
    "        self.position_embedding = nn.Embedding(max_len, embed_dim)\n",
    "        \n",
    "        # Blocs GPT\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GPTBlock(embed_dim, num_heads, max_len, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer norm finale\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Tête de prédiction (projette vers le vocabulaire)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Indices de tokens, shape (batch, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Scores pour chaque token du vocabulaire, shape (batch, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        B, S = x.shape\n",
    "        \n",
    "        # TODO: Implémenter le forward\n",
    "        \n",
    "        # 1. Token embeddings\n",
    "        tok_emb = None  # self.token_embedding(x)\n",
    "        \n",
    "        # 2. Position embeddings\n",
    "        # Créer les positions [0, 1, 2, ..., S-1]\n",
    "        positions = None  # torch.arange(S, device=x.device)\n",
    "        pos_emb = None  # self.position_embedding(positions)\n",
    "        \n",
    "        # 3. Additionner et dropout\n",
    "        x = None  # self.dropout(tok_emb + pos_emb)\n",
    "        \n",
    "        # 4. Passer par tous les blocs\n",
    "        # for block in self.blocks:\n",
    "        #     x = block(x)\n",
    "        \n",
    "        # 5. Layer norm finale\n",
    "        x = None  # self.norm(x)\n",
    "        \n",
    "        # 6. Projection vers le vocabulaire\n",
    "        logits = None  # self.head(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, context, max_new_tokens, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Génère du texte de manière autoregressive.\n",
    "        \n",
    "        Args:\n",
    "            context: Tensor de shape (1, seq_len) avec le contexte initial\n",
    "            max_new_tokens: Nombre de tokens à générer\n",
    "            temperature: Contrôle la \"créativité\" (1.0 = normal, <1 = conservateur, >1 = créatif)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor avec le contexte + tokens générés\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Tronquer si nécessaire\n",
    "            context_truncated = context[:, -self.max_len:]\n",
    "            \n",
    "            # Forward\n",
    "            logits = self(context_truncated)\n",
    "            \n",
    "            # Prendre les logits du dernier token\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Échantillonner\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Ajouter au contexte\n",
    "            context = torch.cat([context, next_token], dim=1)\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test rapide\n",
    "model_test = MiniGPT(vocab_size=100, embed_dim=64, num_heads=4, num_layers=2)\n",
    "x_test = torch.randint(0, 100, (2, 32))\n",
    "out_test = model_test(x_test)\n",
    "print(f\"Input: {x_test.shape}\")\n",
    "print(f\"Output: {out_test.shape}\")  # Attendu: (2, 32, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Préparation des Données\n",
    "\n",
    "On va entraîner notre Mini-GPT sur un corpus de texte français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Télécharger un texte français (Les Misérables - Victor Hugo)\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/135/pg135.txt\"\n",
    "print(\"Téléchargement du corpus...\")\n",
    "\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    text = response.read().decode('utf-8')\n",
    "\n",
    "# Nettoyer (garder une portion pour l'entraînement rapide)\n",
    "# Le texte commence après les en-têtes Gutenberg\n",
    "start_marker = \"PREMIÈRE PARTIE\"\n",
    "start_idx = text.find(start_marker)\n",
    "if start_idx != -1:\n",
    "    text = text[start_idx:]\n",
    "\n",
    "# Limiter la taille pour Colab (environ 500Ko)\n",
    "text = text[:500000]\n",
    "\n",
    "print(f\"Taille du corpus: {len(text):,} caractères\")\n",
    "print(f\"\\nExtrait:\\n{text[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le vocabulaire (niveau caractère)\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Vocabulaire: {vocab_size} caractères uniques\")\n",
    "print(f\"Caractères: {''.join(chars[:50])}...\")\n",
    "\n",
    "# Mappings\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Fonctions d'encodage/décodage\n",
    "def encode(s):\n",
    "    return [char2idx[c] for c in s]\n",
    "\n",
    "def decode(indices):\n",
    "    return ''.join([idx2char[i] for i in indices])\n",
    "\n",
    "# Test\n",
    "test_str = \"Bonjour!\"\n",
    "encoded = encode(test_str)\n",
    "decoded = decode(encoded)\n",
    "print(f\"\\nTest: '{test_str}' -> {encoded} -> '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder tout le texte\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "\n",
    "# Split train/val (90/10)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Train: {len(train_data):,} tokens\")\n",
    "print(f\"Val: {len(val_data):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # x = contexte, y = contexte décalé de 1 (ce qu'on veut prédire)\n",
    "        x = self.data[idx:idx + self.block_size]\n",
    "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "# Configuration\n",
    "BLOCK_SIZE = 128  # Longueur des séquences\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = TextDataset(train_data, BLOCK_SIZE)\n",
    "val_dataset = TextDataset(val_data, BLOCK_SIZE)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Batches train: {len(train_loader)}\")\n",
    "print(f\"Batches val: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier un batch\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(f\"x shape: {x_batch.shape}\")  # (batch, block_size)\n",
    "print(f\"y shape: {y_batch.shape}\")  # (batch, block_size)\n",
    "\n",
    "# Exemple\n",
    "print(f\"\\nExemple (input):  '{decode(x_batch[0].tolist())[:50]}...'\")\n",
    "print(f\"Exemple (target): '{decode(y_batch[0].tolist())[:50]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le modèle\n",
    "model = MiniGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=128,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    "    max_len=BLOCK_SIZE,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Compter les paramètres\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Paramètres: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 3 : Boucle d'entraînement\n",
    "# ============================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # TODO: Implémenter\n",
    "        # 1. Zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward\n",
    "        logits = model(x)  # (batch, seq_len, vocab_size)\n",
    "        \n",
    "        # 3. Loss (cross-entropy)\n",
    "        # Attention: reshape pour cross_entropy\n",
    "        # logits: (B, S, V) -> (B*S, V)\n",
    "        # y: (B, S) -> (B*S,)\n",
    "        B, S, V = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(B*S, V), y.view(B*S))\n",
    "        \n",
    "        # 4. Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Gradient clipping (stabilité)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # 6. Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        B, S, V = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(B*S, V), y.view(B*S))\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EPOCHS = 5\n",
    "LR = 3e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "# Historique\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "print(\"Début de l'entraînement...\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss = evaluate(model, val_loader, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Générer un échantillon\n",
    "    context = torch.tensor([encode(\"Jean Valjean \")], dtype=torch.long, device=device)\n",
    "    generated = model.generate(context, max_new_tokens=100, temperature=0.8)\n",
    "    print(f\"  Génération: {decode(generated[0].tolist())[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(history['train_loss'], label='Train')\n",
    "plt.plot(history['val_loss'], label='Val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Courbe d\\'apprentissage')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Génération de Texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 4 : Expérimenter avec la génération\n",
    "# ============================================\n",
    "\n",
    "def generate_text(prompt, max_tokens=200, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Génère du texte à partir d'un prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Texte de départ\n",
    "        max_tokens: Nombre de caractères à générer\n",
    "        temperature: Créativité (0.5=conservateur, 1.0=normal, 1.5=créatif)\n",
    "    \"\"\"\n",
    "    context = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "    generated = model.generate(context, max_new_tokens=max_tokens, temperature=temperature)\n",
    "    return decode(generated[0].tolist())\n",
    "\n",
    "# Tests avec différentes températures\n",
    "prompt = \"La nuit était \"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    print(f\"\\n--- Température = {temp} ---\")\n",
    "    print(generate_text(prompt, max_tokens=150, temperature=temp))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération interactive\n",
    "prompts = [\n",
    "    \"Il marchait dans la rue \",\n",
    "    \"L'évêque dit à \",\n",
    "    \"Paris est une ville \",\n",
    "    \"Le soleil se levait sur \",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(generate_text(prompt, max_tokens=200, temperature=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Analyse et Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les probabilités de prédiction\n",
    "def visualize_predictions(text):\n",
    "    \"\"\"Visualise les probabilités de prédiction pour chaque caractère.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    x = torch.tensor([encode(text)], dtype=torch.long, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Pour chaque position, afficher le caractère prédit\n",
    "    print(f\"Texte: '{text}'\\n\")\n",
    "    print(\"Position | Réel | Prédit | Confiance\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for i in range(len(text) - 1):\n",
    "        actual = text[i + 1]\n",
    "        pred_idx = probs[0, i].argmax().item()\n",
    "        pred_char = idx2char[pred_idx]\n",
    "        confidence = probs[0, i, char2idx[actual]].item()\n",
    "        \n",
    "        match = \"✓\" if pred_char == actual else \"✗\"\n",
    "        actual_display = repr(actual)[1:-1]  # Afficher les caractères spéciaux\n",
    "        pred_display = repr(pred_char)[1:-1]\n",
    "        \n",
    "        print(f\"   {i:3d}   | '{actual_display:2s}' | '{pred_display:2s}'  | {confidence:.2%} {match}\")\n",
    "\n",
    "visualize_predictions(\"Jean Valjean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Récapitulatif\n",
    "\n",
    "### Ce que vous avez appris\n",
    "\n",
    "1. **Différence Encodeur/Décodeur** : Le masque causal empêche de \"voir le futur\"\n",
    "2. **Architecture GPT** : Attention causale + génération autoregressive\n",
    "3. **Entraînement** : Prédire le caractère suivant (next token prediction)\n",
    "4. **Température** : Contrôle le compromis entre cohérence et créativité\n",
    "\n",
    "### Comparaison avec les vrais GPT\n",
    "\n",
    "| | Notre Mini-GPT | GPT-2 Small | GPT-3 |\n",
    "|--|----------------|-------------|-------|\n",
    "| Paramètres | ~1M | 117M | 175B |\n",
    "| Données | ~500Ko | 40Go | ~500Go |\n",
    "| Vocabulaire | Caractères | BPE (~50k) | BPE (~50k) |\n",
    "| Contexte | 128 | 1024 | 2048 |\n",
    "\n",
    "### Limitations de notre modèle\n",
    "\n",
    "- **Vocabulaire caractère** : Inefficace, perd la notion de \"mot\"\n",
    "- **Petit corpus** : Surapprentissage probable\n",
    "- **Contexte court** : Ne peut pas capturer les dépendances longues\n",
    "\n",
    "### Pour aller plus loin\n",
    "\n",
    "- Utiliser un tokenizer BPE (comme SentencePiece)\n",
    "- Entraîner sur plus de données\n",
    "- Ajouter des techniques modernes (RoPE, KV-cache, Flash Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espace pour vos expérimentations\n",
    "\n",
    "# Essayez différents prompts !\n",
    "mon_prompt = \"Cosette regardait \"\n",
    "print(generate_text(mon_prompt, max_tokens=300, temperature=0.8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
