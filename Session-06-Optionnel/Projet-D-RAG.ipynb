{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet D - RAG : Question-Answering sur Documents\n",
    "\n",
    "**Module** : R√©seaux de Neurones Approfondissement  \n",
    "**Dur√©e** : 2h  \n",
    "**Objectif** : Construire un syst√®me de question-answering avec RAG\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs du projet\n",
    "\n",
    "Dans ce projet, vous allez :\n",
    "1. Comprendre le principe du RAG (Retrieval-Augmented Generation)\n",
    "2. Cr√©er un index de documents avec des embeddings\n",
    "3. Impl√©menter la recherche s√©mantique\n",
    "4. Combiner retrieval + g√©n√©ration pour r√©pondre √† des questions"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "> **üìå Note p√©dagogique** : Le RAG combine **recherche s√©mantique** (embeddings) et **g√©n√©ration** (LLM).\n>\n> **Cross-attention** (mentionn√© en Session 2) : Le mod√®le g√©n√©rateur (ici FLAN-T5) est un **encodeur-d√©codeur** \n> qui utilise du cross-attention : le d√©codeur \"interroge\" l'encodeur qui a trait√© le contexte. \n> C'est ce m√©canisme qui permet au mod√®le de g√©n√©rer une r√©ponse bas√©e sur les documents r√©cup√©r√©s.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers sentence-transformers matplotlib numpy tqdm faiss-cpu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import faiss\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Pourquoi le RAG ?\n",
    "\n",
    "### Le probl√®me des LLMs\n",
    "\n",
    "Les mod√®les de langage (GPT, BERT...) ont des **limitations** :\n",
    "\n",
    "| Limitation | Exemple |\n",
    "|------------|--------|\n",
    "| **Connaissance fig√©e** | Ne conna√Æt pas les √©v√©nements apr√®s son entra√Ænement |\n",
    "| **Hallucinations** | Invente des faits avec confiance |\n",
    "| **Pas de sources** | Ne peut pas citer d'o√π vient l'information |\n",
    "| **Donn√©es priv√©es** | Ne conna√Æt pas vos documents internes |\n",
    "\n",
    "### La solution : RAG\n",
    "\n",
    "**RAG** = Retrieval-Augmented Generation\n",
    "\n",
    "```\n",
    "Question: \"Quelle est la politique de cong√©s de l'entreprise ?\"\n",
    "        ‚Üì\n",
    "    [RETRIEVER]\n",
    "    Cherche dans la base documentaire\n",
    "        ‚Üì\n",
    "    Documents pertinents trouv√©s:\n",
    "    - \"Article 5.2: Les employ√©s ont droit √† 25 jours...\"\n",
    "    - \"Note RH: Les cong√©s doivent √™tre pos√©s 2 semaines avant...\"\n",
    "        ‚Üì\n",
    "    [GENERATOR]\n",
    "    LLM g√©n√®re une r√©ponse bas√©e sur les documents\n",
    "        ‚Üì\n",
    "    R√©ponse: \"Selon l'article 5.2, vous avez droit √† 25 jours de cong√©s.\n",
    "              Les cong√©s doivent √™tre pos√©s 2 semaines √† l'avance.\"\n",
    "```\n",
    "\n",
    "### Architecture RAG\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                         RAG SYSTEM                          ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  Question ‚îÄ‚îÄ‚ñ∫ [Encoder] ‚îÄ‚îÄ‚ñ∫ Query Embedding                 ‚îÇ\n",
    "‚îÇ                                    ‚îÇ                        ‚îÇ\n",
    "‚îÇ                                    ‚ñº                        ‚îÇ\n",
    "‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ\n",
    "‚îÇ              ‚îÇ     INDEX VECTORIEL         ‚îÇ                ‚îÇ\n",
    "‚îÇ              ‚îÇ  (embeddings des documents) ‚îÇ                ‚îÇ\n",
    "‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ\n",
    "‚îÇ                                    ‚îÇ                        ‚îÇ\n",
    "‚îÇ                                    ‚ñº                        ‚îÇ\n",
    "‚îÇ              Top-K documents les plus similaires            ‚îÇ\n",
    "‚îÇ                                    ‚îÇ                        ‚îÇ\n",
    "‚îÇ                                    ‚ñº                        ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ\n",
    "‚îÇ  ‚îÇ Prompt = Question + Contexte (documents trouv√©s)‚îÇ        ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ\n",
    "‚îÇ                                    ‚îÇ                        ‚îÇ\n",
    "‚îÇ                                    ‚ñº                        ‚îÇ\n",
    "‚îÇ                              [LLM / Generator]              ‚îÇ\n",
    "‚îÇ                                    ‚îÇ                        ‚îÇ\n",
    "‚îÇ                                    ‚ñº                        ‚îÇ\n",
    "‚îÇ                               R√©ponse                       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Pr√©paration des Documents\n",
    "\n",
    "On va cr√©er une base de connaissances sur les **Transformers** (le sujet de ce cours !)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base de connaissances (documents)\n",
    "documents = [\n",
    "    # Architecture\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"title\": \"Architecture Transformer\",\n",
    "        \"content\": \"Le Transformer est une architecture de r√©seau de neurones introduite en 2017 dans le papier 'Attention Is All You Need'. Il se compose d'un encodeur et d'un d√©codeur, tous deux bas√©s sur le m√©canisme d'attention. L'encodeur traite la s√©quence d'entr√©e en parall√®le, tandis que le d√©codeur g√©n√®re la s√©quence de sortie de mani√®re autoregressive.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"title\": \"M√©canisme d'Attention\",\n",
    "        \"content\": \"L'attention permet √† chaque position de la s√©quence de 'regarder' toutes les autres positions. Elle utilise trois vecteurs : Query (Q), Key (K) et Value (V). Le score d'attention est calcul√© par le produit scalaire Q¬∑K, normalis√© par la racine de la dimension, puis pass√© dans un softmax. La formule est : Attention(Q,K,V) = softmax(QK^T/‚àöd_k)V.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"title\": \"Multi-Head Attention\",\n",
    "        \"content\": \"Le Multi-Head Attention utilise plusieurs 't√™tes' d'attention en parall√®le. Chaque t√™te peut apprendre √† d√©tecter diff√©rents types de relations : syntaxiques, s√©mantiques, de proximit√©, etc. Les sorties des t√™tes sont concat√©n√©es puis projet√©es. BERT-base utilise 12 t√™tes, GPT-3 en utilise 96.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"title\": \"Positional Encoding\",\n",
    "        \"content\": \"Comme le Transformer traite les tokens en parall√®le, il n'a pas de notion d'ordre. Le Positional Encoding ajoute une information de position √† chaque embedding. La version originale utilise des fonctions sinuso√Ødales : PE(pos,2i) = sin(pos/10000^(2i/d)) et PE(pos,2i+1) = cos(pos/10000^(2i/d)). Les mod√®les r√©cents utilisent souvent des embeddings de position appris.\"\n",
    "    },\n",
    "    # Mod√®les\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"title\": \"BERT\",\n",
    "        \"content\": \"BERT (Bidirectional Encoder Representations from Transformers) est un mod√®le cr√©√© par Google en 2018. Il utilise uniquement l'encodeur du Transformer et est pr√©-entra√Æn√© avec deux t√¢ches : Masked Language Modeling (MLM) o√π 15% des tokens sont masqu√©s, et Next Sentence Prediction (NSP). BERT-base a 110M de param√®tres et 12 couches.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"title\": \"GPT\",\n",
    "        \"content\": \"GPT (Generative Pre-trained Transformer) est d√©velopp√© par OpenAI. Contrairement √† BERT, GPT utilise uniquement le d√©codeur avec un masque causal : chaque token ne peut voir que les tokens pr√©c√©dents. GPT est entra√Æn√© √† pr√©dire le token suivant. GPT-3 a 175 milliards de param√®tres et GPT-4 est estim√© √† plus de 1000 milliards.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"title\": \"T5\",\n",
    "        \"content\": \"T5 (Text-to-Text Transfer Transformer) de Google reformule toutes les t√¢ches NLP en g√©n√©ration de texte. Par exemple, la classification devient : 'classify: This movie is great' ‚Üí 'positive'. T5 utilise l'architecture encodeur-d√©codeur compl√®te. Le mod√®le T5-11B a 11 milliards de param√®tres.\"\n",
    "    },\n",
    "    # Entra√Ænement\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"title\": \"Pr√©-entra√Ænement\",\n",
    "        \"content\": \"Le pr√©-entra√Ænement consiste √† entra√Æner un mod√®le sur une grande quantit√© de texte non annot√© avec des t√¢ches auto-supervis√©es. Pour BERT c'est le MLM, pour GPT c'est la pr√©diction du token suivant. Cette phase capture les connaissances linguistiques g√©n√©rales et n√©cessite d'√©normes ressources de calcul.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 9,\n",
    "        \"title\": \"Fine-tuning\",\n",
    "        \"content\": \"Le fine-tuning adapte un mod√®le pr√©-entra√Æn√© √† une t√¢che sp√©cifique. On ajoute une t√™te de classification et on entra√Æne sur un dataset annot√©. Le fine-tuning n√©cessite beaucoup moins de donn√©es et de calcul que le pr√©-entra√Ænement. Typiquement quelques heures sur un GPU contre des semaines pour le pr√©-entra√Ænement.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"title\": \"Tokenization\",\n",
    "        \"content\": \"La tokenization d√©coupe le texte en unit√©s (tokens). Les m√©thodes modernes comme BPE (Byte Pair Encoding) ou WordPiece cr√©ent un vocabulaire de sous-mots. Par exemple 'unhappiness' devient ['un', '##happy', '##ness']. Cela permet de g√©rer les mots rares tout en gardant un vocabulaire de taille raisonnable (30k-50k tokens).\"\n",
    "    },\n",
    "    # Applications\n",
    "    {\n",
    "        \"id\": 11,\n",
    "        \"title\": \"Classification de texte\",\n",
    "        \"content\": \"Pour la classification avec BERT, on utilise le token [CLS] qui r√©sume toute la s√©quence. On ajoute une couche lin√©aire qui projette l'embedding [CLS] vers le nombre de classes. Pour GPT, on peut utiliser le dernier token ou faire une moyenne des embeddings.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 12,\n",
    "        \"title\": \"Question Answering\",\n",
    "        \"content\": \"En Question Answering extractif, le mod√®le doit trouver la r√©ponse dans un contexte donn√©. Le mod√®le pr√©dit deux positions : le d√©but et la fin de la r√©ponse dans le texte. On entra√Æne avec des datasets comme SQuAD qui contient 100k+ paires question-r√©ponse avec leur contexte.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 13,\n",
    "        \"title\": \"G√©n√©ration de texte\",\n",
    "        \"content\": \"La g√©n√©ration de texte avec GPT est autoregressive : on pr√©dit un token, on l'ajoute √† l'entr√©e, puis on pr√©dit le suivant. La temp√©rature contr√¥le la 'cr√©ativit√©' : temp√©rature basse = plus d√©terministe, temp√©rature haute = plus al√©atoire. Le top-k et top-p (nucleus sampling) limitent les tokens candidats.\"\n",
    "    },\n",
    "    # Concepts avanc√©s\n",
    "    {\n",
    "        \"id\": 14,\n",
    "        \"title\": \"RAG\",\n",
    "        \"content\": \"RAG (Retrieval-Augmented Generation) combine recherche documentaire et g√©n√©ration. Au lieu de tout stocker dans les param√®tres du mod√®le, on cherche les documents pertinents dans une base externe puis on les donne au LLM comme contexte. Cela r√©duit les hallucinations et permet de citer les sources.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 15,\n",
    "        \"title\": \"Embeddings\",\n",
    "        \"content\": \"Les embeddings sont des repr√©sentations vectorielles denses. Les mots similaires ont des embeddings proches dans l'espace vectoriel. Sentence-BERT produit des embeddings de phrases enti√®res, utiles pour la recherche s√©mantique. La similarit√© cosinus mesure la proximit√© entre deux embeddings.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Base de connaissances: {len(documents)} documents\")\n",
    "for doc in documents[:3]:\n",
    "    print(f\"  - {doc['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Cr√©ation des Embeddings\n",
    "\n",
    "On utilise **Sentence-BERT** pour cr√©er des embeddings de documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le mod√®le d'embeddings\n",
    "print(\"Chargement du mod√®le d'embeddings...\")\n",
    "embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "print(f\"Mod√®le charg√© ! Dimension des embeddings: {embedding_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Cr√©er les embeddings des documents\n",
    "# ============================================\n",
    "\n",
    "def create_document_embeddings(documents, model):\n",
    "    \"\"\"\n",
    "    Cr√©e les embeddings pour chaque document.\n",
    "    \n",
    "    Args:\n",
    "        documents: Liste de dicts avec 'title' et 'content'\n",
    "        model: Mod√®le SentenceTransformer\n",
    "    \n",
    "    Returns:\n",
    "        embeddings: np.array de shape (n_docs, embed_dim)\n",
    "    \"\"\"\n",
    "    # TODO: Cr√©er les embeddings\n",
    "    # On combine title + content pour chaque document\n",
    "    \n",
    "    texts = []  # Liste des textes √† encoder\n",
    "    for doc in documents:\n",
    "        # Combiner titre et contenu\n",
    "        text = f\"{doc['title']}: {doc['content']}\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    # Encoder avec le mod√®le\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Cr√©er les embeddings\n",
    "doc_embeddings = create_document_embeddings(documents, embedding_model)\n",
    "print(f\"Embeddings shape: {doc_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Index Vectoriel avec FAISS\n",
    "\n",
    "**FAISS** (Facebook AI Similarity Search) permet de faire des recherches rapides dans de grands ensembles de vecteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 2 : Cr√©er l'index FAISS\n",
    "# ============================================\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Cr√©e un index FAISS pour la recherche de similarit√©.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: np.array de shape (n_docs, embed_dim)\n",
    "    \n",
    "    Returns:\n",
    "        index: Index FAISS\n",
    "    \"\"\"\n",
    "    # TODO: Cr√©er l'index\n",
    "    \n",
    "    # Dimension des embeddings\n",
    "    dim = embeddings.shape[1]\n",
    "    \n",
    "    # Cr√©er un index avec similarit√© cosinus\n",
    "    # On normalise les vecteurs et on utilise IndexFlatIP (Inner Product)\n",
    "    \n",
    "    # Normaliser les embeddings (pour que IP = cosine similarity)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    # Cr√©er l'index\n",
    "    index = faiss.IndexFlatIP(dim)  # IP = Inner Product\n",
    "    \n",
    "    # Ajouter les vecteurs\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "# Cr√©er l'index\n",
    "index = create_faiss_index(doc_embeddings.copy())  # copy car normalize_L2 modifie in-place\n",
    "print(f\"Index cr√©√© avec {index.ntotal} vecteurs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de recherche\n",
    "def search(query, model, index, documents, top_k=3):\n",
    "    \"\"\"\n",
    "    Recherche les documents les plus similaires √† une requ√™te.\n",
    "    \n",
    "    Args:\n",
    "        query: Texte de la requ√™te\n",
    "        model: Mod√®le d'embeddings\n",
    "        index: Index FAISS\n",
    "        documents: Liste des documents\n",
    "        top_k: Nombre de r√©sultats\n",
    "    \n",
    "    Returns:\n",
    "        Liste de (document, score)\n",
    "    \"\"\"\n",
    "    # Encoder la requ√™te\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Normaliser\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Rechercher\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Retourner les r√©sultats\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        results.append((documents[idx], score))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test\n",
    "query = \"Comment fonctionne l'attention dans les Transformers ?\"\n",
    "results = search(query, embedding_model, index, documents, top_k=3)\n",
    "\n",
    "print(f\"Requ√™te: '{query}'\\n\")\n",
    "print(\"R√©sultats:\")\n",
    "for doc, score in results:\n",
    "    print(f\"  [{score:.3f}] {doc['title']}\")\n",
    "    print(f\"           {doc['content'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. G√©n√©ration de R√©ponses\n",
    "\n",
    "On combine maintenant retrieval + g√©n√©ration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un mod√®le de g√©n√©ration\n",
    "print(\"Chargement du mod√®le de g√©n√©ration...\")\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "print(\"Mod√®le charg√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 3 : Impl√©menter le RAG complet\n",
    "# ============================================\n",
    "\n",
    "class SimpleRAG:\n",
    "    \"\"\"\n",
    "    Syst√®me RAG simple pour question-answering.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, documents, embedding_model, generator, top_k=3):\n",
    "        self.documents = documents\n",
    "        self.embedding_model = embedding_model\n",
    "        self.generator = generator\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Cr√©er les embeddings et l'index\n",
    "        print(\"Cr√©ation de l'index...\")\n",
    "        self.embeddings = self._create_embeddings()\n",
    "        self.index = self._create_index()\n",
    "        print(f\"Index pr√™t avec {len(documents)} documents.\")\n",
    "    \n",
    "    def _create_embeddings(self):\n",
    "        texts = [f\"{doc['title']}: {doc['content']}\" for doc in self.documents]\n",
    "        return self.embedding_model.encode(texts)\n",
    "    \n",
    "    def _create_index(self):\n",
    "        embeddings = self.embeddings.copy()\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "        index.add(embeddings)\n",
    "        return index\n",
    "    \n",
    "    def retrieve(self, query):\n",
    "        \"\"\"R√©cup√®re les documents pertinents.\"\"\"\n",
    "        query_emb = self.embedding_model.encode([query])\n",
    "        faiss.normalize_L2(query_emb)\n",
    "        scores, indices = self.index.search(query_emb, self.top_k)\n",
    "        \n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            results.append({\n",
    "                'document': self.documents[idx],\n",
    "                'score': score\n",
    "            })\n",
    "        return results\n",
    "    \n",
    "    def generate_answer(self, query, context_docs):\n",
    "        \"\"\"G√©n√®re une r√©ponse bas√©e sur le contexte.\"\"\"\n",
    "        \n",
    "        # TODO: Construire le prompt\n",
    "        # Format: \"Contexte: ... Question: ... R√©ponse:\"\n",
    "        \n",
    "        # Construire le contexte\n",
    "        context_parts = []\n",
    "        for doc_info in context_docs:\n",
    "            doc = doc_info['document']\n",
    "            context_parts.append(f\"- {doc['title']}: {doc['content']}\")\n",
    "        \n",
    "        context = \"\\n\".join(context_parts)\n",
    "        \n",
    "        # Construire le prompt\n",
    "        prompt = f\"\"\"R√©ponds √† la question en utilisant uniquement les informations du contexte.\n",
    "\n",
    "Contexte:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "R√©ponse:\"\"\"\n",
    "        \n",
    "        # G√©n√©rer\n",
    "        response = self.generator(\n",
    "            prompt,\n",
    "            max_length=200,\n",
    "            num_return_sequences=1\n",
    "        )[0]['generated_text']\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def answer(self, query, verbose=True):\n",
    "        \"\"\"\n",
    "        Pipeline complet : retrieve + generate.\n",
    "        \n",
    "        Args:\n",
    "            query: Question de l'utilisateur\n",
    "            verbose: Afficher les d√©tails\n",
    "        \n",
    "        Returns:\n",
    "            dict avec 'answer', 'sources', 'query'\n",
    "        \"\"\"\n",
    "        # 1. Retrieve\n",
    "        retrieved = self.retrieve(query)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Question: {query}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"\\nDocuments trouv√©s:\")\n",
    "            for r in retrieved:\n",
    "                print(f\"  [{r['score']:.3f}] {r['document']['title']}\")\n",
    "        \n",
    "        # 2. Generate\n",
    "        answer = self.generate_answer(query, retrieved)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nR√©ponse: {answer}\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'sources': [r['document']['title'] for r in retrieved]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le syst√®me RAG\n",
    "rag = SimpleRAG(\n",
    "    documents=documents,\n",
    "    embedding_model=embedding_model,\n",
    "    generator=generator,\n",
    "    top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "questions = [\n",
    "    \"Qu'est-ce que l'attention dans les Transformers ?\",\n",
    "    \"Quelle est la diff√©rence entre BERT et GPT ?\",\n",
    "    \"Comment fonctionne le fine-tuning ?\",\n",
    "    \"Qu'est-ce que le RAG et pourquoi est-ce utile ?\",\n",
    "    \"Combien de param√®tres a GPT-3 ?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    result = rag.answer(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. √âvaluation et Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser la similarit√© entre documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculer la matrice de similarit√©\n",
    "sim_matrix = cosine_similarity(doc_embeddings)\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(sim_matrix, cmap='Blues')\n",
    "plt.colorbar(label='Similarit√© cosinus')\n",
    "\n",
    "# Labels\n",
    "titles = [doc['title'][:15] + '...' if len(doc['title']) > 15 else doc['title'] \n",
    "          for doc in documents]\n",
    "plt.xticks(range(len(documents)), titles, rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(range(len(documents)), titles, fontsize=8)\n",
    "\n",
    "plt.title('Similarit√© entre documents')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation t-SNE des documents\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# R√©duire en 2D\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(doc_embeddings)\n",
    "\n",
    "# Cat√©gories manuelles pour la couleur\n",
    "categories = {\n",
    "    'Architecture': [0, 1, 2, 3],\n",
    "    'Mod√®les': [4, 5, 6],\n",
    "    'Entra√Ænement': [7, 8, 9],\n",
    "    'Applications': [10, 11, 12, 13, 14]\n",
    "}\n",
    "\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "color_map = {}\n",
    "for i, (cat, indices) in enumerate(categories.items()):\n",
    "    for idx in indices:\n",
    "        color_map[idx] = colors[i]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, (x, y) in enumerate(embeddings_2d):\n",
    "    plt.scatter(x, y, c=color_map.get(i, 'gray'), s=100)\n",
    "    plt.annotate(documents[i]['title'][:20], (x, y), fontsize=8)\n",
    "\n",
    "# L√©gende\n",
    "for cat, color in zip(categories.keys(), colors):\n",
    "    plt.scatter([], [], c=color, label=cat)\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Documents dans l\\'espace des embeddings (t-SNE)')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Am√©liorations possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 4 : Ajouter le chunking\n",
    "# ============================================\n",
    "\n",
    "def chunk_document(doc, chunk_size=200, overlap=50):\n",
    "    \"\"\"\n",
    "    D√©coupe un document en chunks plus petits.\n",
    "    \n",
    "    Args:\n",
    "        doc: Dict avec 'content'\n",
    "        chunk_size: Taille max d'un chunk (en mots)\n",
    "        overlap: Chevauchement entre chunks\n",
    "    \n",
    "    Returns:\n",
    "        Liste de chunks (dicts)\n",
    "    \"\"\"\n",
    "    # TODO: Impl√©menter le chunking\n",
    "    \n",
    "    words = doc['content'].split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk_words = words[i:i + chunk_size]\n",
    "        if len(chunk_words) < 20:  # Ignorer les chunks trop petits\n",
    "            continue\n",
    "            \n",
    "        chunk = {\n",
    "            'id': f\"{doc['id']}_chunk_{len(chunks)}\",\n",
    "            'title': doc['title'],\n",
    "            'content': ' '.join(chunk_words),\n",
    "            'parent_id': doc['id']\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    # Si le document est petit, le garder tel quel\n",
    "    if len(chunks) == 0:\n",
    "        chunks.append(doc)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test\n",
    "test_doc = documents[0]\n",
    "chunks = chunk_document(test_doc, chunk_size=30, overlap=10)\n",
    "print(f\"Document original: {len(test_doc['content'].split())} mots\")\n",
    "print(f\"Chunks cr√©√©s: {len(chunks)}\")\n",
    "for c in chunks:\n",
    "    print(f\"  - {len(c['content'].split())} mots: {c['content'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interface interactive\n",
    "def demo_rag():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RAG - Question Answering sur les Transformers\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Posez vos questions sur les Transformers !\")\n",
    "    print(\"Tapez 'quit' pour quitter\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nQuestion: \")\n",
    "        if query.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        result = rag.answer(query)\n",
    "\n",
    "# D√©commenter pour tester\n",
    "# demo_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. R√©capitulatif\n",
    "\n",
    "### Ce que vous avez appris\n",
    "\n",
    "1. **RAG** : Combiner recherche documentaire et g√©n√©ration\n",
    "2. **Embeddings** : Repr√©senter textes et requ√™tes dans le m√™me espace\n",
    "3. **FAISS** : Index vectoriel pour recherche rapide\n",
    "4. **Pipeline** : Retrieve ‚Üí Augment ‚Üí Generate\n",
    "\n",
    "### Architecture RAG en production\n",
    "\n",
    "| Composant | Options populaires |\n",
    "|-----------|--------------------|\n",
    "| Embeddings | OpenAI, Cohere, Sentence-BERT |\n",
    "| Vector DB | Pinecone, Weaviate, Chroma, FAISS |\n",
    "| LLM | GPT-4, Claude, Llama, Mistral |\n",
    "| Framework | LangChain, LlamaIndex |\n",
    "\n",
    "### Am√©liorations possibles\n",
    "\n",
    "- **Chunking intelligent** : D√©couper par paragraphes/sections\n",
    "- **Re-ranking** : R√©ordonner les r√©sultats avec un mod√®le cross-encoder\n",
    "- **Hybrid search** : Combiner recherche s√©mantique et lexicale (BM25)\n",
    "- **Query expansion** : Reformuler la question pour am√©liorer le recall\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Qualit√© d√©pend de la base documentaire\n",
    "- Latence ajout√©e par la recherche\n",
    "- Le LLM peut mal interpr√©ter le contexte\n",
    "- Pas de raisonnement multi-√©tapes complexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espace pour vos exp√©rimentations\n",
    "\n",
    "# Ajoutez vos propres documents !\n",
    "# nouveau_doc = {\n",
    "#     \"id\": 100,\n",
    "#     \"title\": \"Mon sujet\",\n",
    "#     \"content\": \"...\"\n",
    "# }\n",
    "\n",
    "# Essayez diff√©rentes questions\n",
    "ma_question = \"Comment BERT est-il entra√Æn√© ?\"\n",
    "result = rag.answer(ma_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}