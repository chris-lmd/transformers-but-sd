{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet D - RAG : Question-Answering sur Documents\n",
    "\n",
    "**Module** : Réseaux de Neurones Approfondissement  \n",
    "**Durée** : 2h  \n",
    "**Objectif** : Construire un système de question-answering avec RAG\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs du projet\n",
    "\n",
    "Dans ce projet, vous allez :\n",
    "1. Comprendre le principe du RAG (Retrieval-Augmented Generation)\n",
    "2. Créer un index de documents avec des embeddings\n",
    "3. Implémenter la recherche sémantique\n",
    "4. Combiner retrieval + génération pour répondre à des questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers sentence-transformers matplotlib numpy tqdm faiss-cpu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import faiss\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Pourquoi le RAG ?\n",
    "\n",
    "### Le problème des LLMs\n",
    "\n",
    "Les modèles de langage (GPT, BERT...) ont des **limitations** :\n",
    "\n",
    "| Limitation | Exemple |\n",
    "|------------|--------|\n",
    "| **Connaissance figée** | Ne connaît pas les événements après son entraînement |\n",
    "| **Hallucinations** | Invente des faits avec confiance |\n",
    "| **Pas de sources** | Ne peut pas citer d'où vient l'information |\n",
    "| **Données privées** | Ne connaît pas vos documents internes |\n",
    "\n",
    "### La solution : RAG\n",
    "\n",
    "**RAG** = Retrieval-Augmented Generation\n",
    "\n",
    "```\n",
    "Question: \"Quelle est la politique de congés de l'entreprise ?\"\n",
    "        ↓\n",
    "    [RETRIEVER]\n",
    "    Cherche dans la base documentaire\n",
    "        ↓\n",
    "    Documents pertinents trouvés:\n",
    "    - \"Article 5.2: Les employés ont droit à 25 jours...\"\n",
    "    - \"Note RH: Les congés doivent être posés 2 semaines avant...\"\n",
    "        ↓\n",
    "    [GENERATOR]\n",
    "    LLM génère une réponse basée sur les documents\n",
    "        ↓\n",
    "    Réponse: \"Selon l'article 5.2, vous avez droit à 25 jours de congés.\n",
    "              Les congés doivent être posés 2 semaines à l'avance.\"\n",
    "```\n",
    "\n",
    "### Architecture RAG\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                         RAG SYSTEM                          │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  Question ──► [Encoder] ──► Query Embedding                 │\n",
    "│                                    │                        │\n",
    "│                                    ▼                        │\n",
    "│              ┌─────────────────────────────┐                │\n",
    "│              │     INDEX VECTORIEL         │                │\n",
    "│              │  (embeddings des documents) │                │\n",
    "│              └─────────────────────────────┘                │\n",
    "│                                    │                        │\n",
    "│                                    ▼                        │\n",
    "│              Top-K documents les plus similaires            │\n",
    "│                                    │                        │\n",
    "│                                    ▼                        │\n",
    "│  ┌─────────────────────────────────────────────────┐        │\n",
    "│  │ Prompt = Question + Contexte (documents trouvés)│        │\n",
    "│  └─────────────────────────────────────────────────┘        │\n",
    "│                                    │                        │\n",
    "│                                    ▼                        │\n",
    "│                              [LLM / Generator]              │\n",
    "│                                    │                        │\n",
    "│                                    ▼                        │\n",
    "│                               Réponse                       │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Préparation des Documents\n",
    "\n",
    "On va créer une base de connaissances sur les **Transformers** (le sujet de ce cours !)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base de connaissances (documents)\n",
    "documents = [\n",
    "    # Architecture\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"title\": \"Architecture Transformer\",\n",
    "        \"content\": \"Le Transformer est une architecture de réseau de neurones introduite en 2017 dans le papier 'Attention Is All You Need'. Il se compose d'un encodeur et d'un décodeur, tous deux basés sur le mécanisme d'attention. L'encodeur traite la séquence d'entrée en parallèle, tandis que le décodeur génère la séquence de sortie de manière autoregressive.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"title\": \"Mécanisme d'Attention\",\n",
    "        \"content\": \"L'attention permet à chaque position de la séquence de 'regarder' toutes les autres positions. Elle utilise trois vecteurs : Query (Q), Key (K) et Value (V). Le score d'attention est calculé par le produit scalaire Q·K, normalisé par la racine de la dimension, puis passé dans un softmax. La formule est : Attention(Q,K,V) = softmax(QK^T/√d_k)V.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"title\": \"Multi-Head Attention\",\n",
    "        \"content\": \"Le Multi-Head Attention utilise plusieurs 'têtes' d'attention en parallèle. Chaque tête peut apprendre à détecter différents types de relations : syntaxiques, sémantiques, de proximité, etc. Les sorties des têtes sont concaténées puis projetées. BERT-base utilise 12 têtes, GPT-3 en utilise 96.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"title\": \"Positional Encoding\",\n",
    "        \"content\": \"Comme le Transformer traite les tokens en parallèle, il n'a pas de notion d'ordre. Le Positional Encoding ajoute une information de position à chaque embedding. La version originale utilise des fonctions sinusoïdales : PE(pos,2i) = sin(pos/10000^(2i/d)) et PE(pos,2i+1) = cos(pos/10000^(2i/d)). Les modèles récents utilisent souvent des embeddings de position appris.\"\n",
    "    },\n",
    "    # Modèles\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"title\": \"BERT\",\n",
    "        \"content\": \"BERT (Bidirectional Encoder Representations from Transformers) est un modèle créé par Google en 2018. Il utilise uniquement l'encodeur du Transformer et est pré-entraîné avec deux tâches : Masked Language Modeling (MLM) où 15% des tokens sont masqués, et Next Sentence Prediction (NSP). BERT-base a 110M de paramètres et 12 couches.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"title\": \"GPT\",\n",
    "        \"content\": \"GPT (Generative Pre-trained Transformer) est développé par OpenAI. Contrairement à BERT, GPT utilise uniquement le décodeur avec un masque causal : chaque token ne peut voir que les tokens précédents. GPT est entraîné à prédire le token suivant. GPT-3 a 175 milliards de paramètres et GPT-4 est estimé à plus de 1000 milliards.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"title\": \"T5\",\n",
    "        \"content\": \"T5 (Text-to-Text Transfer Transformer) de Google reformule toutes les tâches NLP en génération de texte. Par exemple, la classification devient : 'classify: This movie is great' → 'positive'. T5 utilise l'architecture encodeur-décodeur complète. Le modèle T5-11B a 11 milliards de paramètres.\"\n",
    "    },\n",
    "    # Entraînement\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"title\": \"Pré-entraînement\",\n",
    "        \"content\": \"Le pré-entraînement consiste à entraîner un modèle sur une grande quantité de texte non annoté avec des tâches auto-supervisées. Pour BERT c'est le MLM, pour GPT c'est la prédiction du token suivant. Cette phase capture les connaissances linguistiques générales et nécessite d'énormes ressources de calcul.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 9,\n",
    "        \"title\": \"Fine-tuning\",\n",
    "        \"content\": \"Le fine-tuning adapte un modèle pré-entraîné à une tâche spécifique. On ajoute une tête de classification et on entraîne sur un dataset annoté. Le fine-tuning nécessite beaucoup moins de données et de calcul que le pré-entraînement. Typiquement quelques heures sur un GPU contre des semaines pour le pré-entraînement.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"title\": \"Tokenization\",\n",
    "        \"content\": \"La tokenization découpe le texte en unités (tokens). Les méthodes modernes comme BPE (Byte Pair Encoding) ou WordPiece créent un vocabulaire de sous-mots. Par exemple 'unhappiness' devient ['un', '##happy', '##ness']. Cela permet de gérer les mots rares tout en gardant un vocabulaire de taille raisonnable (30k-50k tokens).\"\n",
    "    },\n",
    "    # Applications\n",
    "    {\n",
    "        \"id\": 11,\n",
    "        \"title\": \"Classification de texte\",\n",
    "        \"content\": \"Pour la classification avec BERT, on utilise le token [CLS] qui résume toute la séquence. On ajoute une couche linéaire qui projette l'embedding [CLS] vers le nombre de classes. Pour GPT, on peut utiliser le dernier token ou faire une moyenne des embeddings.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 12,\n",
    "        \"title\": \"Question Answering\",\n",
    "        \"content\": \"En Question Answering extractif, le modèle doit trouver la réponse dans un contexte donné. Le modèle prédit deux positions : le début et la fin de la réponse dans le texte. On entraîne avec des datasets comme SQuAD qui contient 100k+ paires question-réponse avec leur contexte.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 13,\n",
    "        \"title\": \"Génération de texte\",\n",
    "        \"content\": \"La génération de texte avec GPT est autoregressive : on prédit un token, on l'ajoute à l'entrée, puis on prédit le suivant. La température contrôle la 'créativité' : température basse = plus déterministe, température haute = plus aléatoire. Le top-k et top-p (nucleus sampling) limitent les tokens candidats.\"\n",
    "    },\n",
    "    # Concepts avancés\n",
    "    {\n",
    "        \"id\": 14,\n",
    "        \"title\": \"RAG\",\n",
    "        \"content\": \"RAG (Retrieval-Augmented Generation) combine recherche documentaire et génération. Au lieu de tout stocker dans les paramètres du modèle, on cherche les documents pertinents dans une base externe puis on les donne au LLM comme contexte. Cela réduit les hallucinations et permet de citer les sources.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 15,\n",
    "        \"title\": \"Embeddings\",\n",
    "        \"content\": \"Les embeddings sont des représentations vectorielles denses. Les mots similaires ont des embeddings proches dans l'espace vectoriel. Sentence-BERT produit des embeddings de phrases entières, utiles pour la recherche sémantique. La similarité cosinus mesure la proximité entre deux embeddings.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Base de connaissances: {len(documents)} documents\")\n",
    "for doc in documents[:3]:\n",
    "    print(f\"  - {doc['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Création des Embeddings\n",
    "\n",
    "On utilise **Sentence-BERT** pour créer des embeddings de documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle d'embeddings\n",
    "print(\"Chargement du modèle d'embeddings...\")\n",
    "embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "print(f\"Modèle chargé ! Dimension des embeddings: {embedding_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Créer les embeddings des documents\n",
    "# ============================================\n",
    "\n",
    "def create_document_embeddings(documents, model):\n",
    "    \"\"\"\n",
    "    Crée les embeddings pour chaque document.\n",
    "    \n",
    "    Args:\n",
    "        documents: Liste de dicts avec 'title' et 'content'\n",
    "        model: Modèle SentenceTransformer\n",
    "    \n",
    "    Returns:\n",
    "        embeddings: np.array de shape (n_docs, embed_dim)\n",
    "    \"\"\"\n",
    "    # TODO: Créer les embeddings\n",
    "    # On combine title + content pour chaque document\n",
    "    \n",
    "    texts = []  # Liste des textes à encoder\n",
    "    for doc in documents:\n",
    "        # Combiner titre et contenu\n",
    "        text = f\"{doc['title']}: {doc['content']}\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    # Encoder avec le modèle\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Créer les embeddings\n",
    "doc_embeddings = create_document_embeddings(documents, embedding_model)\n",
    "print(f\"Embeddings shape: {doc_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Index Vectoriel avec FAISS\n",
    "\n",
    "**FAISS** (Facebook AI Similarity Search) permet de faire des recherches rapides dans de grands ensembles de vecteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 2 : Créer l'index FAISS\n",
    "# ============================================\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Crée un index FAISS pour la recherche de similarité.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: np.array de shape (n_docs, embed_dim)\n",
    "    \n",
    "    Returns:\n",
    "        index: Index FAISS\n",
    "    \"\"\"\n",
    "    # TODO: Créer l'index\n",
    "    \n",
    "    # Dimension des embeddings\n",
    "    dim = embeddings.shape[1]\n",
    "    \n",
    "    # Créer un index avec similarité cosinus\n",
    "    # On normalise les vecteurs et on utilise IndexFlatIP (Inner Product)\n",
    "    \n",
    "    # Normaliser les embeddings (pour que IP = cosine similarity)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    # Créer l'index\n",
    "    index = faiss.IndexFlatIP(dim)  # IP = Inner Product\n",
    "    \n",
    "    # Ajouter les vecteurs\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "# Créer l'index\n",
    "index = create_faiss_index(doc_embeddings.copy())  # copy car normalize_L2 modifie in-place\n",
    "print(f\"Index créé avec {index.ntotal} vecteurs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de recherche\n",
    "def search(query, model, index, documents, top_k=3):\n",
    "    \"\"\"\n",
    "    Recherche les documents les plus similaires à une requête.\n",
    "    \n",
    "    Args:\n",
    "        query: Texte de la requête\n",
    "        model: Modèle d'embeddings\n",
    "        index: Index FAISS\n",
    "        documents: Liste des documents\n",
    "        top_k: Nombre de résultats\n",
    "    \n",
    "    Returns:\n",
    "        Liste de (document, score)\n",
    "    \"\"\"\n",
    "    # Encoder la requête\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Normaliser\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Rechercher\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Retourner les résultats\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        results.append((documents[idx], score))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test\n",
    "query = \"Comment fonctionne l'attention dans les Transformers ?\"\n",
    "results = search(query, embedding_model, index, documents, top_k=3)\n",
    "\n",
    "print(f\"Requête: '{query}'\\n\")\n",
    "print(\"Résultats:\")\n",
    "for doc, score in results:\n",
    "    print(f\"  [{score:.3f}] {doc['title']}\")\n",
    "    print(f\"           {doc['content'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Génération de Réponses\n",
    "\n",
    "On combine maintenant retrieval + génération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un modèle de génération\n",
    "print(\"Chargement du modèle de génération...\")\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "print(\"Modèle chargé !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 3 : Implémenter le RAG complet\n",
    "# ============================================\n",
    "\n",
    "class SimpleRAG:\n",
    "    \"\"\"\n",
    "    Système RAG simple pour question-answering.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, documents, embedding_model, generator, top_k=3):\n",
    "        self.documents = documents\n",
    "        self.embedding_model = embedding_model\n",
    "        self.generator = generator\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Créer les embeddings et l'index\n",
    "        print(\"Création de l'index...\")\n",
    "        self.embeddings = self._create_embeddings()\n",
    "        self.index = self._create_index()\n",
    "        print(f\"Index prêt avec {len(documents)} documents.\")\n",
    "    \n",
    "    def _create_embeddings(self):\n",
    "        texts = [f\"{doc['title']}: {doc['content']}\" for doc in self.documents]\n",
    "        return self.embedding_model.encode(texts)\n",
    "    \n",
    "    def _create_index(self):\n",
    "        embeddings = self.embeddings.copy()\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "        index.add(embeddings)\n",
    "        return index\n",
    "    \n",
    "    def retrieve(self, query):\n",
    "        \"\"\"Récupère les documents pertinents.\"\"\"\n",
    "        query_emb = self.embedding_model.encode([query])\n",
    "        faiss.normalize_L2(query_emb)\n",
    "        scores, indices = self.index.search(query_emb, self.top_k)\n",
    "        \n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            results.append({\n",
    "                'document': self.documents[idx],\n",
    "                'score': score\n",
    "            })\n",
    "        return results\n",
    "    \n",
    "    def generate_answer(self, query, context_docs):\n",
    "        \"\"\"Génère une réponse basée sur le contexte.\"\"\"\n",
    "        \n",
    "        # TODO: Construire le prompt\n",
    "        # Format: \"Contexte: ... Question: ... Réponse:\"\n",
    "        \n",
    "        # Construire le contexte\n",
    "        context_parts = []\n",
    "        for doc_info in context_docs:\n",
    "            doc = doc_info['document']\n",
    "            context_parts.append(f\"- {doc['title']}: {doc['content']}\")\n",
    "        \n",
    "        context = \"\\n\".join(context_parts)\n",
    "        \n",
    "        # Construire le prompt\n",
    "        prompt = f\"\"\"Réponds à la question en utilisant uniquement les informations du contexte.\n",
    "\n",
    "Contexte:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Réponse:\"\"\"\n",
    "        \n",
    "        # Générer\n",
    "        response = self.generator(\n",
    "            prompt,\n",
    "            max_length=200,\n",
    "            num_return_sequences=1\n",
    "        )[0]['generated_text']\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def answer(self, query, verbose=True):\n",
    "        \"\"\"\n",
    "        Pipeline complet : retrieve + generate.\n",
    "        \n",
    "        Args:\n",
    "            query: Question de l'utilisateur\n",
    "            verbose: Afficher les détails\n",
    "        \n",
    "        Returns:\n",
    "            dict avec 'answer', 'sources', 'query'\n",
    "        \"\"\"\n",
    "        # 1. Retrieve\n",
    "        retrieved = self.retrieve(query)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Question: {query}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"\\nDocuments trouvés:\")\n",
    "            for r in retrieved:\n",
    "                print(f\"  [{r['score']:.3f}] {r['document']['title']}\")\n",
    "        \n",
    "        # 2. Generate\n",
    "        answer = self.generate_answer(query, retrieved)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nRéponse: {answer}\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'sources': [r['document']['title'] for r in retrieved]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le système RAG\n",
    "rag = SimpleRAG(\n",
    "    documents=documents,\n",
    "    embedding_model=embedding_model,\n",
    "    generator=generator,\n",
    "    top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "questions = [\n",
    "    \"Qu'est-ce que l'attention dans les Transformers ?\",\n",
    "    \"Quelle est la différence entre BERT et GPT ?\",\n",
    "    \"Comment fonctionne le fine-tuning ?\",\n",
    "    \"Qu'est-ce que le RAG et pourquoi est-ce utile ?\",\n",
    "    \"Combien de paramètres a GPT-3 ?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    result = rag.answer(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Évaluation et Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser la similarité entre documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculer la matrice de similarité\n",
    "sim_matrix = cosine_similarity(doc_embeddings)\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(sim_matrix, cmap='Blues')\n",
    "plt.colorbar(label='Similarité cosinus')\n",
    "\n",
    "# Labels\n",
    "titles = [doc['title'][:15] + '...' if len(doc['title']) > 15 else doc['title'] \n",
    "          for doc in documents]\n",
    "plt.xticks(range(len(documents)), titles, rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(range(len(documents)), titles, fontsize=8)\n",
    "\n",
    "plt.title('Similarité entre documents')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation t-SNE des documents\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Réduire en 2D\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(doc_embeddings)\n",
    "\n",
    "# Catégories manuelles pour la couleur\n",
    "categories = {\n",
    "    'Architecture': [0, 1, 2, 3],\n",
    "    'Modèles': [4, 5, 6],\n",
    "    'Entraînement': [7, 8, 9],\n",
    "    'Applications': [10, 11, 12, 13, 14]\n",
    "}\n",
    "\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "color_map = {}\n",
    "for i, (cat, indices) in enumerate(categories.items()):\n",
    "    for idx in indices:\n",
    "        color_map[idx] = colors[i]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, (x, y) in enumerate(embeddings_2d):\n",
    "    plt.scatter(x, y, c=color_map.get(i, 'gray'), s=100)\n",
    "    plt.annotate(documents[i]['title'][:20], (x, y), fontsize=8)\n",
    "\n",
    "# Légende\n",
    "for cat, color in zip(categories.keys(), colors):\n",
    "    plt.scatter([], [], c=color, label=cat)\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Documents dans l\\'espace des embeddings (t-SNE)')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Améliorations possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 4 : Ajouter le chunking\n",
    "# ============================================\n",
    "\n",
    "def chunk_document(doc, chunk_size=200, overlap=50):\n",
    "    \"\"\"\n",
    "    Découpe un document en chunks plus petits.\n",
    "    \n",
    "    Args:\n",
    "        doc: Dict avec 'content'\n",
    "        chunk_size: Taille max d'un chunk (en mots)\n",
    "        overlap: Chevauchement entre chunks\n",
    "    \n",
    "    Returns:\n",
    "        Liste de chunks (dicts)\n",
    "    \"\"\"\n",
    "    # TODO: Implémenter le chunking\n",
    "    \n",
    "    words = doc['content'].split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk_words = words[i:i + chunk_size]\n",
    "        if len(chunk_words) < 20:  # Ignorer les chunks trop petits\n",
    "            continue\n",
    "            \n",
    "        chunk = {\n",
    "            'id': f\"{doc['id']}_chunk_{len(chunks)}\",\n",
    "            'title': doc['title'],\n",
    "            'content': ' '.join(chunk_words),\n",
    "            'parent_id': doc['id']\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    # Si le document est petit, le garder tel quel\n",
    "    if len(chunks) == 0:\n",
    "        chunks.append(doc)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test\n",
    "test_doc = documents[0]\n",
    "chunks = chunk_document(test_doc, chunk_size=30, overlap=10)\n",
    "print(f\"Document original: {len(test_doc['content'].split())} mots\")\n",
    "print(f\"Chunks créés: {len(chunks)}\")\n",
    "for c in chunks:\n",
    "    print(f\"  - {len(c['content'].split())} mots: {c['content'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interface interactive\n",
    "def demo_rag():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RAG - Question Answering sur les Transformers\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Posez vos questions sur les Transformers !\")\n",
    "    print(\"Tapez 'quit' pour quitter\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nQuestion: \")\n",
    "        if query.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        result = rag.answer(query)\n",
    "\n",
    "# Décommenter pour tester\n",
    "# demo_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Récapitulatif\n",
    "\n",
    "### Ce que vous avez appris\n",
    "\n",
    "1. **RAG** : Combiner recherche documentaire et génération\n",
    "2. **Embeddings** : Représenter textes et requêtes dans le même espace\n",
    "3. **FAISS** : Index vectoriel pour recherche rapide\n",
    "4. **Pipeline** : Retrieve → Augment → Generate\n",
    "\n",
    "### Architecture RAG en production\n",
    "\n",
    "| Composant | Options populaires |\n",
    "|-----------|--------------------|\n",
    "| Embeddings | OpenAI, Cohere, Sentence-BERT |\n",
    "| Vector DB | Pinecone, Weaviate, Chroma, FAISS |\n",
    "| LLM | GPT-4, Claude, Llama, Mistral |\n",
    "| Framework | LangChain, LlamaIndex |\n",
    "\n",
    "### Améliorations possibles\n",
    "\n",
    "- **Chunking intelligent** : Découper par paragraphes/sections\n",
    "- **Re-ranking** : Réordonner les résultats avec un modèle cross-encoder\n",
    "- **Hybrid search** : Combiner recherche sémantique et lexicale (BM25)\n",
    "- **Query expansion** : Reformuler la question pour améliorer le recall\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Qualité dépend de la base documentaire\n",
    "- Latence ajoutée par la recherche\n",
    "- Le LLM peut mal interpréter le contexte\n",
    "- Pas de raisonnement multi-étapes complexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espace pour vos expérimentations\n",
    "\n",
    "# Ajoutez vos propres documents !\n",
    "# nouveau_doc = {\n",
    "#     \"id\": 100,\n",
    "#     \"title\": \"Mon sujet\",\n",
    "#     \"content\": \"...\"\n",
    "# }\n",
    "\n",
    "# Essayez différentes questions\n",
    "ma_question = \"Comment BERT est-il entraîné ?\"\n",
    "result = rag.answer(ma_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
