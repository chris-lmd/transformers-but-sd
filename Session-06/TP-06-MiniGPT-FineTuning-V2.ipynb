{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# TP 06 V2 : Fine-tuning GPT-2 - Version Optimis√©e\n\n**Objectif** : Fine-tuner GPT-2 fran√ßais pour g√©n√©rer des descriptions de Pok√©mon\n\n**Nouveaut√©s V2** :\n- Option de choix du mod√®le : small (rapide) vs base (meilleure qualit√©)\n- Option pour ajouter les noms de Pok√©mon au vocabulaire (smart initialization)\n- Option pour figer les couches basses (faster training)\n- Meilleur filtrage des donn√©es\n\n**Dur√©e** : 2h"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Colab)\n",
    "!pip install -q transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Reproductibilit√©\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Configuration des options d'entra√Ænement\n",
    "\n",
    "### Nouvelles options V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n#                    OPTIONS D'ENTRA√éNEMENT V2\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n# Option 1 : Choisir le mod√®le GPT-2 fran√ßais\n# - \"small\" : asi/gpt-fr-cased-small (~124M params) - Plus rapide\n# - \"base\"  : asi/gpt-fr-cased-base (~124M params)  - Meilleure qualit√© (perplexit√© 8x meilleure)\nMODEL_SIZE = \"base\"  # \"small\" ou \"base\"\n\n# Option 2 : Ajouter les noms de Pok√©mon au vocabulaire\n# Les nouveaux tokens seront initialis√©s avec l'embedding de \"Pok√©mon\" ou \"animal\"\n# Avantage : Le mod√®le sait d√©j√† que \"Pikachu\" est un Pok√©mon\nADD_POKEMON_TOKENS = True\n\n# Option 3 : Figer les couches basses du mod√®le\n# Seules les couches hautes seront entra√Æn√©es\n# Avantage : Plus rapide, moins d'overfitting\nFREEZE_LOWER_LAYERS = True\nNUM_LAYERS_TO_FREEZE = 6  # Sur 12 couches, figer les 6 premi√®res\n\n# Autres param√®tres\nMAX_SAMPLES = 5000      # Nombre d'articles pour le fine-tuning\nMAX_LENGTH = 256        # Longueur max des s√©quences\nNUM_EPOCHS = 3          # Nombre d'epochs\n\nprint(\"Configuration V2 :\")\nprint(f\"  - Mod√®le : {MODEL_SIZE}\")\nprint(f\"  - Ajouter tokens Pok√©mon : {ADD_POKEMON_TOKENS}\")\nprint(f\"  - Figer couches basses : {FREEZE_LOWER_LAYERS} ({NUM_LAYERS_TO_FREEZE} couches)\")\nprint(f\"  - Samples : {MAX_SAMPLES}\")\nprint(f\"  - Epochs : {NUM_EPOCHS}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Charger GPT-2 fran√ßais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Charger le tokenizer et le mod√®le\nmodel_name = f\"asi/gpt-fr-cased-{MODEL_SIZE}\"\n\nprint(f\"Chargement du mod√®le : {model_name}\")\nprint(\"Chargement du tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprint(\"Chargement du mod√®le...\")\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\nprint(f\"\\n‚úÖ Mod√®le charg√© !\")\nprint(f\"Param√®tres : {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Vocabulaire initial : {len(tokenizer):,} tokens\")\nprint(f\"Nombre de couches : {model.config.n_layer}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter un token de padding\n",
    "# GPT-2 n'a pas de token PAD natif, on r√©utilise EOS\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Option 1 : Ajouter les tokens Pok√©mon (Smart Initialization)\n",
    "\n",
    "### Pourquoi ?\n",
    "- \"Pikachu\" est tokenis√© en plusieurs sous-tokens par BPE\n",
    "- En ajoutant \"Pikachu\" comme token unique, initialis√© avec \"Pok√©mon\", le mod√®le sait d√©j√† que c'est un Pok√©mon\n",
    "- Apprentissage plus rapide et meilleure qualit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des noms de Pok√©mon √† ajouter\n",
    "# On charge depuis le dataset pokemon-names-fr\n",
    "print(\"Chargement des noms de Pok√©mon...\")\n",
    "names_dataset = load_dataset(\"chris-lmd/pokemon-names-fr\")\n",
    "POKEMON_NAMES = [item[\"name\"] for item in names_dataset[\"train\"]]\n",
    "print(f\"  {len(POKEMON_NAMES)} noms charg√©s\")\n",
    "print(f\"  Exemples : {POKEMON_NAMES[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pokemon_tokens_to_vocab(tokenizer, model, pokemon_names):\n",
    "    \"\"\"\n",
    "    Ajoute les noms de Pok√©mon au vocabulaire et initialise leurs embeddings.\n",
    "    \n",
    "    Les nouveaux tokens sont initialis√©s avec l'embedding de \"Pok√©mon\" ou \"animal\"\n",
    "    pour que le mod√®le sache d√©j√† qu'il s'agit de cr√©atures.\n",
    "    \"\"\"\n",
    "    # Trouver le token de r√©f√©rence pour l'initialisation\n",
    "    # On essaie \"Pok√©mon\", sinon \"Pokemon\", sinon \"animal\"\n",
    "    reference_tokens = [\"Pok√©mon\", \"Pokemon\", \"animal\", \"cr√©ature\"]\n",
    "    reference_id = None\n",
    "    reference_word = None\n",
    "    \n",
    "    for ref in reference_tokens:\n",
    "        tokens = tokenizer.encode(ref, add_special_tokens=False)\n",
    "        if len(tokens) == 1:  # Token unique trouv√©\n",
    "            reference_id = tokens[0]\n",
    "            reference_word = ref\n",
    "            break\n",
    "    \n",
    "    if reference_id is None:\n",
    "        # Fallback : utiliser le premier token de \"Pok√©mon\"\n",
    "        reference_id = tokenizer.encode(\"Pok√©mon\", add_special_tokens=False)[0]\n",
    "        reference_word = \"Pok√©mon (premier sous-token)\"\n",
    "    \n",
    "    print(f\"Token de r√©f√©rence : '{reference_word}' (id={reference_id})\")\n",
    "    \n",
    "    # Filtrer : garder seulement les noms qui ne sont pas d√©j√† des tokens uniques\n",
    "    new_tokens = []\n",
    "    for name in pokemon_names:\n",
    "        tokens = tokenizer.encode(name, add_special_tokens=False)\n",
    "        if len(tokens) > 1:  # Le nom est d√©coup√© en plusieurs tokens\n",
    "            new_tokens.append(name)\n",
    "    \n",
    "    print(f\"Tokens √† ajouter : {len(new_tokens)} (sur {len(pokemon_names)})\")\n",
    "    \n",
    "    if len(new_tokens) == 0:\n",
    "        print(\"Aucun nouveau token √† ajouter.\")\n",
    "        return 0\n",
    "    \n",
    "    # Sauvegarder l'embedding de r√©f√©rence AVANT le resize\n",
    "    with torch.no_grad():\n",
    "        reference_embedding = model.transformer.wte.weight[reference_id].clone()\n",
    "    \n",
    "    # Ajouter les tokens au vocabulaire\n",
    "    num_added = tokenizer.add_tokens(new_tokens)\n",
    "    print(f\"Tokens ajout√©s au vocabulaire : {num_added}\")\n",
    "    \n",
    "    # Redimensionner les embeddings du mod√®le\n",
    "    old_size = model.transformer.wte.weight.shape[0]\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    new_size = model.transformer.wte.weight.shape[0]\n",
    "    print(f\"Embeddings : {old_size:,} ‚Üí {new_size:,}\")\n",
    "    \n",
    "    # Initialiser les nouveaux tokens avec l'embedding de r√©f√©rence\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_added):\n",
    "            new_token_id = old_size + i\n",
    "            # Copier l'embedding de r√©f√©rence + petit bruit pour diff√©rencier\n",
    "            noise = torch.randn_like(reference_embedding) * 0.01\n",
    "            model.transformer.wte.weight[new_token_id] = reference_embedding + noise\n",
    "    \n",
    "    print(f\"‚úÖ {num_added} tokens initialis√©s avec l'embedding de '{reference_word}'\")\n",
    "    return num_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer si l'option est activ√©e\n",
    "if ADD_POKEMON_TOKENS:\n",
    "    print(\"‚ïê\" * 50)\n",
    "    print(\"Ajout des tokens Pok√©mon au vocabulaire...\")\n",
    "    print(\"‚ïê\" * 50)\n",
    "    num_added = add_pokemon_tokens_to_vocab(tokenizer, model, POKEMON_NAMES)\n",
    "    print()\n",
    "    \n",
    "    # V√©rification\n",
    "    test_name = \"Pikachu\"\n",
    "    tokens = tokenizer.encode(test_name, add_special_tokens=False)\n",
    "    print(f\"Test : '{test_name}' ‚Üí {tokens} ({len(tokens)} token(s))\")\n",
    "else:\n",
    "    print(\"Option ADD_POKEMON_TOKENS d√©sactiv√©e.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Option 2 : Figer les couches basses (Partial Freezing)\n",
    "\n",
    "### Pourquoi ?\n",
    "- Les couches basses capturent des features g√©n√©rales (grammaire, syntaxe)\n",
    "- Ces features sont d√©j√† bien apprises par le pr√©-entra√Ænement\n",
    "- On ne fine-tune que les couches hautes (s√©mantique, style)\n",
    "\n",
    "### Avantages\n",
    "- Entra√Ænement plus rapide (moins de gradients)\n",
    "- Moins d'overfitting\n",
    "- Pr√©serve les connaissances linguistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_lower_layers(model, num_layers_to_freeze):\n",
    "    \"\"\"\n",
    "    Fige les embeddings et les N premi√®res couches du transformer.\n",
    "    \n",
    "    Args:\n",
    "        model: Le mod√®le GPT-2\n",
    "        num_layers_to_freeze: Nombre de couches √† figer (depuis le bas)\n",
    "    \"\"\"\n",
    "    total_layers = model.config.n_layer\n",
    "    \n",
    "    if num_layers_to_freeze >= total_layers:\n",
    "        print(f\"‚ö†Ô∏è Attention : vous figez {num_layers_to_freeze} couches sur {total_layers} !\")\n",
    "        num_layers_to_freeze = total_layers - 1\n",
    "    \n",
    "    # Figer les embeddings (tokens et positions)\n",
    "    for param in model.transformer.wte.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.transformer.wpe.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Figer les N premi√®res couches\n",
    "    for i in range(num_layers_to_freeze):\n",
    "        for param in model.transformer.h[i].parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Compter les param√®tres\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    frozen_params = total_params - trainable_params\n",
    "    \n",
    "    print(f\"Couches fig√©es : {num_layers_to_freeze} / {total_layers}\")\n",
    "    print(f\"Param√®tres totaux : {total_params:,}\")\n",
    "    print(f\"Param√®tres fig√©s : {frozen_params:,} ({100*frozen_params/total_params:.1f}%)\")\n",
    "    print(f\"Param√®tres entra√Ænables : {trainable_params:,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "    \n",
    "    return trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer si l'option est activ√©e\n",
    "if FREEZE_LOWER_LAYERS:\n",
    "    print(\"‚ïê\" * 50)\n",
    "    print(\"Freezing des couches basses...\")\n",
    "    print(\"‚ïê\" * 50)\n",
    "    trainable = freeze_lower_layers(model, NUM_LAYERS_TO_FREEZE)\n",
    "else:\n",
    "    print(\"Option FREEZE_LOWER_LAYERS d√©sactiv√©e.\")\n",
    "    print(\"Tous les param√®tres seront entra√Æn√©s (full fine-tuning).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©placer le mod√®le sur GPU\n",
    "model = model.to(device)\n",
    "print(f\"\\nMod√®le d√©plac√© sur : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Test du mod√®le avant fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100, temperature=0.8, top_k=50):\n",
    "    \"\"\"G√©n√®re du texte √† partir d'un prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avant fine-tuning\n",
    "prompt = \"Pikachu est un Pok√©mon de type\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"G√©n√©ration (AVANT fine-tuning):\")\n",
    "print(generate_text(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Pr√©parer le dataset Pok√©mon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset\n",
    "print(\"Chargement du dataset Pok√©mon...\")\n",
    "dataset = load_dataset(\"chris-lmd/pokepedia-fr\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset charg√© !\")\n",
    "print(f\"Articles : {len(dataset['train']):,}\")\n",
    "print(f\"Colonnes : {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aper√ßu d'un article\n",
    "sample = dataset['train'][0]\n",
    "print(f\"Titre: {sample.get('title', 'N/A')}\")\n",
    "print(f\"Types: {sample.get('types', [])}\")\n",
    "print(f\"\\nContenu (500 premiers caract√®res):\")\n",
    "print(sample['content'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filtrage V2 am√©lior√© : prioriser les vrais Pok√©mon\n# Les vrais Pok√©mon ont au moins un type dans le champ 'types'\n\ndef has_pokemon_types(example):\n    \"\"\"Identifie les vrais Pok√©mon (ceux avec des types).\"\"\"\n    return len(example.get('types', [])) > 0\n\ndef has_pokemon_patterns(example):\n    \"\"\"Identifie les articles li√©s aux Pok√©mon (par patterns).\"\"\"\n    text = example['content'].lower()\n    return any(p in text for p in [\"est un pok√©mon\", \"√©volution\", \"capacit√©\", \"pok√©dex\"])\n\n# S√©parer les vrais Pok√©mon des autres articles li√©s\npokemon_dataset = dataset['train'].filter(has_pokemon_types)\nother_dataset = dataset['train'].filter(\n    lambda x: not has_pokemon_types(x) and has_pokemon_patterns(x)\n)\n\nprint(\"‚ïê\" * 50)\nprint(\"Analyse du dataset\")\nprint(\"‚ïê\" * 50)\nprint(f\"Total articles : {len(dataset['train']):,}\")\nprint(f\"Vrais Pok√©mon (avec types) : {len(pokemon_dataset):,}\")\nprint(f\"Autres articles Pok√©mon : {len(other_dataset):,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Constitution du train_dataset avec priorit√© aux vrais Pok√©mon\nfrom datasets import concatenate_datasets\n\nnum_pokemon = len(pokemon_dataset)\nnum_other = len(other_dataset)\n\nif num_pokemon >= MAX_SAMPLES:\n    # Assez de vrais Pok√©mon : on en prend MAX_SAMPLES al√©atoirement\n    train_dataset = pokemon_dataset.shuffle(seed=42).select(range(MAX_SAMPLES))\n    print(f\"\\n‚úÖ Train dataset : {MAX_SAMPLES} vrais Pok√©mon (√©chantillon al√©atoire)\")\nelse:\n    # Pas assez de vrais Pok√©mon : on prend tous + compl√©ment al√©atoire\n    remaining = MAX_SAMPLES - num_pokemon\n    \n    if remaining <= num_other:\n        # Assez d'autres articles pour compl√©ter\n        other_sample = other_dataset.shuffle(seed=42).select(range(remaining))\n    else:\n        # Prendre tous les autres articles disponibles\n        other_sample = other_dataset.shuffle(seed=42)\n        remaining = len(other_sample)\n    \n    # Combiner : tous les Pok√©mon + √©chantillon al√©atoire des autres\n    train_dataset = concatenate_datasets([pokemon_dataset, other_sample])\n    train_dataset = train_dataset.shuffle(seed=42)  # M√©langer le tout\n    \n    print(f\"\\n‚úÖ Train dataset : {len(train_dataset):,} articles\")\n    print(f\"   - Vrais Pok√©mon : {num_pokemon:,} (100%)\")\n    print(f\"   - Autres articles : {remaining:,} (compl√©ment al√©atoire)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize les textes avec troncature.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['content'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "print(\"Tokenization en cours...\")\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "# Ajouter les labels\n",
    "def add_labels(examples):\n",
    "    examples['labels'] = examples['input_ids'].copy()\n",
    "    return examples\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)\n",
    "print(f\"‚úÖ Tokenization termin√©e !\")\n",
    "print(f\"Colonnes : {tokenized_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-pokemon-v2\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # Hyperparam√®tres\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    # Optimiseur\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Misc\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Configuration :\")\n",
    "print(f\"  - Epochs : {training_args.num_train_epochs}\")\n",
    "print(f\"  - Batch size effectif : {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Learning rate : {training_args.learning_rate}\")\n",
    "print(f\"  - FP16 : {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer cr√©√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# R√©sum√© de la configuration\nprint(\"‚ïê\" * 60)\nprint(\"           R√âSUM√â DE LA CONFIGURATION V2\")\nprint(\"‚ïê\" * 60)\nprint(f\"\\nü§ñ Mod√®le : {model_name}\")\nprint(f\"üìä Dataset : {len(train_dataset):,} articles\")\nprint(f\"üìù Max length : {MAX_LENGTH} tokens\")\nprint(f\"üîÑ Epochs : {NUM_EPOCHS}\")\nprint(f\"\\nüéØ Options :\")\nprint(f\"   - Mod√®le : {MODEL_SIZE} ({'meilleure qualit√©' if MODEL_SIZE == 'base' else 'plus rapide'})\")\nprint(f\"   - Tokens Pok√©mon ajout√©s : {ADD_POKEMON_TOKENS}\")\nprint(f\"   - Couches fig√©es : {FREEZE_LOWER_LAYERS} ({NUM_LAYERS_TO_FREEZE if FREEZE_LOWER_LAYERS else 0}/{model.config.n_layer})\")\nprint(f\"\\nüíæ Vocabulaire : {len(tokenizer):,} tokens\")\n\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"üß† Param√®tres entra√Ænables : {trainable:,} / {total:,} ({100*trainable/total:.1f}%)\")\nprint(\"‚ïê\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancer le fine-tuning\n",
    "print(\"üöÄ Fine-tuning V2 en cours...\")\n",
    "print()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Fine-tuning termin√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le\n",
    "trainer.save_model(\"./gpt2-pokemon-v2-final\")\n",
    "tokenizer.save_pretrained(\"./gpt2-pokemon-v2-final\")\n",
    "print(\"‚úÖ Mod√®le V2 sauvegard√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. G√©n√©ration de descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(prompt, max_length=150, temperature=0.8, top_k=50, top_p=0.9):\n",
    "    \"\"\"G√©n√®re une description de Pok√©mon.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test : Pikachu\n",
    "prompt = \"Pikachu est un Pok√©mon de type\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nG√©n√©ration (APR√àS fine-tuning V2):\")\n",
    "print(generate_description(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test : Pok√©mon invent√©\n",
    "prompt = \"Flamador est un Pok√©mon de type Feu. Il\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nG√©n√©ration:\")\n",
    "print(generate_description(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test : √âvolution\n",
    "prompt = \"Dracaufeu √©volue √† partir de\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nG√©n√©ration:\")\n",
    "print(generate_description(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 9. Comparaison des configurations\n\nPour comparer l'effet des options, relancez le notebook avec diff√©rentes configurations :\n\n| Configuration | MODEL_SIZE | ADD_POKEMON_TOKENS | FREEZE_LOWER_LAYERS | R√©sultat attendu |\n|---------------|-----------|-------------------|---------------------|------------------|\n| Baseline | small | False | False | R√©f√©rence rapide |\n| + Base model | **base** | False | False | Meilleure qualit√© |\n| + Tokens | base | **True** | False | Meilleure connaissance des noms |\n| + Freeze | base | False | **True** | Plus rapide, moins d'overfitting |\n| Full V2 | base | True | True | Combinaison optimale |\n\n### Comparaison des mod√®les\n\n| Mod√®le | Perplexit√© | Taille | Recommandation |\n|--------|------------|--------|----------------|\n| `asi/gpt-fr-cased-small` | 109.2 | ~124M | Tests rapides |\n| `asi/gpt-fr-cased-base` | **12.9** | ~124M | **Production** |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 10. R√©capitulatif V2\n\n### Nouvelles techniques apprises\n\n1. **Choix du mod√®le** : Comparer small vs base selon le ratio qualit√©/vitesse\n2. **Smart Token Initialization** : Ajouter des tokens au vocabulaire et les initialiser intelligemment\n3. **Partial Freezing** : Figer les couches basses pour un entra√Ænement plus efficace\n4. **Meilleur filtrage** : Utiliser les m√©tadonn√©es (types) pour s√©lectionner les bons articles\n\n### Quand utiliser ces techniques ?\n\n| Technique | Quand l'utiliser |\n|-----------|------------------|\n| **Base model** | Besoin de qualit√©, GPU disponible |\n| **Small model** | Tests rapides, ressources limit√©es |\n| **Add tokens** | Vocabulaire sp√©cialis√© (noms propres, termes techniques) |\n| **Freeze layers** | Petit dataset, risque d'overfitting, GPU limit√© |\n| **Full fine-tuning** | Grand dataset, GPU puissant, besoin de flexibilit√© |"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}