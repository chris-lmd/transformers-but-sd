{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 06 V2 : Fine-tuning GPT-2 - Version Optimis√©e\n",
    "\n",
    "**Objectif** : Fine-tuner GPT-2 fran√ßais pour g√©n√©rer des descriptions de Pok√©mon\n",
    "\n",
    "**Nouveaut√©s V2** :\n",
    "- Option pour ajouter les noms de Pok√©mon au vocabulaire (smart initialization)\n",
    "- Option pour figer les couches basses (faster training)\n",
    "- Meilleur filtrage des donn√©es\n",
    "\n",
    "**Dur√©e** : 2h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Colab)\n",
    "!pip install -q transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Reproductibilit√©\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Configuration des options d'entra√Ænement\n",
    "\n",
    "### Nouvelles options V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "#                    OPTIONS D'ENTRA√éNEMENT V2\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Option 1 : Ajouter les noms de Pok√©mon au vocabulaire\n",
    "# Les nouveaux tokens seront initialis√©s avec l'embedding de \"Pok√©mon\" ou \"animal\"\n",
    "# Avantage : Le mod√®le sait d√©j√† que \"Pikachu\" est un Pok√©mon\n",
    "ADD_POKEMON_TOKENS = True\n",
    "\n",
    "# Option 2 : Figer les couches basses du mod√®le\n",
    "# Seules les couches hautes seront entra√Æn√©es\n",
    "# Avantage : Plus rapide, moins d'overfitting\n",
    "FREEZE_LOWER_LAYERS = True\n",
    "NUM_LAYERS_TO_FREEZE = 6  # Sur 12 couches, figer les 6 premi√®res\n",
    "\n",
    "# Autres param√®tres\n",
    "MAX_SAMPLES = 5000      # Nombre d'articles pour le fine-tuning\n",
    "MAX_LENGTH = 256        # Longueur max des s√©quences\n",
    "NUM_EPOCHS = 3          # Nombre d'epochs\n",
    "\n",
    "print(\"Configuration V2 :\")\n",
    "print(f\"  - Ajouter tokens Pok√©mon : {ADD_POKEMON_TOKENS}\")\n",
    "print(f\"  - Figer couches basses : {FREEZE_LOWER_LAYERS} ({NUM_LAYERS_TO_FREEZE} couches)\")\n",
    "print(f\"  - Samples : {MAX_SAMPLES}\")\n",
    "print(f\"  - Epochs : {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Charger GPT-2 fran√ßais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le tokenizer et le mod√®le\n",
    "model_name = \"asi/gpt-fr-cased-small\"\n",
    "\n",
    "print(\"Chargement du tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Chargement du mod√®le...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"\\n‚úÖ Mod√®le charg√© !\")\n",
    "print(f\"Param√®tres : {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Vocabulaire initial : {len(tokenizer):,} tokens\")\n",
    "print(f\"Nombre de couches : {model.config.n_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter un token de padding\n",
    "# GPT-2 n'a pas de token PAD natif, on r√©utilise EOS\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Option 1 : Ajouter les tokens Pok√©mon (Smart Initialization)\n",
    "\n",
    "### Pourquoi ?\n",
    "- \"Pikachu\" est tokenis√© en plusieurs sous-tokens par BPE\n",
    "- En ajoutant \"Pikachu\" comme token unique, initialis√© avec \"Pok√©mon\", le mod√®le sait d√©j√† que c'est un Pok√©mon\n",
    "- Apprentissage plus rapide et meilleure qualit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des noms de Pok√©mon √† ajouter\n",
    "# On charge depuis le dataset pokemon-names-fr\n",
    "print(\"Chargement des noms de Pok√©mon...\")\n",
    "names_dataset = load_dataset(\"chris-lmd/pokemon-names-fr\")\n",
    "POKEMON_NAMES = [item[\"name\"] for item in names_dataset[\"train\"]]\n",
    "print(f\"  {len(POKEMON_NAMES)} noms charg√©s\")\n",
    "print(f\"  Exemples : {POKEMON_NAMES[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pokemon_tokens_to_vocab(tokenizer, model, pokemon_names):\n",
    "    \"\"\"\n",
    "    Ajoute les noms de Pok√©mon au vocabulaire et initialise leurs embeddings.\n",
    "    \n",
    "    Les nouveaux tokens sont initialis√©s avec l'embedding de \"Pok√©mon\" ou \"animal\"\n",
    "    pour que le mod√®le sache d√©j√† qu'il s'agit de cr√©atures.\n",
    "    \"\"\"\n",
    "    # Trouver le token de r√©f√©rence pour l'initialisation\n",
    "    # On essaie \"Pok√©mon\", sinon \"Pokemon\", sinon \"animal\"\n",
    "    reference_tokens = [\"Pok√©mon\", \"Pokemon\", \"animal\", \"cr√©ature\"]\n",
    "    reference_id = None\n",
    "    reference_word = None\n",
    "    \n",
    "    for ref in reference_tokens:\n",
    "        tokens = tokenizer.encode(ref, add_special_tokens=False)\n",
    "        if len(tokens) == 1:  # Token unique trouv√©\n",
    "            reference_id = tokens[0]\n",
    "            reference_word = ref\n",
    "            break\n",
    "    \n",
    "    if reference_id is None:\n",
    "        # Fallback : utiliser le premier token de \"Pok√©mon\"\n",
    "        reference_id = tokenizer.encode(\"Pok√©mon\", add_special_tokens=False)[0]\n",
    "        reference_word = \"Pok√©mon (premier sous-token)\"\n",
    "    \n",
    "    print(f\"Token de r√©f√©rence : '{reference_word}' (id={reference_id})\")\n",
    "    \n",
    "    # Filtrer : garder seulement les noms qui ne sont pas d√©j√† des tokens uniques\n",
    "    new_tokens = []\n",
    "    for name in pokemon_names:\n",
    "        tokens = tokenizer.encode(name, add_special_tokens=False)\n",
    "        if len(tokens) > 1:  # Le nom est d√©coup√© en plusieurs tokens\n",
    "            new_tokens.append(name)\n",
    "    \n",
    "    print(f\"Tokens √† ajouter : {len(new_tokens)} (sur {len(pokemon_names)})\")\n",
    "    \n",
    "    if len(new_tokens) == 0:\n",
    "        print(\"Aucun nouveau token √† ajouter.\")\n",
    "        return 0\n",
    "    \n",
    "    # Sauvegarder l'embedding de r√©f√©rence AVANT le resize\n",
    "    with torch.no_grad():\n",
    "        reference_embedding = model.transformer.wte.weight[reference_id].clone()\n",
    "    \n",
    "    # Ajouter les tokens au vocabulaire\n",
    "    num_added = tokenizer.add_tokens(new_tokens)\n",
    "    print(f\"Tokens ajout√©s au vocabulaire : {num_added}\")\n",
    "    \n",
    "    # Redimensionner les embeddings du mod√®le\n",
    "    old_size = model.transformer.wte.weight.shape[0]\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    new_size = model.transformer.wte.weight.shape[0]\n",
    "    print(f\"Embeddings : {old_size:,} ‚Üí {new_size:,}\")\n",
    "    \n",
    "    # Initialiser les nouveaux tokens avec l'embedding de r√©f√©rence\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_added):\n",
    "            new_token_id = old_size + i\n",
    "            # Copier l'embedding de r√©f√©rence + petit bruit pour diff√©rencier\n",
    "            noise = torch.randn_like(reference_embedding) * 0.01\n",
    "            model.transformer.wte.weight[new_token_id] = reference_embedding + noise\n",
    "    \n",
    "    print(f\"‚úÖ {num_added} tokens initialis√©s avec l'embedding de '{reference_word}'\")\n",
    "    return num_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer si l'option est activ√©e\n",
    "if ADD_POKEMON_TOKENS:\n",
    "    print(\"‚ïê\" * 50)\n",
    "    print(\"Ajout des tokens Pok√©mon au vocabulaire...\")\n",
    "    print(\"‚ïê\" * 50)\n",
    "    num_added = add_pokemon_tokens_to_vocab(tokenizer, model, POKEMON_NAMES)\n",
    "    print()\n",
    "    \n",
    "    # V√©rification\n",
    "    test_name = \"Pikachu\"\n",
    "    tokens = tokenizer.encode(test_name, add_special_tokens=False)\n",
    "    print(f\"Test : '{test_name}' ‚Üí {tokens} ({len(tokens)} token(s))\")\n",
    "else:\n",
    "    print(\"Option ADD_POKEMON_TOKENS d√©sactiv√©e.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Option 2 : Figer les couches basses (Partial Freezing)\n",
    "\n",
    "### Pourquoi ?\n",
    "- Les couches basses capturent des features g√©n√©rales (grammaire, syntaxe)\n",
    "- Ces features sont d√©j√† bien apprises par le pr√©-entra√Ænement\n",
    "- On ne fine-tune que les couches hautes (s√©mantique, style)\n",
    "\n",
    "### Avantages\n",
    "- Entra√Ænement plus rapide (moins de gradients)\n",
    "- Moins d'overfitting\n",
    "- Pr√©serve les connaissances linguistiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_lower_layers(model, num_layers_to_freeze):\n",
    "    \"\"\"\n",
    "    Fige les embeddings et les N premi√®res couches du transformer.\n",
    "    \n",
    "    Args:\n",
    "        model: Le mod√®le GPT-2\n",
    "        num_layers_to_freeze: Nombre de couches √† figer (depuis le bas)\n",
    "    \"\"\"\n",
    "    total_layers = model.config.n_layer\n",
    "    \n",
    "    if num_layers_to_freeze >= total_layers:\n",
    "        print(f\"‚ö†Ô∏è Attention : vous figez {num_layers_to_freeze} couches sur {total_layers} !\")\n",
    "        num_layers_to_freeze = total_layers - 1\n",
    "    \n",
    "    # Figer les embeddings (tokens et positions)\n",
    "    for param in model.transformer.wte.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.transformer.wpe.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Figer les N premi√®res couches\n",
    "    for i in range(num_layers_to_freeze):\n",
    "        for param in model.transformer.h[i].parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Compter les param√®tres\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    frozen_params = total_params - trainable_params\n",
    "    \n",
    "    print(f\"Couches fig√©es : {num_layers_to_freeze} / {total_layers}\")\n",
    "    print(f\"Param√®tres totaux : {total_params:,}\")\n",
    "    print(f\"Param√®tres fig√©s : {frozen_params:,} ({100*frozen_params/total_params:.1f}%)\")\n",
    "    print(f\"Param√®tres entra√Ænables : {trainable_params:,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "    \n",
    "    return trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer si l'option est activ√©e\n",
    "if FREEZE_LOWER_LAYERS:\n",
    "    print(\"‚ïê\" * 50)\n",
    "    print(\"Freezing des couches basses...\")\n",
    "    print(\"‚ïê\" * 50)\n",
    "    trainable = freeze_lower_layers(model, NUM_LAYERS_TO_FREEZE)\n",
    "else:\n",
    "    print(\"Option FREEZE_LOWER_LAYERS d√©sactiv√©e.\")\n",
    "    print(\"Tous les param√®tres seront entra√Æn√©s (full fine-tuning).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©placer le mod√®le sur GPU\n",
    "model = model.to(device)\n",
    "print(f\"\\nMod√®le d√©plac√© sur : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Test du mod√®le avant fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100, temperature=0.8, top_k=50):\n",
    "    \"\"\"G√©n√®re du texte √† partir d'un prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avant fine-tuning\n",
    "prompt = \"Pikachu est un Pok√©mon de type\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"G√©n√©ration (AVANT fine-tuning):\")\n",
    "print(generate_text(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Pr√©parer le dataset Pok√©mon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset\n",
    "print(\"Chargement du dataset Pok√©mon...\")\n",
    "dataset = load_dataset(\"chris-lmd/pokepedia-fr\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset charg√© !\")\n",
    "print(f\"Articles : {len(dataset['train']):,}\")\n",
    "print(f\"Colonnes : {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aper√ßu d'un article\n",
    "sample = dataset['train'][0]\n",
    "print(f\"Titre: {sample.get('title', 'N/A')}\")\n",
    "print(f\"Types: {sample.get('types', [])}\")\n",
    "print(f\"\\nContenu (500 premiers caract√®res):\")\n",
    "print(sample['content'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meilleur filtre V2 : utiliser le champ 'types'\n",
    "# Les vrais Pok√©mon ont au moins un type\n",
    "def is_pokemon_description(example):\n",
    "    \"\"\"Filtre les articles d√©crivant des Pok√©mon.\"\"\"\n",
    "    # Crit√®re 1 : a des types (c'est un vrai Pok√©mon)\n",
    "    has_types = len(example.get('types', [])) > 0\n",
    "    \n",
    "    # Crit√®re 2 : contient des patterns de description\n",
    "    text = example['content'].lower()\n",
    "    has_patterns = any(p in text for p in [\"est un pok√©mon\", \"√©volution\", \"capacit√©\"])\n",
    "    \n",
    "    return has_types or has_patterns\n",
    "\n",
    "# Filtrer\n",
    "filtered_dataset = dataset['train'].filter(is_pokemon_description)\n",
    "print(f\"Articles apr√®s filtrage : {len(filtered_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limiter le nombre de samples\n",
    "if len(filtered_dataset) > MAX_SAMPLES:\n",
    "    train_dataset = filtered_dataset.shuffle(seed=42).select(range(MAX_SAMPLES))\n",
    "else:\n",
    "    train_dataset = filtered_dataset\n",
    "\n",
    "print(f\"√âchantillon pour fine-tuning : {len(train_dataset):,} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize les textes avec troncature.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['content'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "print(\"Tokenization en cours...\")\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "# Ajouter les labels\n",
    "def add_labels(examples):\n",
    "    examples['labels'] = examples['input_ids'].copy()\n",
    "    return examples\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)\n",
    "print(f\"‚úÖ Tokenization termin√©e !\")\n",
    "print(f\"Colonnes : {tokenized_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-pokemon-v2\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # Hyperparam√®tres\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    # Optimiseur\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Misc\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Configuration :\")\n",
    "print(f\"  - Epochs : {training_args.num_train_epochs}\")\n",
    "print(f\"  - Batch size effectif : {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Learning rate : {training_args.learning_rate}\")\n",
    "print(f\"  - FP16 : {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer cr√©√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© de la configuration\n",
    "print(\"‚ïê\" * 60)\n",
    "print(\"           R√âSUM√â DE LA CONFIGURATION V2\")\n",
    "print(\"‚ïê\" * 60)\n",
    "print(f\"\\nüìä Dataset : {len(train_dataset):,} articles\")\n",
    "print(f\"üìù Max length : {MAX_LENGTH} tokens\")\n",
    "print(f\"üîÑ Epochs : {NUM_EPOCHS}\")\n",
    "print(f\"\\nüéØ Options :\")\n",
    "print(f\"   - Tokens Pok√©mon ajout√©s : {ADD_POKEMON_TOKENS}\")\n",
    "print(f\"   - Couches fig√©es : {FREEZE_LOWER_LAYERS} ({NUM_LAYERS_TO_FREEZE if FREEZE_LOWER_LAYERS else 0}/{model.config.n_layer})\")\n",
    "print(f\"\\nüíæ Vocabulaire : {len(tokenizer):,} tokens\")\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"üß† Param√®tres entra√Ænables : {trainable:,} / {total:,} ({100*trainable/total:.1f}%)\")\n",
    "print(\"‚ïê\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancer le fine-tuning\n",
    "print(\"üöÄ Fine-tuning V2 en cours...\")\n",
    "print()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Fine-tuning termin√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le\n",
    "trainer.save_model(\"./gpt2-pokemon-v2-final\")\n",
    "tokenizer.save_pretrained(\"./gpt2-pokemon-v2-final\")\n",
    "print(\"‚úÖ Mod√®le V2 sauvegard√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. G√©n√©ration de descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(prompt, max_length=150, temperature=0.8, top_k=50, top_p=0.9):\n",
    "    \"\"\"G√©n√®re une description de Pok√©mon.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test : Pikachu\n",
    "prompt = \"Pikachu est un Pok√©mon de type\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nG√©n√©ration (APR√àS fine-tuning V2):\")\n",
    "print(generate_description(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test : Pok√©mon invent√©\n",
    "prompt = \"Flamador est un Pok√©mon de type Feu. Il\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nG√©n√©ration:\")\n",
    "print(generate_description(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test : √âvolution\n",
    "prompt = \"Dracaufeu √©volue √† partir de\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nG√©n√©ration:\")\n",
    "print(generate_description(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Comparaison des configurations\n",
    "\n",
    "Pour comparer l'effet des options, relancez le notebook avec diff√©rentes configurations :\n",
    "\n",
    "| Configuration | ADD_POKEMON_TOKENS | FREEZE_LOWER_LAYERS | R√©sultat attendu |\n",
    "|---------------|-------------------|---------------------|------------------|\n",
    "| Baseline | False | False | R√©f√©rence |\n",
    "| + Tokens | True | False | Meilleure connaissance des noms |\n",
    "| + Freeze | False | True | Plus rapide, moins d'overfitting |\n",
    "| Full V2 | True | True | Combinaison des deux |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. R√©capitulatif V2\n",
    "\n",
    "### Nouvelles techniques apprises\n",
    "\n",
    "1. **Smart Token Initialization** : Ajouter des tokens au vocabulaire et les initialiser intelligemment\n",
    "2. **Partial Freezing** : Figer les couches basses pour un entra√Ænement plus efficace\n",
    "3. **Meilleur filtrage** : Utiliser les m√©tadonn√©es (types) pour s√©lectionner les bons articles\n",
    "\n",
    "### Quand utiliser ces techniques ?\n",
    "\n",
    "| Technique | Quand l'utiliser |\n",
    "|-----------|------------------|\n",
    "| **Add tokens** | Vocabulaire sp√©cialis√© (noms propres, termes techniques) |\n",
    "| **Freeze layers** | Petit dataset, risque d'overfitting, GPU limit√© |\n",
    "| **Full fine-tuning** | Grand dataset, GPU puissant, besoin de flexibilit√© |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
