{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 06 : Fine-tuning GPT-2 pour g√©n√©rer des Pok√©mon\n",
    "\n",
    "**Objectif** : Fine-tuner GPT-2 fran√ßais pour qu'il g√©n√®re des descriptions de Pok√©mon\n",
    "\n",
    "**Dur√©e d'entra√Ænement** : ~20 minutes\n",
    "\n",
    "> **Pendant l'entra√Ænement**, ouvrez le notebook **TP-06-Exploration.ipynb** pour explorer le dataset et comprendre les techniques utilis√©es."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Colab)\n",
    "!pip install -q transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, TrainerCallback\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"\\n‚ö†Ô∏è  GPU non disponible ! L'entra√Ænement sera tr√®s lent.\")\n",
    "    print(\"   Sur Colab : Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Configuration\n",
    "\n",
    "Ces param√®tres sont pr√©-optimis√©s. Vous pouvez les modifier pour exp√©rimenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë                       CONFIGURATION                              ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "MODEL_SIZE = \"base\"           # \"small\" (rapide) ou \"base\" (meilleur)\n",
    "NUM_EPOCHS = 10               # Nombre d'epochs\n",
    "MAX_LENGTH = 256              # Longueur max (tokens)\n",
    "LEARNING_RATE = 5e-5          # Taux d'apprentissage\n",
    "\n",
    "# Techniques d'optimisation (recommand√© : laisser √† True)\n",
    "ADD_POKEMON_TOKENS = True     # Ajouter les noms au vocabulaire\n",
    "FREEZE_LOWER_LAYERS = True    # Figer 50% des couches basses\n",
    "\n",
    "print(\"Configuration charg√©e !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Chargement des donn√©es et du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les noms de Pok√©mon\n",
    "print(\"Chargement des noms de Pok√©mon...\")\n",
    "pokemon_names_ds = load_dataset(\"chris-lmd/pokemon-names-fr\")\n",
    "POKEMON_NAMES = [item[\"name\"] for item in pokemon_names_ds[\"train\"]]\n",
    "print(f\"  {len(POKEMON_NAMES)} noms charg√©s\")\n",
    "\n",
    "# Charger le dataset Pokepedia\n",
    "print(\"\\nChargement du dataset Pokepedia...\")\n",
    "dataset = load_dataset(\"chris-lmd/pokepedia-fr\")\n",
    "print(f\"  {len(dataset['train']):,} articles au total\")\n",
    "\n",
    "# Filtrer pour ne garder que les vrais articles Pok√©mon\n",
    "pokemon_names_set = set(name.lower() for name in POKEMON_NAMES)\n",
    "train_dataset = dataset['train'].filter(\n",
    "    lambda x: x.get('title', '').lower() in pokemon_names_set\n",
    ")\n",
    "print(f\"  {len(train_dataset):,} articles Pok√©mon retenus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le mod√®le GPT-2 fran√ßais\n",
    "model_name = f\"asi/gpt-fr-cased-{MODEL_SIZE}\"\n",
    "print(f\"Chargement de {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"\\n‚úÖ Mod√®le charg√© !\")\n",
    "print(f\"   Param√®tres : {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Couches : {model.config.n_layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Techniques d'optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# TECHNIQUE 1 : Ajouter les tokens Pok√©mon au vocabulaire\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "if ADD_POKEMON_TOKENS:\n",
    "    print(\"Ajout des tokens Pok√©mon...\")\n",
    "    \n",
    "    # Trouver le token de r√©f√©rence pour l'initialisation\n",
    "    for ref in [\"Pok√©mon\", \"Pokemon\", \"animal\"]:\n",
    "        tokens = tokenizer.encode(ref, add_special_tokens=False)\n",
    "        if len(tokens) == 1:\n",
    "            reference_id = tokens[0]\n",
    "            print(f\"  Token de r√©f√©rence : '{ref}'\")\n",
    "            break\n",
    "    \n",
    "    # Tokens √† ajouter (ceux qui ne sont pas d√©j√† uniques)\n",
    "    new_tokens = [name for name in POKEMON_NAMES \n",
    "                  if len(tokenizer.encode(name, add_special_tokens=False)) > 1]\n",
    "    \n",
    "    # Sauvegarder l'embedding de r√©f√©rence\n",
    "    with torch.no_grad():\n",
    "        ref_embedding = model.transformer.wte.weight[reference_id].clone()\n",
    "    \n",
    "    # Ajouter et initialiser\n",
    "    num_added = tokenizer.add_tokens(new_tokens)\n",
    "    old_size = model.transformer.wte.weight.shape[0]\n",
    "    model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_added):\n",
    "            noise = torch.randn_like(ref_embedding) * 0.01\n",
    "            model.transformer.wte.weight[old_size + i] = ref_embedding + noise\n",
    "    \n",
    "    print(f\"  ‚úÖ {num_added} tokens ajout√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# TECHNIQUE 2 : Figer les couches basses\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "if FREEZE_LOWER_LAYERS:\n",
    "    print(\"Freezing des couches basses...\")\n",
    "    \n",
    "    num_to_freeze = model.config.n_layer // 2\n",
    "    \n",
    "    # Figer embeddings\n",
    "    for param in model.transformer.wte.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.transformer.wpe.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Figer les N premi√®res couches\n",
    "    for i in range(num_to_freeze):\n",
    "        for param in model.transformer.h[i].parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  Couches fig√©es : {num_to_freeze}/{model.config.n_layer}\")\n",
    "    print(f\"  ‚úÖ Param√®tres entra√Ænables : {trainable:,} ({100*trainable/total:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©placer sur GPU\n",
    "model = model.to(device)\n",
    "print(f\"Mod√®le sur : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Pr√©paration des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['content'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "print(\"Tokenization...\")\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "# Ajouter les labels (pour le language modeling)\n",
    "tokenized_dataset = tokenized_dataset.map(\n",
    "    lambda x: {'labels': x['input_ids'].copy()},\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ {len(tokenized_dataset)} exemples pr√™ts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Entra√Ænement\n",
    "\n",
    "**‚è±Ô∏è Dur√©e estim√©e : ~20 minutes**\n",
    "\n",
    "> Pendant ce temps, ouvrez **TP-06-Exploration.ipynb** pour explorer le dataset et les techniques !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback pour tracker la loss\n",
    "class LossCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.steps = []\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "            self.steps.append(state.global_step)\n",
    "\n",
    "loss_callback = LossCallback()\n",
    "\n",
    "# Configuration du trainer\n",
    "batch_size = 2 if MODEL_SIZE == \"base\" else 4\n",
    "grad_accum = 8 if MODEL_SIZE == \"base\" else 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-pokemon\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    callbacks=[loss_callback],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Pr√™t pour l'entra√Ænement !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√©\n",
    "print(\"‚ïî\" + \"‚ïê\" * 50 + \"‚ïó\")\n",
    "print(\"‚ïë\" + \" R√âSUM√â \".center(50) + \"‚ïë\")\n",
    "print(\"‚ï†\" + \"‚ïê\" * 50 + \"‚ï£\")\n",
    "print(f\"‚ïë  Mod√®le : {model_name:<38} ‚ïë\")\n",
    "print(f\"‚ïë  Dataset : {len(train_dataset):,} articles Pok√©mon{'':<20} ‚ïë\")\n",
    "print(f\"‚ïë  Epochs : {NUM_EPOCHS}{'':<38} ‚ïë\")\n",
    "print(f\"‚ïë  Tokens ajout√©s : {ADD_POKEMON_TOKENS}{'':<30} ‚ïë\")\n",
    "print(f\"‚ïë  Couches fig√©es : {FREEZE_LOWER_LAYERS}{'':<30} ‚ïë\")\n",
    "print(\"‚ïö\" + \"‚ïê\" * 50 + \"‚ïù\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ LANCER L'ENTRA√éNEMENT\n",
    "print(\"üöÄ Fine-tuning en cours...\")\n",
    "print(\"\\n‚è±Ô∏è  Allez explorer TP-06-Exploration.ipynb pendant ce temps !\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Fine-tuning termin√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Courbe de la loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(loss_callback.steps, loss_callback.losses, 'b-', alpha=0.7)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Loss initiale : {loss_callback.losses[0]:.3f}\")\n",
    "print(f\"Loss finale   : {loss_callback.losses[-1]:.3f}\")\n",
    "print(f\"R√©duction     : {(1 - loss_callback.losses[-1]/loss_callback.losses[0])*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le\n",
    "trainer.save_model(\"./gpt2-pokemon-final\")\n",
    "tokenizer.save_pretrained(\"./gpt2-pokemon-final\")\n",
    "print(\"‚úÖ Mod√®le sauvegard√© dans ./gpt2-pokemon-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Test de g√©n√©ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, temperature=0.7, max_length=150):\n",
    "    \"\"\"G√©n√®re du texte √† partir d'un prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test : Pikachu\n",
    "prompt = \"Pikachu est un Pok√©mon de type\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nG√©n√©ration:\")\n",
    "print(generate(prompt, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test : Dracaufeu\n",
    "prompt = \"Dracaufeu est un Pok√©mon de type\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nG√©n√©ration:\")\n",
    "print(generate(prompt, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test : Pok√©mon invent√©\n",
    "prompt = \"Aqualis est un Pok√©mon de type Eau. Il\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nG√©n√©ration:\")\n",
    "print(generate(prompt, temperature=0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéÆ Testez vos propres prompts !\n",
    "mon_prompt = \"Flamador est un Pok√©mon l√©gendaire de type Feu et Dragon. Ce\"\n",
    "\n",
    "print(f\"Prompt: {mon_prompt}\")\n",
    "print(f\"\\nG√©n√©ration:\")\n",
    "print(generate(mon_prompt, temperature=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Pour aller plus loin\n",
    "\n",
    "**Exp√©rimentations sugg√©r√©es :**\n",
    "- Changer `temperature` : 0.3 (conservateur) ‚Üí 1.0 (cr√©atif)\n",
    "- Essayer diff√©rents prompts\n",
    "- Comparer avec `MODEL_SIZE = \"small\"`\n",
    "\n",
    "**Voir aussi :**\n",
    "- `TP-06-Exploration.ipynb` : Comprendre les techniques en d√©tail"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
