{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 06 : Mini-GPT Partie 2 - Fine-tuning GPT-2\n",
    "\n",
    "**Objectif** : Fine-tuner GPT-2 franÃ§ais pour gÃ©nÃ©rer des descriptions de PokÃ©mon\n",
    "\n",
    "**Programme** :\n",
    "1. Rappel : Ce qu'on a fait avec les noms\n",
    "2. DÃ©mo comparative : From scratch vs Fine-tuning\n",
    "3. Introduction au Transfer Learning\n",
    "4. Charger GPT-2 franÃ§ais\n",
    "5. PrÃ©parer le dataset PokÃ©mon\n",
    "6. Fine-tuning sur Colab\n",
    "7. GÃ©nÃ©ration de descriptions\n",
    "8. Pipeline complet : Nom + Description\n",
    "9. DÃ©mo collective : Inventez votre PokÃ©mon !\n",
    "\n",
    "**DurÃ©e** : 2h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dÃ©pendances (Colab)\n",
    "!pip install -q transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# ReproductibilitÃ©\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Rappel : Ce qu'on a fait avec les noms\n",
    "\n",
    "En Session 5, nous avons :\n",
    "- Construit un **Mini-GPT from scratch** (~500K paramÃ¨tres)\n",
    "- EntraÃ®nÃ© sur **1211 noms de PokÃ©mon**\n",
    "- GÃ©nÃ©rÃ© des noms comme \"Pikadon\", \"SalamÃ¨dre\", \"Florizard\"\n",
    "\n",
    "### Limites du modÃ¨le from scratch\n",
    "\n",
    "| Aspect | From Scratch | PrÃ©-entraÃ®nÃ© |\n",
    "|--------|--------------|---------------|\n",
    "| DonnÃ©es nÃ©cessaires | Beaucoup | Peu (fine-tuning) |\n",
    "| Temps d'entraÃ®nement | Long | Court |\n",
    "| QualitÃ© du langage | LimitÃ©e | Excellente |\n",
    "| Vocabulaire | Restreint | Complet |\n",
    "\n",
    "**Question** : Et si on voulait gÃ©nÃ©rer des **descriptions complÃ¨tes** de PokÃ©mon ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. DÃ©mo comparative : From scratch vs Fine-tuning\n",
    "\n",
    "Imaginons entraÃ®ner un modÃ¨le from scratch sur notre corpus PokÃ©mon (~69 MB de texte).\n",
    "\n",
    "### ProblÃ¨mes :\n",
    "1. **Temps** : Des heures/jours d'entraÃ®nement\n",
    "2. **Ressources** : GPU puissant nÃ©cessaire\n",
    "3. **DonnÃ©es** : 69 MB = petit pour un LLM\n",
    "4. **QualitÃ©** : Le modÃ¨le doit tout apprendre (grammaire, syntaxe, sÃ©mantique...)\n",
    "\n",
    "### Solution : Transfer Learning !\n",
    "\n",
    "```\n",
    "ModÃ¨le prÃ©-entraÃ®nÃ© (GPT-2)     Notre corpus PokÃ©mon\n",
    "        â†“                              â†“\n",
    "   Connaissance                    SpÃ©cialisation\n",
    "   du franÃ§ais                     PokÃ©mon\n",
    "        â†“                              â†“\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â†“\n",
    "          ModÃ¨le fine-tunÃ©\n",
    "          (Expert PokÃ©mon)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Introduction au Transfer Learning\n",
    "\n",
    "### Qu'est-ce que le Transfer Learning ?\n",
    "\n",
    "**DÃ©finition** : Utiliser un modÃ¨le prÃ©-entraÃ®nÃ© sur une grande quantitÃ© de donnÃ©es, puis l'adapter Ã  une tÃ¢che spÃ©cifique.\n",
    "\n",
    "### Analogie\n",
    "\n",
    "- **From scratch** = Apprendre le franÃ§ais Ã  un bÃ©bÃ©, puis lui enseigner les PokÃ©mon\n",
    "- **Fine-tuning** = Prendre un adulte francophone et lui montrer des articles PokÃ©mon\n",
    "\n",
    "### Avantages du fine-tuning\n",
    "\n",
    "1. **RapiditÃ©** : Quelques minutes au lieu d'heures\n",
    "2. **EfficacitÃ©** : Peu de donnÃ©es suffisent\n",
    "3. **QualitÃ©** : Le modÃ¨le sait dÃ©jÃ  Ã©crire en franÃ§ais\n",
    "4. **CoÃ»t** : Moins de ressources GPU\n",
    "\n",
    "### Types de fine-tuning\n",
    "\n",
    "| Type | Description | Usage |\n",
    "|------|-------------|-------|\n",
    "| **Full fine-tuning** | Tous les poids sont mis Ã  jour | Petit modÃ¨le, beaucoup de donnÃ©es |\n",
    "| **LoRA** | Seuls quelques poids sont ajoutÃ©s | Grand modÃ¨le, peu de ressources |\n",
    "| **Prompt tuning** | Seul le prompt est optimisÃ© | TrÃ¨s peu de donnÃ©es |\n",
    "\n",
    "Aujourd'hui, nous ferons du **full fine-tuning** sur un petit GPT-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Charger GPT-2 franÃ§ais\n",
    "\n",
    "Nous utilisons `asi/gpt-fr-cased-small` : un GPT-2 entraÃ®nÃ© sur du texte franÃ§ais.\n",
    "\n",
    "### CaractÃ©ristiques\n",
    "- **~117M paramÃ¨tres** (vs 1.5B pour GPT-2 XL)\n",
    "- **Vocabulaire franÃ§ais** optimisÃ©\n",
    "- **Licence** : Open source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le tokenizer et le modÃ¨le\n",
    "model_name = \"asi/gpt-fr-cased-small\"\n",
    "\n",
    "print(\"Chargement du tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Chargement du modÃ¨le...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"\\nâœ… ModÃ¨le chargÃ© !\")\n",
    "print(f\"ParamÃ¨tres : {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Vocabulaire : {len(tokenizer):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter un token de padding (nÃ©cessaire pour le fine-tuning)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du modÃ¨le avant fine-tuning\n",
    "\n",
    "Que sait GPT-2 franÃ§ais des PokÃ©mon ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100, temperature=0.8, top_k=50):\n",
    "    \"\"\"GÃ©nÃ¨re du texte Ã  partir d'un prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# DÃ©placer le modÃ¨le sur GPU si disponible\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avant fine-tuning\n",
    "prompt = \"Pikachu est un PokÃ©mon de type\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"GÃ©nÃ©ration (AVANT fine-tuning):\")\n",
    "print(generate_text(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Le modÃ¨le peut gÃ©nÃ©rer du franÃ§ais correct, mais ne connaÃ®t pas vraiment les PokÃ©mon. C'est lÃ  qu'intervient le fine-tuning !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. PrÃ©parer le dataset PokÃ©mon\n",
    "\n",
    "Notre corpus : `chris-lmd/pokepedia-fr` (28,677 articles, ~69 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset\n",
    "print(\"Chargement du dataset PokÃ©mon...\")\n",
    "dataset = load_dataset(\"chris-lmd/pokepedia-fr\")\n",
    "\n",
    "print(f\"\\nâœ… Dataset chargÃ© !\")\n",
    "print(f\"Articles : {len(dataset['train']):,}\")\n",
    "print(f\"Colonnes : {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AperÃ§u d'un article\n",
    "sample = dataset['train'][0]\n",
    "print(f\"Titre: {sample.get('title', 'N/A')}\")\n",
    "print(f\"\\nContenu (500 premiers caractÃ¨res):\")\n",
    "print(sample['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrer les articles pertinents\n",
    "\n",
    "On garde les articles qui dÃ©crivent des PokÃ©mon (pas les pages techniques, listes, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pokemon_description(example):\n",
    "    \"\"\"Filtre les articles dÃ©crivant des PokÃ©mon.\"\"\"\n",
    "    text = example['text'].lower()\n",
    "    # Cherche des patterns typiques de descriptions de PokÃ©mon\n",
    "    pokemon_patterns = [\n",
    "        \"est un pokÃ©mon\",\n",
    "        \"type \",\n",
    "        \"Ã©volution\",\n",
    "        \"espÃ¨ce\",\n",
    "        \"capacitÃ©\",\n",
    "        \"gÃ©nÃ©ration\"\n",
    "    ]\n",
    "    return any(pattern in text for pattern in pokemon_patterns)\n",
    "\n",
    "# Filtrer\n",
    "filtered_dataset = dataset['train'].filter(is_pokemon_description)\n",
    "print(f\"Articles aprÃ¨s filtrage : {len(filtered_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour le fine-tuning, on limite Ã  un sous-ensemble (vitesse)\n",
    "# En production, on utiliserait tout le dataset\n",
    "MAX_SAMPLES = 2000\n",
    "\n",
    "if len(filtered_dataset) > MAX_SAMPLES:\n",
    "    train_dataset = filtered_dataset.shuffle(seed=42).select(range(MAX_SAMPLES))\n",
    "else:\n",
    "    train_dataset = filtered_dataset\n",
    "\n",
    "print(f\"Ã‰chantillon pour fine-tuning : {len(train_dataset):,} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MAX_LENGTH = 256  # Longueur max des sÃ©quences\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize les textes avec troncature.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "# Tokenizer le dataset\n",
    "print(\"Tokenization en cours...\")\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"âœ… Tokenization terminÃ©e !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour le language modeling, labels = input_ids\n",
    "def add_labels(examples):\n",
    "    examples['labels'] = examples['input_ids'].copy()\n",
    "    return examples\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)\n",
    "print(f\"Colonnes finales : {tokenized_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Fine-tuning sur Colab\n",
    "\n",
    "On utilise le `Trainer` de Hugging Face pour simplifier l'entraÃ®nement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'entraÃ®nement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-pokemon\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # HyperparamÃ¨tres\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 16\n",
    "    \n",
    "    # Optimiseur\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Misc\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision si GPU\n",
    "    report_to=\"none\",  # DÃ©sactiver wandb\n",
    ")\n",
    "\n",
    "print(\"Configuration :\")\n",
    "print(f\"  - Epochs : {training_args.num_train_epochs}\")\n",
    "print(f\"  - Batch size effectif : {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Learning rate : {training_args.learning_rate}\")\n",
    "print(f\"  - FP16 : {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er le Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer crÃ©Ã© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancer le fine-tuning\n",
    "print(\"ğŸš€ Fine-tuning en cours...\")\n",
    "print(\"(Cela prend environ 10-15 minutes sur GPU Colab)\")\n",
    "print()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nâœ… Fine-tuning terminÃ© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modÃ¨le\n",
    "trainer.save_model(\"./gpt2-pokemon-final\")\n",
    "tokenizer.save_pretrained(\"./gpt2-pokemon-final\")\n",
    "print(\"âœ… ModÃ¨le sauvegardÃ© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. GÃ©nÃ©ration de descriptions\n",
    "\n",
    "Testons notre modÃ¨le fine-tunÃ© !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recharger le modÃ¨le fine-tunÃ© (optionnel, dÃ©jÃ  en mÃ©moire)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"./gpt2-pokemon-final\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(prompt, max_length=150, temperature=0.8, top_k=50, top_p=0.9):\n",
    "    \"\"\"GÃ©nÃ¨re une description de PokÃ©mon.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            repetition_penalty=1.2,  # Ã‰vite les rÃ©pÃ©titions\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1 : Pikachu\n",
    "prompt = \"Pikachu est un PokÃ©mon de type\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nGÃ©nÃ©ration (APRÃˆS fine-tuning):\")\n",
    "print(generate_description(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2 : PokÃ©mon inventÃ©\n",
    "prompt = \"Flamador est un PokÃ©mon de type Feu. Il\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nGÃ©nÃ©ration:\")\n",
    "print(generate_description(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3 : Ã‰volution\n",
    "prompt = \"Aquazur Ã©volue en\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nGÃ©nÃ©ration:\")\n",
    "print(generate_description(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorer les paramÃ¨tres de gÃ©nÃ©ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Voltorex est un PokÃ©mon de type Ã‰lectrik. Ses capacitÃ©s incluent\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Effet de la tempÃ©rature\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for temp in [0.3, 0.7, 1.0, 1.3]:\n",
    "    print(f\"\\nğŸŒ¡ï¸ TempÃ©rature = {temp}\")\n",
    "    print(generate_description(prompt, temperature=temp, max_length=100))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** :\n",
    "- **TempÃ©rature basse (0.3)** : Texte rÃ©pÃ©titif mais cohÃ©rent\n",
    "- **TempÃ©rature moyenne (0.7)** : Bon Ã©quilibre\n",
    "- **TempÃ©rature haute (1.3)** : Plus crÃ©atif mais parfois incohÃ©rent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Pipeline complet : Nom + Description\n",
    "\n",
    "Combinons notre Mini-GPT (noms) et GPT-2 fine-tunÃ© (descriptions) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modÃ¨le de noms (depuis la session 5)\n",
    "# Si vous avez sauvegardÃ© le modÃ¨le, chargez-le ici\n",
    "# Sinon, on utilise une version simplifiÃ©e\n",
    "\n",
    "NOMS_POKEMON_EXEMPLES = [\n",
    "    \"Flamidor\", \"Aquazur\", \"Voltorex\", \"Plantix\", \"Glacior\",\n",
    "    \"Pyralis\", \"Hydragon\", \"Ã‰lectrus\", \"Floralix\", \"Givreon\",\n",
    "    \"Magmador\", \"Ondiral\", \"Fulguras\", \"Sylvora\", \"Crystallix\",\n",
    "    \"Embrasol\", \"Tsunamix\", \"Zappeur\", \"Verdora\", \"Frostix\"\n",
    "]\n",
    "\n",
    "TYPES_POKEMON = [\n",
    "    \"Feu\", \"Eau\", \"Ã‰lectrik\", \"Plante\", \"Glace\",\n",
    "    \"Combat\", \"Poison\", \"Sol\", \"Vol\", \"Psy\",\n",
    "    \"Insecte\", \"Roche\", \"Spectre\", \"Dragon\", \"TÃ©nÃ¨bres\",\n",
    "    \"Acier\", \"FÃ©e\", \"Normal\"\n",
    "]\n",
    "\n",
    "def generate_name():\n",
    "    \"\"\"GÃ©nÃ¨re un nom de PokÃ©mon (version simplifiÃ©e).\"\"\"\n",
    "    # En production, utiliser le Mini-GPT de la session 5\n",
    "    return random.choice(NOMS_POKEMON_EXEMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pokemon():\n",
    "    \"\"\"Pipeline complet : gÃ©nÃ¨re un PokÃ©mon avec nom et description.\"\"\"\n",
    "    # 1. GÃ©nÃ©rer un nom\n",
    "    nom = generate_name()\n",
    "    \n",
    "    # 2. Choisir un type\n",
    "    type_pokemon = random.choice(TYPES_POKEMON)\n",
    "    \n",
    "    # 3. GÃ©nÃ©rer la description\n",
    "    prompt = f\"{nom} est un PokÃ©mon de type {type_pokemon}.\"\n",
    "    description = generate_description(prompt, max_length=200, temperature=0.8)\n",
    "    \n",
    "    return {\n",
    "        \"nom\": nom,\n",
    "        \"type\": type_pokemon,\n",
    "        \"description\": description\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GÃ©nÃ©rer un PokÃ©mon complet\n",
    "pokemon = create_pokemon()\n",
    "\n",
    "print(\"ğŸ® NOUVEAU POKÃ‰MON GÃ‰NÃ‰RÃ‰ !\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“› Nom : {pokemon['nom']}\")\n",
    "print(f\"âš¡ Type : {pokemon['type']}\")\n",
    "print(f\"\\nğŸ“ Description :\")\n",
    "print(pokemon['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version amÃ©liorÃ©e avec le Mini-GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si vous avez sauvegardÃ© votre Mini-GPT de la session 5 :\n",
    "# DÃ©commentez et adaptez le chemin\n",
    "\n",
    "\"\"\"\n",
    "from minigpt_session5 import MiniGPT, CharTokenizer\n",
    "\n",
    "# Charger le modÃ¨le de noms\n",
    "char_tokenizer = CharTokenizer()\n",
    "minigpt = MiniGPT(vocab_size=len(char_tokenizer), ...)\n",
    "minigpt.load_state_dict(torch.load(\"minigpt_pokemon.pt\"))\n",
    "minigpt.eval()\n",
    "\n",
    "def generate_name_with_minigpt(prefix=\"\"):\n",
    "    # GÃ©nÃ©ration avec le vrai Mini-GPT\n",
    "    return minigpt.generate(prefix, max_length=15)\n",
    "\"\"\"\n",
    "print(\"ğŸ’¡ Pour utiliser le vrai Mini-GPT, chargez le modÃ¨le de la session 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. DÃ©mo collective : Inventez votre PokÃ©mon !\n",
    "\n",
    "Chaque Ã©lÃ¨ve peut maintenant crÃ©er son propre PokÃ©mon unique !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inventez_votre_pokemon(prefixe_nom=None, type_souhaite=None):\n",
    "    \"\"\"\n",
    "    CrÃ©ez votre propre PokÃ©mon !\n",
    "    \n",
    "    Args:\n",
    "        prefixe_nom: DÃ©but du nom (ex: \"Pika\", \"Dra\", \"Fla\")\n",
    "        type_souhaite: Type voulu (ex: \"Feu\", \"Eau\", \"Dragon\")\n",
    "    \"\"\"\n",
    "    # Nom\n",
    "    if prefixe_nom:\n",
    "        # ComplÃ©ter le prÃ©fixe (version simplifiÃ©e)\n",
    "        suffixes = [\"dor\", \"rex\", \"ix\", \"on\", \"us\", \"al\", \"or\", \"ax\", \"is\", \"eon\"]\n",
    "        nom = prefixe_nom.capitalize() + random.choice(suffixes)\n",
    "    else:\n",
    "        nom = generate_name()\n",
    "    \n",
    "    # Type\n",
    "    type_pokemon = type_souhaite if type_souhaite else random.choice(TYPES_POKEMON)\n",
    "    \n",
    "    # Description\n",
    "    prompt = f\"{nom} est un PokÃ©mon de type {type_pokemon}.\"\n",
    "    description = generate_description(prompt, max_length=200, temperature=0.8)\n",
    "    \n",
    "    # Affichage stylÃ©\n",
    "    print(\"\\n\" + \"â•\" * 60)\n",
    "    print(\"          ğŸ® VOTRE POKÃ‰MON UNIQUE ! ğŸ®\")\n",
    "    print(\"â•\" * 60)\n",
    "    print(f\"\\n  ğŸ“› Nom : {nom}\")\n",
    "    print(f\"  âš¡ Type : {type_pokemon}\")\n",
    "    print(f\"\\n  ğŸ“ Description :\")\n",
    "    print(f\"  {description}\")\n",
    "    print(\"\\n\" + \"â•\" * 60)\n",
    "    \n",
    "    return {\"nom\": nom, \"type\": type_pokemon, \"description\": description}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple 1 : PokÃ©mon totalement alÃ©atoire\n",
    "inventez_votre_pokemon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple 2 : Avec un prÃ©fixe de nom\n",
    "inventez_votre_pokemon(prefixe_nom=\"Pyro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple 3 : Avec prÃ©fixe et type\n",
    "inventez_votre_pokemon(prefixe_nom=\"Crystal\", type_souhaite=\"Glace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ Ã€ VOUS DE JOUER !\n",
    "# Remplacez les valeurs ci-dessous pour crÃ©er VOTRE PokÃ©mon\n",
    "\n",
    "inventez_votre_pokemon(\n",
    "    prefixe_nom=\"\",      # Votre prÃ©fixe (ex: \"MÃ©ga\", \"Ultra\", votre prÃ©nom...)\n",
    "    type_souhaite=\"\"     # Votre type (ex: \"Dragon\", \"FÃ©e\", \"Spectre\"...)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. RÃ©capitulatif\n",
    "\n",
    "### Ce que vous avez appris\n",
    "\n",
    "1. **Transfer Learning** : Pourquoi c'est plus efficace que from scratch\n",
    "2. **Fine-tuning** : Adapter un modÃ¨le prÃ©-entraÃ®nÃ© Ã  une tÃ¢che\n",
    "3. **Hugging Face** : Charger modÃ¨les et tokenizers\n",
    "4. **Trainer API** : EntraÃ®ner facilement avec PyTorch\n",
    "5. **GÃ©nÃ©ration** : ParamÃ¨tres (temperature, top_k, top_p)\n",
    "6. **Pipeline ML** : Combiner plusieurs modÃ¨les\n",
    "\n",
    "### Architecture du projet\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    PIPELINE POKÃ‰MON                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
    "â”‚    â”‚   Mini-GPT      â”‚     â”‚    GPT-2 Fine-tunÃ©      â”‚     â”‚\n",
    "â”‚    â”‚  (from scratch) â”‚     â”‚    (Transfer Learning)  â”‚     â”‚\n",
    "â”‚    â”‚                 â”‚     â”‚                         â”‚     â”‚\n",
    "â”‚    â”‚  GÃ©nÃ¨re des     â”‚â”€â”€â”€â”€â–¶â”‚  GÃ©nÃ¨re des             â”‚     â”‚\n",
    "â”‚    â”‚  NOMS           â”‚     â”‚  DESCRIPTIONS           â”‚     â”‚\n",
    "â”‚    â”‚                 â”‚     â”‚                         â”‚     â”‚\n",
    "â”‚    â”‚  ~500K params   â”‚     â”‚  ~117M params           â”‚     â”‚\n",
    "â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚                           â†“                                 â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\n",
    "â”‚               â”‚   PokÃ©mon Complet !   â”‚                     â”‚\n",
    "â”‚               â”‚                       â”‚                     â”‚\n",
    "â”‚               â”‚  Nom + Type +         â”‚                     â”‚\n",
    "â”‚               â”‚  Description          â”‚                     â”‚\n",
    "â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Pour aller plus loin\n",
    "\n",
    "- **LoRA** : Fine-tuning efficace pour grands modÃ¨les\n",
    "- **RLHF** : Reinforcement Learning from Human Feedback\n",
    "- **RAG** : Retrieval Augmented Generation\n",
    "- **ModÃ¨les multimodaux** : GÃ©nÃ©ration image + texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ FÃ©licitations !\n",
    "\n",
    "Vous avez maintenant :\n",
    "- Compris les **Transformers** de A Ã  Z\n",
    "- ImplÃ©mentÃ© l'**Attention**, **Multi-Head**, et un **Transformer Block**\n",
    "- CrÃ©Ã© un **Mini-GPT from scratch**\n",
    "- **Fine-tunÃ© GPT-2** sur un corpus personnalisÃ©\n",
    "- Construit un **pipeline de gÃ©nÃ©ration** complet\n",
    "\n",
    "**Vous Ãªtes prÃªts pour le monde des LLM !** ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
