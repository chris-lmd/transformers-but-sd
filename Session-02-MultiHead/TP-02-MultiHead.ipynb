{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 02 - Multi-Head Attention\n",
    "\n",
    "**Module** : Réseaux de Neurones Approfondissement  \n",
    "**Durée** : 2h  \n",
    "**Objectif** : Comprendre et implémenter le Multi-Head Attention\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs pédagogiques\n",
    "\n",
    "À la fin de ce TP, vous serez capable de :\n",
    "1. Expliquer pourquoi plusieurs têtes d'attention sont utiles\n",
    "2. Implémenter le Multi-Head Attention from scratch\n",
    "3. Visualiser ce que chaque tête apprend\n",
    "4. Comprendre le lien avec les Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances (Google Colab)\n",
    "!pip install torch matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rappel : Single-Head Attention\n",
    "\n",
    "Reprenons notre fonction d'attention du TP précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries, shape (..., seq_len, d_k)\n",
    "        K: Keys, shape (..., seq_len, d_k)\n",
    "        V: Values, shape (..., seq_len, d_v)\n",
    "        mask: Masque optionnel\n",
    "    \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Pourquoi Multi-Head ?\n",
    "\n",
    "### Le problème avec une seule tête\n",
    "\n",
    "Une seule tête d'attention calcule **une** représentation des relations entre mots.\n",
    "\n",
    "Mais dans une phrase, il y a **plusieurs types de relations** :\n",
    "- Relations syntaxiques (sujet-verbe)\n",
    "- Relations sémantiques (sens)\n",
    "- Relations de proximité\n",
    "- etc.\n",
    "\n",
    "### La solution : plusieurs têtes\n",
    "\n",
    "Chaque tête peut apprendre à détecter un type de relation différent !\n",
    "\n",
    "**Analogie** : C'est comme avoir plusieurs experts qui analysent une phrase sous différents angles, puis combinent leurs analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration : différentes têtes peuvent capturer différentes relations\n",
    "phrase = [\"Le\", \"chat\", \"noir\", \"mange\", \"la\", \"souris\"]\n",
    "\n",
    "# Tête 1 : Relations syntaxiques (sujet-verbe)\n",
    "attention_syntaxe = torch.tensor([\n",
    "    [0.3, 0.5, 0.1, 0.05, 0.03, 0.02],  # \"Le\" → \"chat\"\n",
    "    [0.1, 0.3, 0.1, 0.4, 0.05, 0.05],   # \"chat\" → \"mange\"\n",
    "    [0.1, 0.6, 0.2, 0.05, 0.03, 0.02],  # \"noir\" → \"chat\"\n",
    "    [0.05, 0.5, 0.05, 0.2, 0.1, 0.1],   # \"mange\" → \"chat\"\n",
    "    [0.02, 0.03, 0.02, 0.03, 0.3, 0.6], # \"la\" → \"souris\"\n",
    "    [0.02, 0.1, 0.02, 0.4, 0.06, 0.4],  # \"souris\" → \"mange\"\n",
    "])\n",
    "\n",
    "# Tête 2 : Relations de proximité\n",
    "attention_proximite = torch.tensor([\n",
    "    [0.5, 0.4, 0.08, 0.01, 0.005, 0.005],\n",
    "    [0.3, 0.4, 0.25, 0.04, 0.005, 0.005],\n",
    "    [0.1, 0.35, 0.35, 0.15, 0.03, 0.02],\n",
    "    [0.02, 0.1, 0.3, 0.35, 0.18, 0.05],\n",
    "    [0.01, 0.02, 0.05, 0.25, 0.4, 0.27],\n",
    "    [0.005, 0.01, 0.02, 0.1, 0.35, 0.515],\n",
    "])\n",
    "\n",
    "# Visualisation côte à côte\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (attn, title) in enumerate([\n",
    "    (attention_syntaxe, \"Tête 1 : Relations syntaxiques\"),\n",
    "    (attention_proximite, \"Tête 2 : Proximité\")\n",
    "]):\n",
    "    ax = axes[idx]\n",
    "    im = ax.imshow(attn, cmap='Blues')\n",
    "    ax.set_xticks(range(6))\n",
    "    ax.set_xticklabels(phrase, rotation=45)\n",
    "    ax.set_yticks(range(6))\n",
    "    ax.set_yticklabels(phrase)\n",
    "    ax.set_title(title)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Architecture Multi-Head Attention\n",
    "\n",
    "### Schéma\n",
    "\n",
    "```\n",
    "Input (seq_len, embed_dim)\n",
    "        ↓\n",
    "   ┌────┴────┬────────┬────────┐\n",
    "   ↓         ↓        ↓        ↓\n",
    " Head 1   Head 2   Head 3   Head 4   (chaque tête a sa propre projection Q, K, V)\n",
    "   ↓         ↓        ↓        ↓\n",
    "   └────┬────┴────────┴────────┘\n",
    "        ↓\n",
    "    Concat\n",
    "        ↓\n",
    "   Linear (projection de sortie)\n",
    "        ↓\n",
    "Output (seq_len, embed_dim)\n",
    "```\n",
    "\n",
    "### Dimensions\n",
    "\n",
    "- **embed_dim** : Dimension des embeddings (ex: 512)\n",
    "- **num_heads** : Nombre de têtes (ex: 8)\n",
    "- **d_k = embed_dim / num_heads** : Dimension par tête (ex: 64)\n",
    "\n",
    "Chaque tête travaille avec une dimension réduite, puis on concatène."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implémentation étape par étape\n",
    "\n",
    "### Étape 1 : Projections Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "embed_dim = 32\n",
    "num_heads = 4\n",
    "d_k = embed_dim // num_heads  # 32 / 4 = 8\n",
    "\n",
    "print(f\"embed_dim: {embed_dim}\")\n",
    "print(f\"num_heads: {num_heads}\")\n",
    "print(f\"d_k (dim par tête): {d_k}\")\n",
    "\n",
    "# Input\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "print(f\"\\nInput shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les projections\n",
    "# On projette vers embed_dim (pas num_heads * d_k, c'est la même chose)\n",
    "W_q = nn.Linear(embed_dim, embed_dim)\n",
    "W_k = nn.Linear(embed_dim, embed_dim)\n",
    "W_v = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "# Projeter\n",
    "Q = W_q(x)  # (batch, seq_len, embed_dim)\n",
    "K = W_k(x)\n",
    "V = W_v(x)\n",
    "\n",
    "print(f\"Q shape après projection: {Q.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 2 : Reshape pour séparer les têtes\n",
    "\n",
    "On doit transformer `(batch, seq_len, embed_dim)` en `(batch, num_heads, seq_len, d_k)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Reshape pour multi-head\n",
    "# ============================================\n",
    "\n",
    "# Transformer (batch, seq_len, embed_dim) en (batch, num_heads, seq_len, d_k)\n",
    "# Étapes:\n",
    "# 1. view(batch, seq_len, num_heads, d_k)\n",
    "# 2. transpose(1, 2) pour avoir num_heads en position 1\n",
    "\n",
    "def split_heads(x, num_heads):\n",
    "    \"\"\"\n",
    "    Reshape (batch, seq_len, embed_dim) -> (batch, num_heads, seq_len, d_k)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, embed_dim = x.shape\n",
    "    d_k = embed_dim // num_heads\n",
    "    \n",
    "    # TODO: Implémenter le reshape\n",
    "    # 1. x.view(batch_size, seq_len, num_heads, d_k)\n",
    "    # 2. .transpose(1, 2) pour obtenir (batch, num_heads, seq_len, d_k)\n",
    "    \n",
    "    x = None  # À compléter\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Test\n",
    "Q_heads = split_heads(Q, num_heads)\n",
    "print(f\"Q shape après split: {Q_heads.shape}\")  # Attendu: (2, 4, 6, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 3 : Attention par tête"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer l'attention sur chaque tête\n",
    "# Grâce aux dimensions batch, PyTorch calcule toutes les têtes en parallèle\n",
    "\n",
    "Q_heads = split_heads(Q, num_heads)\n",
    "K_heads = split_heads(K, num_heads)\n",
    "V_heads = split_heads(V, num_heads)\n",
    "\n",
    "# L'attention fonctionne sur les 2 dernières dimensions\n",
    "attn_output, attn_weights = scaled_dot_product_attention(Q_heads, K_heads, V_heads)\n",
    "\n",
    "print(f\"Attention output shape: {attn_output.shape}\")  # (batch, num_heads, seq_len, d_k)\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")  # (batch, num_heads, seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 4 : Concaténer les têtes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 2 : Concat des têtes\n",
    "# ============================================\n",
    "\n",
    "def concat_heads(x):\n",
    "    \"\"\"\n",
    "    Reshape (batch, num_heads, seq_len, d_k) -> (batch, seq_len, embed_dim)\n",
    "    C'est l'inverse de split_heads\n",
    "    \"\"\"\n",
    "    batch_size, num_heads, seq_len, d_k = x.shape\n",
    "    embed_dim = num_heads * d_k\n",
    "    \n",
    "    # TODO: Implémenter le reshape inverse\n",
    "    # 1. transpose(1, 2) pour avoir (batch, seq_len, num_heads, d_k)\n",
    "    # 2. .contiguous() (nécessaire après transpose)\n",
    "    # 3. .view(batch_size, seq_len, embed_dim)\n",
    "    \n",
    "    x = None  # À compléter\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Test\n",
    "concat_output = concat_heads(attn_output)\n",
    "print(f\"Output après concat: {concat_output.shape}\")  # Attendu: (2, 6, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 5 : Projection de sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection finale\n",
    "W_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "final_output = W_o(concat_output)\n",
    "print(f\"Final output shape: {final_output.shape}\")  # (2, 6, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Classe MultiHeadAttention complète\n",
    "\n",
    "### Exercice 3 : Implémenter la classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention.\n",
    "    \n",
    "    Args:\n",
    "        embed_dim: Dimension des embeddings\n",
    "        num_heads: Nombre de têtes d'attention\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim doit être divisible par num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads\n",
    "        \n",
    "        # TODO: Créer les 4 projections linéaires\n",
    "        self.W_q = None  # nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = None\n",
    "        self.W_v = None\n",
    "        self.W_o = None\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"(batch, seq_len, embed_dim) -> (batch, num_heads, seq_len, d_k)\"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        # TODO: Implémenter\n",
    "        return None\n",
    "    \n",
    "    def concat_heads(self, x):\n",
    "        \"\"\"(batch, num_heads, seq_len, d_k) -> (batch, seq_len, embed_dim)\"\"\"\n",
    "        batch_size, _, seq_len, _ = x.shape\n",
    "        # TODO: Implémenter\n",
    "        return None\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input, shape (batch, seq_len, embed_dim)\n",
    "            mask: Masque optionnel\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, embed_dim)\n",
    "            attention_weights: (batch, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # TODO: Implémenter le forward\n",
    "        # 1. Projeter x en Q, K, V\n",
    "        Q = None\n",
    "        K = None\n",
    "        V = None\n",
    "        \n",
    "        # 2. Split en têtes\n",
    "        Q = None  # self.split_heads(Q)\n",
    "        K = None\n",
    "        V = None\n",
    "        \n",
    "        # 3. Attention\n",
    "        attn_output, attn_weights = None, None  # scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 4. Concat\n",
    "        concat_output = None  # self.concat_heads(attn_output)\n",
    "        \n",
    "        # 5. Projection de sortie\n",
    "        output = None  # self.W_o(concat_output)\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre implémentation\n",
    "mha = MultiHeadAttention(embed_dim=32, num_heads=4)\n",
    "\n",
    "x = torch.randn(2, 6, 32)  # batch=2, seq=6, embed=32\n",
    "output, weights = mha(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")  # Attendu: (2, 6, 32)\n",
    "print(f\"Weights shape: {weights.shape}\")  # Attendu: (2, 4, 6, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Visualisation des têtes d'attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase de test\n",
    "phrase = [\"Le\", \"chat\", \"mange\", \"la\", \"souris\", \"grise\"]\n",
    "seq_len = len(phrase)\n",
    "\n",
    "# Créer le modèle et faire un forward\n",
    "torch.manual_seed(42)\n",
    "mha = MultiHeadAttention(embed_dim=32, num_heads=4)\n",
    "\n",
    "x = torch.randn(1, seq_len, 32)\n",
    "output, weights = mha(x)\n",
    "\n",
    "# Visualiser chaque tête\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for head in range(4):\n",
    "    ax = axes[head]\n",
    "    w = weights[0, head].detach().numpy()\n",
    "    \n",
    "    im = ax.imshow(w, cmap='Blues')\n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_xticklabels(phrase, rotation=45, fontsize=9)\n",
    "    ax.set_yticks(range(seq_len))\n",
    "    ax.set_yticklabels(phrase, fontsize=9)\n",
    "    ax.set_title(f\"Tête {head + 1}\")\n",
    "\n",
    "plt.suptitle(\"Attention par tête (poids aléatoires)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : Avec des poids aléatoires (non entraînés), les patterns n'ont pas de sens. Après entraînement, chaque tête capture des relations différentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Comparaison avec PyTorch nn.MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch fournit une implémentation optimisée\n",
    "mha_pytorch = nn.MultiheadAttention(embed_dim=32, num_heads=4, batch_first=True)\n",
    "\n",
    "x = torch.randn(2, 6, 32)\n",
    "\n",
    "# Pour nn.MultiheadAttention, on passe query, key, value séparément\n",
    "# En self-attention, query = key = value = x\n",
    "output_pytorch, weights_pytorch = mha_pytorch(x, x, x)\n",
    "\n",
    "print(f\"Output shape (PyTorch): {output_pytorch.shape}\")\n",
    "print(f\"Weights shape (PyTorch): {weights_pytorch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Exercice de synthèse : Nombre de paramètres\n",
    "\n",
    "Calculons le nombre de paramètres dans notre MultiHeadAttention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 4 : Calcul des paramètres\n",
    "# ============================================\n",
    "\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "# Combien de paramètres dans :\n",
    "# - W_q : Linear(embed_dim, embed_dim) = embed_dim * embed_dim + embed_dim (weights + bias)\n",
    "# - W_k : idem\n",
    "# - W_v : idem\n",
    "# - W_o : idem\n",
    "\n",
    "params_per_linear = None  # TODO: Calculer\n",
    "total_params = None  # TODO: 4 * params_per_linear\n",
    "\n",
    "print(f\"Paramètres par couche linéaire: {params_per_linear:,}\")\n",
    "print(f\"Total paramètres MHA: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification\n",
    "mha_test = MultiHeadAttention(embed_dim=512, num_heads=8)\n",
    "real_params = sum(p.numel() for p in mha_test.parameters())\n",
    "print(f\"Vérification PyTorch: {real_params:,} paramètres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Récapitulatif\n",
    "\n",
    "### Ce que nous avons appris\n",
    "\n",
    "1. **Multi-Head** : Plusieurs têtes capturent différents types de relations\n",
    "2. **Architecture** : Split → Attention → Concat → Projection\n",
    "3. **Dimensions** : d_k = embed_dim / num_heads\n",
    "4. **Parallélisation** : Toutes les têtes sont calculées en parallèle (grâce aux dimensions batch)\n",
    "\n",
    "### Formule Multi-Head\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "où $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$\n",
    "\n",
    "### Hyperparamètres typiques\n",
    "\n",
    "| Modèle | embed_dim | num_heads | d_k |\n",
    "|--------|-----------|-----------|-----|\n",
    "| BERT-base | 768 | 12 | 64 |\n",
    "| BERT-large | 1024 | 16 | 64 |\n",
    "| GPT-2 small | 768 | 12 | 64 |\n",
    "| GPT-3 | 12288 | 96 | 128 |\n",
    "\n",
    "### Prochaine session\n",
    "\n",
    "Nous assemblerons le **Transformer complet** : Multi-Head Attention + Feed-Forward + Residuals + LayerNorm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Pour aller plus loin (optionnel)\n",
    "\n",
    "### Cross-Attention\n",
    "\n",
    "En self-attention, Q = K = V proviennent de la même source.\n",
    "\n",
    "En **cross-attention** (utilisé dans les encodeur-décodeur), Q provient d'une source et K, V d'une autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : Cross-attention pour traduction\n",
    "# L'encodeur traite la phrase source (anglais)\n",
    "# Le décodeur génère la phrase cible (français)\n",
    "# Cross-attention : le décodeur \"regarde\" l'encodeur\n",
    "\n",
    "encoder_output = torch.randn(1, 10, 32)  # 10 tokens anglais\n",
    "decoder_state = torch.randn(1, 8, 32)   # 8 tokens français (en cours de génération)\n",
    "\n",
    "mha_cross = nn.MultiheadAttention(embed_dim=32, num_heads=4, batch_first=True)\n",
    "\n",
    "# Query = decoder, Key = Value = encoder\n",
    "cross_output, cross_weights = mha_cross(\n",
    "    query=decoder_state,\n",
    "    key=encoder_output,\n",
    "    value=encoder_output\n",
    ")\n",
    "\n",
    "print(f\"Cross-attention output: {cross_output.shape}\")  # (1, 8, 32)\n",
    "print(f\"Cross-attention weights: {cross_weights.shape}\")  # (1, 8, 10) - décodeur regarde encodeur"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
