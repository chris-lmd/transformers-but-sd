{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 02 - Multi-Head Attention\n",
    "\n",
    "**Module** : Réseaux de Neurones Approfondissement  \n",
    "**Durée** : 2h  \n",
    "**Objectif** : Comprendre et implémenter le Multi-Head Attention\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs pédagogiques\n",
    "\n",
    "À la fin de ce TP, vous serez capable de :\n",
    "1. Expliquer pourquoi plusieurs têtes d'attention sont utiles\n",
    "2. Implémenter le Multi-Head Attention from scratch\n",
    "3. Visualiser ce que chaque tête apprend\n",
    "4. Comprendre le lien avec les Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances (Google Colab)\n",
    "!pip install torch matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rappel : Single-Head Attention\n",
    "\n",
    "Reprenons notre fonction d'attention du TP précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries, shape (..., seq_len, d_k)\n",
    "        K: Keys, shape (..., seq_len, d_k)\n",
    "        V: Values, shape (..., seq_len, d_v)\n",
    "        mask: Masque optionnel\n",
    "    \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Pourquoi Multi-Head ?\n",
    "\n",
    "### Le problème avec une seule tête\n",
    "\n",
    "Une seule tête d'attention calcule **une** représentation des relations entre mots.\n",
    "\n",
    "Mais dans une phrase, il y a **plusieurs types de relations** :\n",
    "- Relations syntaxiques (sujet-verbe)\n",
    "- Relations sémantiques (sens)\n",
    "- Relations de proximité\n",
    "- etc.\n",
    "\n",
    "### La solution : plusieurs têtes\n",
    "\n",
    "Chaque tête peut apprendre à détecter un type de relation différent !\n",
    "\n",
    "**Analogie** : C'est comme avoir plusieurs experts qui analysent une phrase sous différents angles, puis combinent leurs analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration : différentes têtes peuvent capturer différentes relations\n",
    "phrase = [\"Le\", \"chat\", \"noir\", \"mange\", \"la\", \"souris\"]\n",
    "\n",
    "# Tête 1 : Relations syntaxiques (sujet-verbe)\n",
    "attention_syntaxe = torch.tensor([\n",
    "    [0.3, 0.5, 0.1, 0.05, 0.03, 0.02],  # \"Le\" → \"chat\"\n",
    "    [0.1, 0.3, 0.1, 0.4, 0.05, 0.05],   # \"chat\" → \"mange\"\n",
    "    [0.1, 0.6, 0.2, 0.05, 0.03, 0.02],  # \"noir\" → \"chat\"\n",
    "    [0.05, 0.5, 0.05, 0.2, 0.1, 0.1],   # \"mange\" → \"chat\"\n",
    "    [0.02, 0.03, 0.02, 0.03, 0.3, 0.6], # \"la\" → \"souris\"\n",
    "    [0.02, 0.1, 0.02, 0.4, 0.06, 0.4],  # \"souris\" → \"mange\"\n",
    "])\n",
    "\n",
    "# Tête 2 : Relations de proximité\n",
    "attention_proximite = torch.tensor([\n",
    "    [0.5, 0.4, 0.08, 0.01, 0.005, 0.005],\n",
    "    [0.3, 0.4, 0.25, 0.04, 0.005, 0.005],\n",
    "    [0.1, 0.35, 0.35, 0.15, 0.03, 0.02],\n",
    "    [0.02, 0.1, 0.3, 0.35, 0.18, 0.05],\n",
    "    [0.01, 0.02, 0.05, 0.25, 0.4, 0.27],\n",
    "    [0.005, 0.01, 0.02, 0.1, 0.35, 0.515],\n",
    "])\n",
    "\n",
    "# Visualisation côte à côte\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (attn, title) in enumerate([\n",
    "    (attention_syntaxe, \"Tête 1 : Relations syntaxiques\"),\n",
    "    (attention_proximite, \"Tête 2 : Proximité\")\n",
    "]):\n",
    "    ax = axes[idx]\n",
    "    im = ax.imshow(attn, cmap='Blues')\n",
    "    ax.set_xticks(range(6))\n",
    "    ax.set_xticklabels(phrase, rotation=45)\n",
    "    ax.set_yticks(range(6))\n",
    "    ax.set_yticklabels(phrase)\n",
    "    ax.set_title(title)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Architecture Multi-Head Attention\n",
    "\n",
    "### Schéma\n",
    "\n",
    "```\n",
    "Input (seq_len, embed_dim)\n",
    "        ↓\n",
    "   ┌────┴────┬────────┬────────┐\n",
    "   ↓         ↓        ↓        ↓\n",
    " Head 1   Head 2   Head 3   Head 4   (chaque tête a sa propre projection Q, K, V)\n",
    "   ↓         ↓        ↓        ↓\n",
    "   └────┬────┴────────┴────────┘\n",
    "        ↓\n",
    "    Concat\n",
    "        ↓\n",
    "   Linear (projection de sortie)\n",
    "        ↓\n",
    "Output (seq_len, embed_dim)\n",
    "```\n",
    "\n",
    "### Dimensions\n",
    "\n",
    "- **embed_dim** : Dimension des embeddings (ex: 512)\n",
    "- **num_heads** : Nombre de têtes (ex: 8)\n",
    "- **d_k = embed_dim / num_heads** : Dimension par tête (ex: 64)\n",
    "\n",
    "Chaque tête travaille avec une dimension réduite, puis on concatène."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implémentation étape par étape\n",
    "\n",
    "### Étape 1 : Projections Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "embed_dim = 32\n",
    "num_heads = 4\n",
    "d_k = embed_dim // num_heads  # 32 / 4 = 8\n",
    "\n",
    "print(f\"embed_dim: {embed_dim}\")\n",
    "print(f\"num_heads: {num_heads}\")\n",
    "print(f\"d_k (dim par tête): {d_k}\")\n",
    "\n",
    "# Input\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "print(f\"\\nInput shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les projections\n",
    "# On projette vers embed_dim (pas num_heads * d_k, c'est la même chose)\n",
    "W_q = nn.Linear(embed_dim, embed_dim)\n",
    "W_k = nn.Linear(embed_dim, embed_dim)\n",
    "W_v = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "# Projeter\n",
    "Q = W_q(x)  # (batch, seq_len, embed_dim)\n",
    "K = W_k(x)\n",
    "V = W_v(x)\n",
    "\n",
    "print(f\"Q shape après projection: {Q.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 2 : Reshape pour séparer les têtes\n",
    "\n",
    "On doit transformer `(batch, seq_len, embed_dim)` en `(batch, num_heads, seq_len, d_k)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Reshape pour multi-head\n",
    "# ============================================\n",
    "\n",
    "# Transformer (batch, seq_len, embed_dim) en (batch, num_heads, seq_len, d_k)\n",
    "# Étapes:\n",
    "# 1. view(batch, seq_len, num_heads, d_k)\n",
    "# 2. transpose(1, 2) pour avoir num_heads en position 1\n",
    "\n",
    "def split_heads(x, num_heads):\n",
    "    \"\"\"\n",
    "    Reshape (batch, seq_len, embed_dim) -> (batch, num_heads, seq_len, d_k)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, embed_dim = x.shape\n",
    "    d_k = embed_dim // num_heads\n",
    "    \n",
    "    # TODO: Implémenter le reshape\n",
    "    # 1. x.view(batch_size, seq_len, num_heads, d_k)\n",
    "    # 2. .transpose(1, 2) pour obtenir (batch, num_heads, seq_len, d_k)\n",
    "    \n",
    "    x = None  # À compléter\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Test\n",
    "Q_heads = split_heads(Q, num_heads)\n",
    "print(f\"Q shape après split: {Q_heads.shape}\")  # Attendu: (2, 4, 6, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 3 : Attention par tête"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer l'attention sur chaque tête\n",
    "# Grâce aux dimensions batch, PyTorch calcule toutes les têtes en parallèle\n",
    "\n",
    "Q_heads = split_heads(Q, num_heads)\n",
    "K_heads = split_heads(K, num_heads)\n",
    "V_heads = split_heads(V, num_heads)\n",
    "\n",
    "# L'attention fonctionne sur les 2 dernières dimensions\n",
    "attn_output, attn_weights = scaled_dot_product_attention(Q_heads, K_heads, V_heads)\n",
    "\n",
    "print(f\"Attention output shape: {attn_output.shape}\")  # (batch, num_heads, seq_len, d_k)\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")  # (batch, num_heads, seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 4 : Concaténer les têtes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 2 : Concat des têtes\n",
    "# ============================================\n",
    "\n",
    "def concat_heads(x):\n",
    "    \"\"\"\n",
    "    Reshape (batch, num_heads, seq_len, d_k) -> (batch, seq_len, embed_dim)\n",
    "    C'est l'inverse de split_heads\n",
    "    \"\"\"\n",
    "    batch_size, num_heads, seq_len, d_k = x.shape\n",
    "    embed_dim = num_heads * d_k\n",
    "    \n",
    "    # TODO: Implémenter le reshape inverse\n",
    "    # 1. transpose(1, 2) pour avoir (batch, seq_len, num_heads, d_k)\n",
    "    # 2. .contiguous() (nécessaire après transpose)\n",
    "    # 3. .view(batch_size, seq_len, embed_dim)\n",
    "    \n",
    "    x = None  # À compléter\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Test\n",
    "concat_output = concat_heads(attn_output)\n",
    "print(f\"Output après concat: {concat_output.shape}\")  # Attendu: (2, 6, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étape 5 : Projection de sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection finale\n",
    "W_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "final_output = W_o(concat_output)\n",
    "print(f\"Final output shape: {final_output.shape}\")  # (2, 6, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Classe MultiHeadAttention complète\n",
    "\n",
    "### Exercice 3 : Implémenter la classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention.\n",
    "    \n",
    "    Args:\n",
    "        embed_dim: Dimension des embeddings\n",
    "        num_heads: Nombre de têtes d'attention\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim doit être divisible par num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads\n",
    "        \n",
    "        # TODO: Créer les 4 projections linéaires\n",
    "        self.W_q = None  # nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = None\n",
    "        self.W_v = None\n",
    "        self.W_o = None\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"(batch, seq_len, embed_dim) -> (batch, num_heads, seq_len, d_k)\"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        # TODO: Implémenter\n",
    "        return None\n",
    "    \n",
    "    def concat_heads(self, x):\n",
    "        \"\"\"(batch, num_heads, seq_len, d_k) -> (batch, seq_len, embed_dim)\"\"\"\n",
    "        batch_size, _, seq_len, _ = x.shape\n",
    "        # TODO: Implémenter\n",
    "        return None\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input, shape (batch, seq_len, embed_dim)\n",
    "            mask: Masque optionnel\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, embed_dim)\n",
    "            attention_weights: (batch, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # TODO: Implémenter le forward\n",
    "        # 1. Projeter x en Q, K, V\n",
    "        Q = None\n",
    "        K = None\n",
    "        V = None\n",
    "        \n",
    "        # 2. Split en têtes\n",
    "        Q = None  # self.split_heads(Q)\n",
    "        K = None\n",
    "        V = None\n",
    "        \n",
    "        # 3. Attention\n",
    "        attn_output, attn_weights = None, None  # scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 4. Concat\n",
    "        concat_output = None  # self.concat_heads(attn_output)\n",
    "        \n",
    "        # 5. Projection de sortie\n",
    "        output = None  # self.W_o(concat_output)\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre implémentation\n",
    "mha = MultiHeadAttention(embed_dim=32, num_heads=4)\n",
    "\n",
    "x = torch.randn(2, 6, 32)  # batch=2, seq=6, embed=32\n",
    "output, weights = mha(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")  # Attendu: (2, 6, 32)\n",
    "print(f\"Weights shape: {weights.shape}\")  # Attendu: (2, 4, 6, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 6. Visualisation des têtes sur un vrai modèle\n\nVoyons ce que les différentes têtes capturent sur un modèle **réellement entraîné**."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Charger DistilBERT (comme en Session 1)\n!pip install transformers -q\n\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, output_attentions=True)\nmodel.eval()\n\n# Phrase de test\nphrase = \"The cat sat on the mat because it was tired\"\ninputs = tokenizer(phrase, return_tensors=\"pt\")\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n\n# Forward pass\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Extraire les attentions de la couche 5 (12 têtes)\nlayer = 4  # Couche 5 (index 0-5)\nattention = outputs.attentions[layer][0]  # (num_heads, seq_len, seq_len)\n\nprint(f\"Phrase: {phrase}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Shape attention couche {layer+1}: {attention.shape}\")  # (12, 11, 11)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Visualiser 4 têtes différentes de la même couche\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# Sélectionner 4 têtes intéressantes\nheads_to_show = [1, 2, 5, 10]  # Différentes têtes\n\nfor idx, head in enumerate(heads_to_show):\n    ax = axes[idx // 2, idx % 2]\n    w = attention[head].numpy()\n    \n    im = ax.imshow(w, cmap='Blues')\n    ax.set_xticks(range(len(tokens)))\n    ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=9)\n    ax.set_yticks(range(len(tokens)))\n    ax.set_yticklabels(tokens, fontsize=9)\n    ax.set_title(f\"Tête {head + 1}\", fontsize=12)\n    plt.colorbar(im, ax=ax)\n\nplt.suptitle(f\"Différentes têtes de la couche {layer+1} - Chaque tête capture des relations différentes\", fontsize=13)\nplt.tight_layout()\nplt.show()\n\n# Analyse spécifique : que regarde \"it\" selon différentes têtes ?\nit_index = tokens.index(\"it\")\nprint(f\"\\n{'='*60}\")\nprint(f\"Que regarde le pronom 'it' selon chaque tête ?\")\nprint(f\"{'='*60}\")\n\nfor head in heads_to_show:\n    print(f\"\\n--- Tête {head + 1} ---\")\n    weights = attention[head, it_index].numpy()\n    top_indices = weights.argsort()[-3:][::-1]  # Top 3\n    for i in top_indices:\n        bar = \"█\" * int(weights[i] * 20)\n        print(f\"  {tokens[i]:10} {weights[i]:.2f} {bar}\")"
  },
  {
   "cell_type": "code",
   "source": "**Observation** : Chaque tête a appris à capturer des **relations différentes** :\n- Certaines têtes se concentrent sur la **coréférence** (\"it\" → \"cat\")\n- D'autres sur la **proximité** (mots voisins)\n- D'autres sur la **syntaxe** (sujet-verbe)\n- Certaines regardent le token **[CLS]** (représentation globale)\n\nC'est exactement ce qu'on voulait ! Multi-head = **plusieurs experts** qui analysent la phrase sous différents angles.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Comparaison avec PyTorch nn.MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch fournit une implémentation optimisée\n",
    "mha_pytorch = nn.MultiheadAttention(embed_dim=32, num_heads=4, batch_first=True)\n",
    "\n",
    "x = torch.randn(2, 6, 32)\n",
    "\n",
    "# Pour nn.MultiheadAttention, on passe query, key, value séparément\n",
    "# En self-attention, query = key = value = x\n",
    "output_pytorch, weights_pytorch = mha_pytorch(x, x, x)\n",
    "\n",
    "print(f\"Output shape (PyTorch): {output_pytorch.shape}\")\n",
    "print(f\"Weights shape (PyTorch): {weights_pytorch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Exercice de synthèse : Nombre de paramètres\n",
    "\n",
    "Calculons le nombre de paramètres dans notre MultiHeadAttention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 4 : Calcul des paramètres\n",
    "# ============================================\n",
    "\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "\n",
    "# Combien de paramètres dans :\n",
    "# - W_q : Linear(embed_dim, embed_dim) = embed_dim * embed_dim + embed_dim (weights + bias)\n",
    "# - W_k : idem\n",
    "# - W_v : idem\n",
    "# - W_o : idem\n",
    "\n",
    "params_per_linear = None  # TODO: Calculer\n",
    "total_params = None  # TODO: 4 * params_per_linear\n",
    "\n",
    "print(f\"Paramètres par couche linéaire: {params_per_linear:,}\")\n",
    "print(f\"Total paramètres MHA: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification\n",
    "mha_test = MultiHeadAttention(embed_dim=512, num_heads=8)\n",
    "real_params = sum(p.numel() for p in mha_test.parameters())\n",
    "print(f\"Vérification PyTorch: {real_params:,} paramètres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 10. Pour aller plus loin : Cross-Attention\n\n> **Note** : Le cross-attention sera abordé en détail dans la **Session 6 optionnelle** (Mini-GPT / RAG).\n\n**Self-attention** (ce qu'on a fait) : Q, K, V viennent de la **même** source.\n\n**Cross-attention** : Q vient d'une source, K et V d'une **autre** source.\n- Utilisé en **traduction** : le décodeur (français) \"interroge\" l'encodeur (anglais)\n- Utilisé dans les **modèles génératifs** avec contexte externe (RAG)\n\n```\nSelf-attention:     x ──► Q, K, V     (même source)\nCross-attention:    x_dec ──► Q       (une source)\n                    x_enc ──► K, V    (autre source)\n```\n\nDans les sessions 3-5, nous utiliserons uniquement la **self-attention** (encodeur). Le cross-attention sera exploré en Session 6 avec les architectures génératives."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : Cross-attention pour traduction\n",
    "# L'encodeur traite la phrase source (anglais)\n",
    "# Le décodeur génère la phrase cible (français)\n",
    "# Cross-attention : le décodeur \"regarde\" l'encodeur\n",
    "\n",
    "encoder_output = torch.randn(1, 10, 32)  # 10 tokens anglais\n",
    "decoder_state = torch.randn(1, 8, 32)   # 8 tokens français (en cours de génération)\n",
    "\n",
    "mha_cross = nn.MultiheadAttention(embed_dim=32, num_heads=4, batch_first=True)\n",
    "\n",
    "# Query = decoder, Key = Value = encoder\n",
    "cross_output, cross_weights = mha_cross(\n",
    "    query=decoder_state,\n",
    "    key=encoder_output,\n",
    "    value=encoder_output\n",
    ")\n",
    "\n",
    "print(f\"Cross-attention output: {cross_output.shape}\")  # (1, 8, 32)\n",
    "print(f\"Cross-attention weights: {cross_weights.shape}\")  # (1, 8, 10) - décodeur regarde encodeur"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}