{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# TP 02 - Positional Encoding et Bases de l'Attention\n\n**Module** : R√©seaux de Neurones Approfondissement  \n**Dur√©e** : 2h  \n**Objectif** : Comprendre le Positional Encoding et les bases de l'attention\n\n---\n\n## Objectifs p√©dagogiques\n\n√Ä la fin de cette session, vous serez capable de :\n1. Expliquer pourquoi le **Positional Encoding** est n√©cessaire\n2. Calculer le PE avec la formule sin/cos\n3. Comprendre la relation entre **similarit√©** et **produit scalaire**\n4. Calculer les **scores d'attention** √©tape par √©tape\n\n---\n\n**Note** : Ce TP pose les fondations. Le TP suivant impl√©mentera l'attention compl√®te."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (Google Colab)\n",
    "!pip install torch matplotlib numpy transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Configuration\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Rappel du TP1 - Chargement CamemBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cuperation d'un embedding sur un mod√®le d√©j√† entrain√© (BERT)\n",
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "\n",
    "print(\"Chargement de CamemBERT (mod√®le fran√ßais)...\")\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "model_camembert = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# On utilise UNIQUEMENT la couche d'embeddings\n",
    "embedding_layer = model_camembert.embeddings.word_embeddings\n",
    "\n",
    "print(\"‚úÖ CamemBERT charg√© !\")\n",
    "print(f\"   Dimension des embeddings : {embedding_layer.embedding_dim}\")\n",
    "\n",
    "def get_french_embeddings(phrase, target_dim=100):\n",
    "    \"\"\"\n",
    "    Extrait les embeddings d'une phrase fran√ßaise.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        phrase,\n",
    "        is_split_into_words=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings_768 = embedding_layer(inputs[\"input_ids\"][0])\n",
    "\n",
    "    if not hasattr(get_french_embeddings, 'projection'):\n",
    "        torch.manual_seed(42)\n",
    "        get_french_embeddings.projection = torch.randn(768, target_dim) / 30\n",
    "\n",
    "    embeddings = embeddings_768 @ get_french_embeddings.projection\n",
    "    return embeddings, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice de rappel : similarit√©\n",
    "mots = [\"Paris\", \"tour\", \"Eiffel\"]\n",
    "embeddings, tokens = get_french_embeddings(mots)\n",
    "\n",
    "emb_paris = embeddings[tokens.index(\"‚ñÅParis\")]\n",
    "emb_tour = embeddings[tokens.index(\"‚ñÅtour\")]\n",
    "emb_eiffel = embeddings[tokens.index(\"‚ñÅEiffel\")]\n",
    "\n",
    "emb_tour_eiffel = (emb_tour + emb_eiffel) / 2\n",
    "\n",
    "sim_paris_tour_eiffel = F.cosine_similarity(\n",
    "    emb_paris.unsqueeze(0),\n",
    "    emb_tour_eiffel.unsqueeze(0)\n",
    ")\n",
    "\n",
    "sim_paris_tour = F.cosine_similarity(\n",
    "    emb_paris.unsqueeze(0),\n",
    "    emb_tour.unsqueeze(0)\n",
    ")\n",
    "\n",
    "print(f\"Similarit√© Paris / tour Eiffel : {sim_paris_tour_eiffel.item():.4f}\")\n",
    "print(f\"Similarit√© Paris / tour : {sim_paris_tour.item():.4f}\")\n",
    "print(\"\\n‚Üí Paris est plus proche de 'tour Eiffel' que de 'tour' seul\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 2. Introduction : Pourquoi l'attention ?\n\nJusqu'en 2017, la majorit√© des mod√®les de langage (RNN & LSTM) lisaient les phrases s√©quentiellement, de gauche √† droite. Cette approche pr√©sente deux limites majeures :\n\n- elle est **lente**,\n- un mot plac√© au d√©but de la phrase a du mal √† int√©grer des informations situ√©es beaucoup plus loin.\n\nLe **Transformer** rompt compl√®tement avec cette logique.\n\nDans un Transformer, tous les mots de la phrase sont trait√©s **simultan√©ment**.\n\nPrenons la phrase :\n\n*\"Le chat dort sur le canap√©\"*\n\nElle contient six mots : le mod√®le re√ßoit donc six √©l√©ments en parall√®le.\n\nLa premi√®re √©tape consiste √† transformer chaque mot en nombres : ce sont les **embeddings**.\n\nChaque mot est repr√©sent√© par un vecteur de dimension fixe (par exemple 100 ou 512), qui encode des informations s√©mantiques.\n\n√Ä ce stade :\n- on ne manipule plus du texte,\n- mais une suite de vecteurs num√©riques.\n\nCependant, ces vecteurs sont encore ind√©pendants les uns des autres. Le vecteur correspondant √† \"chat\" ne sait rien de \"dort\" ou de \"canap√©\".\n\nüëâ **L'attention va pr√©cis√©ment servir √† cr√©er ces liens.**\n\n### 2.1 L'architecture Transformer\n\nLe **Transformer** (2017) abandonne la r√©currence. Chaque mot peut regarder **tous les autres directement** :\n\n```\nEntr√©e: \"Le chat dort sur le canap√©\" (6 tokens)\n         ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ\n         ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ          EMBEDDINGS (6 vecteurs)            ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ\n         ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ             SELF-ATTENTION                  ‚îÇ\n‚îÇ   Chaque vecteur regarde les 5 autres       ‚îÇ\n‚îÇ   ‚Üí Enrichit chaque mot avec le CONTEXTE    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ\n         ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº\n       (6 vecteurs enrichis)\n         ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ\n         ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ        FEED-FORWARD (par position)          ‚îÇ\n‚îÇ   Exploite le contexte enrichi              ‚îÇ\n‚îÇ   (comme un r√©seau de neurones classique)   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ    ‚îÇ\n         ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº    ‚ñº\n      Sortie: 6 vecteurs transform√©s\n```\n\n**Points cl√©s** :\n- **Entr√©e = Sortie** : Si une phrase contient 6 tokens, le Transformer produit 6 vecteurs en sortie, un par token, enrichis par le contexte.\n- **Taille variable** : Tu peux entrer 5, 50, ou 500 mots (jusqu'√† une limite : 512 pour BERT, 128K pour GPT-4)\n- **Self-Attention** : Donne du contexte √† chaque mot\n- **Feed-Forward** : Exploite ce contexte (transformation non-lin√©aire)\n\n**Que sort le Transformer ? Quelle utilit√© ?**\n\nLe Transformer produit des **vecteurs enrichis** (repr√©sentations). Une couche de sortie (ajout√©e selon la t√¢che) les transforme en r√©sultat :\n- **Classification** ‚Üí probabilit√© par classe (ex: 70% positif, 30% n√©gatif)\n- **G√©n√©ration** ‚Üí probabilit√© du prochain mot\n- **Traduction** ‚Üí phrase dans l'autre langue\n\n### Comment les mots entrent dans le Transformer ?\n\nChaque mot passe par **deux √©tapes** avant d'entrer :\n\n```\nMot \"chat\" (position 1)\n        ‚îÇ\n        ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Token Embedding (fixe pour chaque token)        ‚îÇ\n‚îÇ \"chat\" ‚Üí [0.8, 0.1, 0.3, ...]                   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n        ‚îÇ\n        + (addition)\n        ‚îÇ\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Positional Encoding (fixe pour chaque position) ‚îÇ\n‚îÇ position 1 ‚Üí [0.0, 0.1, 0.0, ...]               ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n        ‚îÇ\n        ‚ñº\nVecteur d'entr√©e = [0.8, 0.2, 0.3, ...]\n```\n\n**Deux composants distincts :**\n\n| Composant | Taille | R√¥le |\n|-----------|--------|------|\n| Token embeddings | ~50k √ó dim | \"Qui suis-je ?\" (sens du mot) |\n| Positional encodings | max_len √ó dim | \"O√π suis-je ?\" (position dans la phrase) |\n\n**Pourquoi c'est important ?** Sans le positional encoding, le mod√®le ne distinguerait pas :\n- *\"Le chat mange la souris\"*\n- *\"La souris mange le chat\"*\n\n(M√™mes tokens, ordre diff√©rent ‚Üí sens oppos√© !)\n\nC'est l'empilement de ces blocs d'attention avec des blocs de Feed-Forward qui constitue le Transformer.\n\n### 2.2 Ce qu'on va construire\n\n```\n    TRANSFORMER\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚îÇ  Embedding + Positional    ‚îÇ\n    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n    ‚îÇ ‚îÇ   SELF-ATTENTION  ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îº‚îÄ‚îÄ‚îÄ Sessions 2-3\n    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n    ‚îÇ ‚îÇ     FEED-FORWARD       ‚îÇ ‚îÇ\n    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n    ‚îÇ         √ó N blocs          ‚îÇ\n    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n    ‚îÇ     Couche de sortie       ‚îÇ\n    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Plan du cours** :\n- **Session 1** : Fondamentaux NLP (tokenization, embeddings)\n- **Session 2** : PE + bases de l'attention (ce TP)\n- **Session 3** : Impl√©menter l'attention compl√®te + intro Multi-Head\n- **Session 4** : Multi-Head + Transformer + masque causal\n- **Sessions 5-6** : Projets Mini-GPT\n\n### 2.3 L'id√©e cl√© de l'attention\n\nL'attention r√©pond √† la question : **\"Pour comprendre ce mot, quels autres mots dois-je regarder ?\"**\n\n**Exemple** : *\"Le chat qui dormait sur le canap√© a saut√©\"*\n- Pour comprendre **\"a saut√©\"** ‚Üí regarder **\"chat\"** (le sujet, pas \"canap√©\")\n\nGr√¢ce √† l'attention :\n- chaque mot int√®gre l'information pertinente des autres mots,\n- les mots importants contribuent davantage au vecteur final,\n- l'ordre et le contexte sont pris en compte sans lecture s√©quentielle.\n\nLe mot \"chat\" ne repr√©sente plus un animal abstrait, mais un chat qui dort sur un canap√©.\n\n---\n\n### Pour approfondir RNN/LSTM (optionnel)\n\n**Vid√©os en fran√ßais** :\n- [Machine Learnia - Les RNN expliqu√©s](https://www.youtube.com/watch?v=EL439RMv3Xc) (~20 min)\n- [Science4All - Comprendre les LSTM](https://www.youtube.com/watch?v=WCUNPb-5EYI) (~15 min)\n\n**Articles en fran√ßais** :\n- [Pens√©e Artificielle - Introduction aux RNN](https://www.penseeartificielle.fr/comprendre-reseaux-neurones-recurrents-rnn/)\n- [DataScientest - LSTM expliqu√© simplement](https://datascientest.com/lstm-tout-savoir)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. Le probl√®me : l'ordre des mots\n\nLes embeddings seuls ne capturent pas la **position** des mots dans la phrase.\n\n```\n\"Le chat mange la souris\"  ‚â†  \"La souris mange le chat\"\n```\n\nPourtant, les m√™mes mots ont les m√™mes embeddings !"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. Positional Encoding\n\n### La formule\n\n$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n\n$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n\n- **pos** : position dans la s√©quence (0, 1, 2, ...)\n- **i** : indice de la dimension\n- **d_model** : dimension totale des embeddings"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_positional_encoding(seq_len, d_model):\n    \"\"\"\n    G√©n√®re le positional encoding avec la formule sin/cos.\n    \n    Args:\n        seq_len: longueur de la s√©quence\n        d_model: dimension des embeddings\n    \n    Returns:\n        Tensor de shape (seq_len, d_model) avec le PE\n    \n    Rappel des formules:\n        PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n        PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n    \"\"\"\n    # TODO: Cr√©er un tensor des positions (0, 1, 2, ..., seq_len-1)\n    # Indice: torch.arange() puis .unsqueeze(1) pour avoir shape (seq_len, 1)\n    position = None  # TODO\n    \n    # TODO: Calculer le terme de division 10000^(2i/d_model)\n    # Indice: utiliser torch.exp et torch.arange(0, d_model, 2)\n    # Note: on calcule -log(10000)/d_model pour la stabilit√© num√©rique\n    div_term = None  # TODO\n    \n    # TODO: Cr√©er le tensor PE de z√©ros avec la bonne shape\n    pe = None  # TODO\n    \n    # TODO: Remplir les dimensions paires (0, 2, 4, ...) avec sin\n    # Indice: pe[:, 0::2] s√©lectionne les colonnes paires\n    # TODO\n    \n    # TODO: Remplir les dimensions impaires (1, 3, 5, ...) avec cos\n    # Indice: pe[:, 1::2] s√©lectionne les colonnes impaires\n    # TODO\n    \n    return pe\n\n# Test de votre impl√©mentation\npe_test = get_positional_encoding(4, 8)\nif pe_test is not None:\n    print(f\"Shape du PE: {pe_test.shape}\")\n    print(f\"PE[0, 0] (sin de pos 0): {pe_test[0, 0]:.4f}\")\n    print(f\"PE[0, 1] (cos de pos 0): {pe_test[0, 1]:.4f}\")\n    print(f\"PE[1, 0] (sin de pos 1): {pe_test[1, 0]:.4f}\")\nelse:\n    print(\"‚ö†Ô∏è Impl√©mentez la fonction get_positional_encoding\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1 : Pourquoi le PE est n√©cessaire ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Exercice 1 : Pourquoi le Positional Encoding ? ===\")\n",
    "\n",
    "phrase_a = [\"Pikachu\", \"attaque\", \"Dracaufeu\"]\n",
    "phrase_b = [\"Dracaufeu\", \"attaque\", \"Pikachu\"]\n",
    "\n",
    "def get_same_embeddings(tokens):\n",
    "    \"\"\"Retourne embeddings identiques pour tokens identiques.\"\"\"\n",
    "    vocab = {\"Pikachu\": 0, \"attaque\": 1, \"Dracaufeu\": 2}\n",
    "    torch.manual_seed(42)\n",
    "    base = torch.randn(3, 100)\n",
    "    return torch.stack([base[vocab[t]] for t in tokens])\n",
    "\n",
    "emb_a = get_same_embeddings(phrase_a)\n",
    "emb_b = get_same_embeddings(phrase_b)\n",
    "\n",
    "print(f\"Phrase A : {phrase_a}\")\n",
    "print(f\"Phrase B : {phrase_b}\")\n",
    "\n",
    "# Calculer la diff√©rence\n",
    "diff_sans_pe = torch.norm(emb_a - emb_b)\n",
    "print(f\"\\nDiff√©rence SANS PE : {diff_sans_pe:.4f}\")\n",
    "\n",
    "# Pourquoi pas identiques ?\n",
    "print(\"\\n‚Üí La diff√©rence n'est PAS nulle car les embeddings sont dans un ordre diff√©rent.\")\n",
    "print(\"   Mais la SOMME des embeddings serait identique !\")\n",
    "print(f\"   Somme A : {emb_a.sum():.4f}\")\n",
    "print(f\"   Somme B : {emb_b.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : Calculer le PE manuellement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Exercice 2 : Calculer le PE manuellement ===\")\n",
    "\n",
    "print(\"\\nCalculons le PE pour d_model=4, seq_len=3\")\n",
    "print(\"\\nFormule :\")\n",
    "print(\"  PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\")\n",
    "print(\"  PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\\n\")\n",
    "\n",
    "# PE(0, 0) : pos=0, i=0 (dimension paire) ‚Üí sin(0 / 10000^0) = sin(0) = 0\n",
    "pe_0_0 = math.sin(0 / (10000 ** (0/4)))\n",
    "print(f\"PE(0, 0) = sin(0 / 10000^0) = sin(0) = {pe_0_0:.4f}\")\n",
    "\n",
    "# PE(0, 1) : pos=0, i=0 (dimension impaire) ‚Üí cos(0 / 10000^0) = cos(0) = 1\n",
    "pe_0_1 = math.cos(0 / (10000 ** (0/4)))\n",
    "print(f\"PE(0, 1) = cos(0 / 10000^0) = cos(0) = {pe_0_1:.4f}\")\n",
    "\n",
    "# PE(1, 0) : pos=1, i=0 (dimension paire) ‚Üí sin(1 / 10000^0) = sin(1)\n",
    "pe_1_0 = math.sin(1 / (10000 ** (0/4)))\n",
    "print(f\"PE(1, 0) = sin(1 / 10000^0) = sin(1) = {pe_1_0:.4f}\")\n",
    "\n",
    "# V√©rification avec notre fonction\n",
    "pe_verif = get_positional_encoding(2, 4)\n",
    "print(f\"\\nV√©rification avec get_positional_encoding :\")\n",
    "print(f\"  PE[0, 0] = {pe_verif[0, 0]:.4f}\")\n",
    "print(f\"  PE[0, 1] = {pe_verif[0, 1]:.4f}\")\n",
    "print(f\"  PE[1, 0] = {pe_verif[1, 0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3 : Propri√©t√©s du PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Exercice 3 : V√©rifier les propri√©t√©s du PE ===\")\n",
    "\n",
    "seq_len_test = 10\n",
    "pe_test = get_positional_encoding(seq_len_test, 100)\n",
    "\n",
    "# Propri√©t√© 1 : Valeurs ‚àà [-1, 1]\n",
    "min_val = pe_test.min().item()\n",
    "max_val = pe_test.max().item()\n",
    "print(f\"\\nMin : {min_val:.4f}\")\n",
    "print(f\"Max : {max_val:.4f}\")\n",
    "print(f\"‚Üí Toutes les valeurs sont bien dans [-1, 1] ‚úì\")\n",
    "\n",
    "# Propri√©t√© 2 : Diff√©rence entre positions cons√©cutives\n",
    "diff_pos_0_1 = torch.norm(pe_test[0] - pe_test[1]).item()\n",
    "diff_pos_4_5 = torch.norm(pe_test[4] - pe_test[5]).item()\n",
    "print(f\"\\nDistance pos 0 ‚Üí 1 : {diff_pos_0_1:.4f}\")\n",
    "print(f\"Distance pos 4 ‚Üí 5 : {diff_pos_4_5:.4f}\")\n",
    "print(f\"‚Üí Les distances sont similaires (positions relatives)\")\n",
    "\n",
    "# Propri√©t√© 3 : Dimensions paires et impaires\n",
    "dim_0_all_pos = pe_test[:, 0]  # sin de toutes les positions\n",
    "dim_1_all_pos = pe_test[:, 1]  # cos de toutes les positions\n",
    "print(f\"\\nDimension 0 (sin) pour toutes les positions :\")\n",
    "print(f\"  {[f'{x:.3f}' for x in dim_0_all_pos.tolist()[:5]]}...\")\n",
    "print(f\"Dimension 1 (cos) pour toutes les positions :\")\n",
    "print(f\"  {[f'{x:.3f}' for x in dim_1_all_pos.tolist()[:5]]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4 : Impact du PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Exercice 4 : D√©monstration de l'impact du PE ===\")\n",
    "\n",
    "phrase_a = [\"Le\", \"chat\", \"mange\", \"la\", \"souris\"]\n",
    "phrase_b = [\"La\", \"souris\", \"mange\", \"le\", \"chat\"]\n",
    "\n",
    "# Vocabulaire simplifi√©\n",
    "vocab = {\"Le\": 0, \"chat\": 1, \"mange\": 2, \"la\": 3, \"souris\": 4, \"La\": 3, \"le\": 0}\n",
    "\n",
    "def get_embeddings_simple(tokens, vocab):\n",
    "    torch.manual_seed(42)\n",
    "    base = torch.randn(5, 100)\n",
    "    return torch.stack([base[vocab[t]] for t in tokens])\n",
    "\n",
    "emb_a = get_embeddings_simple(phrase_a, vocab)\n",
    "emb_b = get_embeddings_simple(phrase_b, vocab)\n",
    "\n",
    "# Positional encoding\n",
    "pe = get_positional_encoding(5, 100)\n",
    "\n",
    "# Embeddings + PE\n",
    "emb_a_pe = emb_a + pe\n",
    "emb_b_pe = emb_b + pe\n",
    "\n",
    "# Distances SANS PE\n",
    "print(\"\\nDistances SANS Positional Encoding :\")\n",
    "for i, (wa, wb) in enumerate(zip(phrase_a, phrase_b)):\n",
    "    dist = torch.norm(emb_a[i] - emb_b[i]).item()\n",
    "    print(f\"  Position {i}: '{wa}' vs '{wb}' ‚Üí {dist:.4f}\")\n",
    "\n",
    "print(\"\\nDistances AVEC Positional Encoding :\")\n",
    "for i, (wa, wb) in enumerate(zip(phrase_a, phrase_b)):\n",
    "    dist = torch.norm(emb_a_pe[i] - emb_b_pe[i]).item()\n",
    "    print(f\"  Position {i}: '{wa}' vs '{wb}' ‚Üí {dist:.4f}\")\n",
    "\n",
    "print(\"\\n‚Üí Avec PE, m√™me les mots identiques √† la m√™me position diff√®rent\")\n",
    "print(\"  car le contexte global (ordre des mots) est diff√©rent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation du Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation du PE\n",
    "pe_visu = get_positional_encoding(50, 64)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pe_visu.T, cmap='RdBu', aspect='auto')\n",
    "plt.xlabel('Position dans la s√©quence')\n",
    "plt.ylabel('Dimension')\n",
    "plt.title('Positional Encoding (sin/cos)')\n",
    "plt.colorbar(label='Valeur')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation :\")\n",
    "print(\"- Les basses fr√©quences (dimensions hautes) varient lentement\")\n",
    "print(\"- Les hautes fr√©quences (dimensions basses) varient rapidement\")\n",
    "print(\"- Chaque position a un pattern unique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 5. Similarit√© et Produit Scalaire\n\nAvant d'aborder l'attention, comprenons le lien entre **similarit√© cosinus** et **produit scalaire**.\n\n### Formules\n\n- **Produit scalaire** : $\\vec{a} \\cdot \\vec{b} = \\sum_i a_i b_i$\n\n- **Similarit√© cosinus** : $\\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{||\\vec{a}|| \\cdot ||\\vec{b}||}$\n\nLe produit scalaire mesure la \"compatibilit√©\" entre deux vecteurs."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison similarit√© cosinus vs produit scalaire\n",
    "print(\"=\" * 60)\n",
    "print(\"SIMILARIT√â COSINUS vs PRODUIT SCALAIRE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "v1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "v2 = torch.tensor([2.0, 3.0, 4.0])\n",
    "\n",
    "dot_product = torch.dot(v1, v2)\n",
    "cos_sim = F.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0))\n",
    "cos_sim_manual = dot_product / (torch.norm(v1) * torch.norm(v2))\n",
    "\n",
    "print(f\"\\nVecteur 1 : {v1}\")\n",
    "print(f\"Vecteur 2 : {v2}\")\n",
    "print(f\"\\nProduit scalaire : {dot_product:.4f}\")\n",
    "print(f\"Similarit√© cosinus : {cos_sim.item():.4f}\")\n",
    "print(f\"Similarit√© cosinus (manuel) : {cos_sim_manual:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Le produit scalaire Q¬∑K mesure la 'compatibilit√©' entre tokens\")\n",
    "print(\"   Le softmax transforme ces scores en probabilit√©s d'attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 6. Introduction √† l'Attention\n\n### La formule de l'attention\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\n| Composant | R√¥le |\n|-----------|------|\n| **Q** (Query) | Ce que je cherche |\n| **K** (Key) | Ce que j'offre pour √™tre trouv√© |\n| **V** (Value) | L'information que je transmets |\n\n### Analogie : Biblioth√®que\n\n- **Query** = Ta question (\"Je cherche un livre sur Python\")\n- **Key** = Les √©tiquettes des livres (\"Python\", \"Java\", \"Cuisine\"...)\n- **Value** = Le contenu des livres\n\nL'attention calcule **quels livres sont pertinents** pour ta question."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercice 6.1 : Calcul des scores"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Exercice 6.1 : Calcul des scores ===\")\n\n# Configuration\nseq_len = 3  # 3 tokens\nd_k = 4      # dimension des vecteurs\n\n# Cr√©er Q, K, V al√©atoires\ntorch.manual_seed(42)\nQ = torch.randn(seq_len, d_k)\nK = torch.randn(seq_len, d_k)\nV = torch.randn(seq_len, d_k)\n\nprint(\"Q (Queries) - Ce que chaque token cherche :\")\nprint(Q)\nprint(f\"\\nK (Keys) - Comment chaque token se pr√©sente :\")\nprint(K)\n\n# TODO: Calculer les scores d'attention : QK^T\n# Chaque ligne = un token qui \"interroge\" tous les autres\n# Indice: utilisez l'op√©rateur @ pour la multiplication matricielle\n# K.T transpose K pour avoir shape (d_k, seq_len)\nscores = None  # TODO\n\nif scores is not None:\n    print(f\"\\nScores (QK^T) - Compatibilit√© entre tokens :\")\n    print(scores)\n    print(f\"\\nShape: {scores.shape}  (3 tokens √ó 3 tokens)\")\nelse:\n    print(\"\\n‚ö†Ô∏è Calculez scores = Q @ K.T\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercice 6.2 : Scaling"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Exercice 6.2 : Scaling ===\")\n\n# Pourquoi diviser par sqrt(d_k) ?\n# ‚Üí √âviter que les scores soient trop grands (gradients instables)\n\n# TODO: Appliquer le scaling aux scores\n# Divisez par la racine carr√©e de d_k\n# Indice: math.sqrt(d_k) ou d_k ** 0.5\nscaled_scores = None  # TODO\n\nif scores is not None and scaled_scores is not None:\n    print(f\"Scores originaux :\")\n    print(scores)\n    print(f\"\\nScores apr√®s scaling (√∑‚àö{d_k} = √∑{math.sqrt(d_k):.2f}) :\")\n    print(scaled_scores)\n    print(f\"\\n‚Üí Les valeurs sont r√©duites, le softmax sera plus 'doux'\")\nelse:\n    print(\"‚ö†Ô∏è Calculez d'abord les scores (exercice 6.1), puis appliquez le scaling\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercice 6.3 : Softmax"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Exercice 6.3 : Softmax ===\")\n\n# Softmax transforme les scores en probabilit√©s\n# Chaque ligne somme √† 1\n\n# TODO: Appliquer softmax sur les scores scal√©s\n# Indice: F.softmax(tensor, dim=-1) applique softmax sur la derni√®re dimension\n# dim=-1 signifie qu'on normalise sur les colonnes (chaque ligne somme √† 1)\nattention_weights = None  # TODO\n\nif attention_weights is not None:\n    print(\"Poids d'attention (apr√®s softmax) :\")\n    print(attention_weights)\n\n    print(f\"\\nV√©rification - Somme par ligne :\")\n    print(attention_weights.sum(dim=1))\n    print(\"\\n‚Üí Chaque ligne = distribution de probabilit√©s sur les tokens\")\nelse:\n    print(\"‚ö†Ô∏è Appliquez F.softmax() sur scaled_scores\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercice 6.4 : Output"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Exercice 6.4 : Output ===\")\n\n# L'output = moyenne pond√©r√©e des Values par les poids d'attention\n\nprint(\"V (Values) - L'information de chaque token :\")\nprint(V)\n\n# TODO: Calculer l'output de l'attention\n# C'est la multiplication matricielle des poids d'attention avec V\n# Indice: output = attention_weights @ V\noutput = None  # TODO\n\nif output is not None:\n    print(f\"\\nOutput (weights @ V) :\")\n    print(output)\n    print(f\"\\nShape: {output.shape}  (m√™me que V)\")\n\n    print(\"\\n‚Üí Chaque token a maintenant une repr√©sentation enrichie\")\n    print(\"   qui int√®gre l'information des tokens 'pertinents'\")\nelse:\n    print(\"\\n‚ö†Ô∏è Calculez output = attention_weights @ V\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation de l'attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "tokens_demo = [\"Le\", \"chat\", \"dort\"]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention_weights.detach().numpy(), cmap='Blues')\n",
    "plt.xticks(range(3), tokens_demo)\n",
    "plt.yticks(range(3), tokens_demo)\n",
    "plt.xlabel(\"Tokens regard√©s (Keys)\")\n",
    "plt.ylabel(\"Tokens qui regardent (Queries)\")\n",
    "plt.title(\"Matrice d'attention\")\n",
    "plt.colorbar(label=\"Poids\")\n",
    "\n",
    "# Ajouter les valeurs\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        val = attention_weights[i, j].item()\n",
    "        plt.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
    "                color='white' if val > 0.5 else 'black', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## R√©capitulatif\n",
    "\n",
    "### Ce que nous avons appris\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Positional Encoding** | Encode la position avec sin/cos, valeurs ‚àà [-1, 1] |\n",
    "| **Q, K, V** | Query = question, Key = √©tiquette, Value = contenu |\n",
    "| **Scores** | QK^T = compatibilit√© entre tokens |\n",
    "| **Scaling** | Diviser par ‚àöd_k pour stabiliser |\n",
    "| **Softmax** | Transformer en probabilit√©s |\n",
    "| **Output** | Moyenne pond√©r√©e des Values |\n",
    "\n",
    "### Formule compl√®te\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### Prochaine session\n",
    "\n",
    "On va :\n",
    "1. Impl√©menter la **fonction d'attention compl√®te**\n",
    "2. Cr√©er une **classe SelfAttention** en PyTorch\n",
    "3. Visualiser l'attention sur un **vrai mod√®le** (DistilBERT)\n",
    "4. D√©couvrir le **Multi-Head Attention**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}