{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c# Comprendre le Positional Encoding (PE)\n",
    "\n",
    "**Note de synthèse**\n",
    "\n",
    "---\n",
    "\n",
    "## Pourquoi le PE est nécessaire ?\n",
    "\n",
    "Dans un Transformer, tous les mots sont traités **en parallèle** (pas séquentiellement comme un RNN). Sans information de position, le modèle ne distinguerait pas :\n",
    "\n",
    "- *\"Le chat mange la souris\"*\n",
    "- *\"La souris mange le chat\"*\n",
    "\n",
    "Les mêmes mots → les mêmes embeddings → même résultat. C'est un problème !\n",
    "\n",
    "**Solution** : Ajouter un vecteur de position (PE) à chaque embedding.\n",
    "\n",
    "```\n",
    "embedding_final = embedding_mot + PE[position]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## La formule du PE sinusoïdal\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "### Décortiquons chaque terme\n",
    "\n",
    "| Terme | Signification |\n",
    "|-------|---------------|\n",
    "| `PE` | Matrice de taille (seq_len × d_model) |\n",
    "| `pos` | Position **absolue** du mot dans la séquence (0, 1, 2, ...) |\n",
    "| `2i` / `2i+1` | Indice de la dimension (colonne) du vecteur |\n",
    "| `d_model` | Taille des embeddings (ex: 512) |\n",
    "| `10000` | Constante arbitraire (assez grande pour couvrir de longues séquences) |\n",
    "\n",
    "### Ce que ça donne concrètement\n",
    "\n",
    "Pour un mot à la position `pos`, on calcule un vecteur de `d_model` éléments :\n",
    "\n",
    "```\n",
    "PE[pos] = [élément_0, élément_1, élément_2, élément_3, ...]\n",
    "```\n",
    "\n",
    "- **Élément 0** (dim paire, i=0) : `sin(pos / 10000^(0/d_model))` = `sin(pos / 1)` = `sin(pos)`\n",
    "- **Élément 1** (dim impaire, i=0) : `cos(pos / 10000^(0/d_model))` = `cos(pos / 1)` = `cos(pos)`\n",
    "- **Élément 2** (dim paire, i=1) : `sin(pos / 10000^(2/d_model))`\n",
    "- **Élément 3** (dim impaire, i=1) : `cos(pos / 10000^(2/d_model))`\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Génère le positional encoding avec la formule sin/cos.\n",
    "    \n",
    "    Retourne une matrice (seq_len, d_model) où chaque ligne\n",
    "    est le vecteur PE pour une position donnée.\n",
    "    \"\"\"\n",
    "    position = torch.arange(seq_len).unsqueeze(1)  # (seq_len, 1)\n",
    "    div_term = torch.exp(\n",
    "        torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "    )  # (d_model/2,)\n",
    "    \n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)  # dimensions paires\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)  # dimensions impaires\n",
    "    \n",
    "    return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exemple détaillé : d_model=8, pos=3\n",
    "\n",
    "Calculons le vecteur PE pour le mot en position 3 avec des embeddings de taille 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 8\n",
    "pos = 3\n",
    "\n",
    "print(f\"PE pour position {pos} avec d_model={d_model}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for dim in range(d_model):\n",
    "    i = dim // 2  # i pour la formule\n",
    "    exposant = (2 * i) / d_model\n",
    "    diviseur = 10000 ** exposant\n",
    "    argument = pos / diviseur\n",
    "    \n",
    "    if dim % 2 == 0:  # dimension paire → sin\n",
    "        valeur = math.sin(argument)\n",
    "        formule = f\"sin({pos} / 10000^{exposant:.2f})\"\n",
    "        func = \"sin\"\n",
    "    else:  # dimension impaire → cos\n",
    "        valeur = math.cos(argument)\n",
    "        formule = f\"cos({pos} / 10000^{exposant:.2f})\"\n",
    "        func = \"cos\"\n",
    "    \n",
    "    print(f\"Dim {dim} | i={i} | {func}({pos}/{diviseur:>7.1f}) = {func}({argument:.4f}) = {valeur:>7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification avec notre fonction\n",
    "pe = get_positional_encoding(10, 8)\n",
    "print(f\"\\nVecteur PE[{pos}] complet :\")\n",
    "print(pe[pos].numpy().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Le rôle du diviseur : pourquoi les valeurs varient différemment\n",
    "\n",
    "Le diviseur `10000^(2i/d_model)` augmente avec l'indice de dimension :\n",
    "\n",
    "| Dimension | i | Exposant (2i/d_model) | Diviseur |\n",
    "|-----------|---|----------------------|----------|\n",
    "| 0-1 | 0 | 0 | 1 |\n",
    "| 2-3 | 1 | 0.25 | 10 |\n",
    "| 4-5 | 2 | 0.5 | 100 |\n",
    "| 6-7 | 3 | 0.75 | 1000 |\n",
    "\n",
    "**Conséquence** : l'argument de sin/cos devient de plus en plus petit pour les dimensions hautes.\n",
    "\n",
    "- Dimensions basses → `sin(pos)` → varie beaucoup entre positions\n",
    "- Dimensions hautes → `sin(pos/1000)` → varie peu entre positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison : comment chaque dimension varie selon la position\n",
    "print(\"Dimension 0 (diviseur=1) vs Dimension 6 (diviseur=1000)\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'pos':<5} {'dim 0 (sin(pos/1))':<22} {'dim 6 (sin(pos/1000))':<22}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for pos in [0, 1, 2, 3, 10, 100]:\n",
    "    val_dim0 = math.sin(pos / 1)\n",
    "    val_dim6 = math.sin(pos / 1000)\n",
    "    print(f\"{pos:<5} {val_dim0:<22.4f} {val_dim6:<22.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : \n",
    "- Dimension 0 : les valeurs changent beaucoup (0 → 0.84 → 0.91 → 0.14...)\n",
    "- Dimension 6 : les valeurs changent peu (0 → 0.001 → 0.002 → 0.003...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pourquoi utiliser sin ET cos ?\n",
    "\n",
    "Chaque paire (sin, cos) forme un **point sur un cercle** dans l'espace 2D :\n",
    "\n",
    "```\n",
    "Position 0 : (sin(0), cos(0)) = (0, 1)       → haut du cercle\n",
    "Position 1 : (sin(1), cos(1)) = (0.84, 0.54) → tourne...\n",
    "Position 2 : (sin(2), cos(2)) = (0.91, -0.42)\n",
    "...\n",
    "```\n",
    "\n",
    "C'est comme une **aiguille qui tourne** sur un cadran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation : le cercle sin/cos pour les dimensions 0-1\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "positions = range(20)\n",
    "\n",
    "# Cercle pour dimensions 0-1 (diviseur = 1)\n",
    "ax = axes[0]\n",
    "sins = [math.sin(p / 1) for p in positions]\n",
    "coss = [math.cos(p / 1) for p in positions]\n",
    "ax.scatter(sins, coss, c=positions, cmap='viridis', s=100)\n",
    "for p in positions:\n",
    "    ax.annotate(str(p), (sins[p], coss[p]), fontsize=8)\n",
    "ax.set_xlabel('sin (dim 0)')\n",
    "ax.set_ylabel('cos (dim 1)')\n",
    "ax.set_title('Dimensions 0-1 (diviseur=1)\\nTourne VITE')\n",
    "ax.set_xlim(-1.3, 1.3)\n",
    "ax.set_ylim(-1.3, 1.3)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Cercle pour dimensions 4-5 (diviseur = 100)\n",
    "ax = axes[1]\n",
    "sins = [math.sin(p / 100) for p in positions]\n",
    "coss = [math.cos(p / 100) for p in positions]\n",
    "ax.scatter(sins, coss, c=positions, cmap='viridis', s=100)\n",
    "for p in positions:\n",
    "    ax.annotate(str(p), (sins[p], coss[p]), fontsize=8)\n",
    "ax.set_xlabel('sin (dim 4)')\n",
    "ax.set_ylabel('cos (dim 5)')\n",
    "ax.set_title('Dimensions 4-5 (diviseur=100)\\nTourne LENTEMENT')\n",
    "ax.set_xlim(-0.3, 0.3)\n",
    "ax.set_ylim(0.95, 1.01)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pourquoi des diviseurs différents ? (L'analogie de l'horloge)\n",
    "\n",
    "C'est comme une horloge avec plusieurs aiguilles :\n",
    "\n",
    "| Dimensions | Diviseur | Analogie |\n",
    "|------------|----------|----------|\n",
    "| 0-1 | 1 | Trotteuse (secondes) - tourne vite |\n",
    "| 2-3 | 10 | Aiguille des minutes |\n",
    "| 4-5 | 100 | Aiguille des heures |\n",
    "| 6-7 | 1000 | Calendrier (jours) - tourne lentement |\n",
    "\n",
    "**Ensemble**, ces aiguilles créent une **signature unique** pour chaque instant (position).\n",
    "\n",
    "Pour que deux positions aient le même PE, il faudrait que **TOUTES** les aiguilles soient au même endroit simultanément. C'est quasi-impossible car les périodes ne sont pas des multiples entiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## À quoi sert le PE dans le Transformer ?\n",
    "\n",
    "Le PE est ajouté **une seule fois** au début, puis reste mélangé à l'embedding.\n",
    "\n",
    "```\n",
    "embedding_final = embedding_mot + PE[position]\n",
    "```\n",
    "\n",
    "### Impact sur l'attention\n",
    "\n",
    "Quand on calcule Q, K, V :\n",
    "\n",
    "```\n",
    "Q = W_q × (embedding + PE)    → contient l'info de position\n",
    "K = W_k × (embedding + PE)    → contient l'info de position\n",
    "\n",
    "Score = Q · K  → peut prendre en compte \"qui est où\"\n",
    "```\n",
    "\n",
    "**Sans PE** : l'attention ne sait pas si un mot est avant ou après un autre.\n",
    "\n",
    "**Avec PE** : le modèle peut apprendre des patterns comme :\n",
    "- \"Le sujet est souvent en position 0-1\"\n",
    "- \"L'adjectif est souvent juste avant le nom\"\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## L'impact de d_model sur le PE\n",
    "\n",
    "La formule est **conçue pour s'adapter** à n'importe quel d_model.\n",
    "\n",
    "Grâce à l'exposant `2i/d_model`, le diviseur va toujours de **1 à ~10000**, quelle que soit la taille de l'embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison d_model=64 vs d_model=512\n",
    "print(\"Plage des diviseurs selon d_model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for d_model in [64, 256, 512]:\n",
    "    div_min = 10000 ** (0 / d_model)\n",
    "    div_max = 10000 ** ((d_model - 2) / d_model)\n",
    "    n_frequences = d_model // 2\n",
    "    print(f\"d_model={d_model:3d} : diviseur de {div_min:.0f} à {div_max:.0f} ({n_frequences} paires sin/cos)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analogie** : C'est comme un escalier de même hauteur totale.\n",
    "\n",
    "```\n",
    "d_model=64  : ████████ (32 marches hautes)\n",
    "\n",
    "d_model=512 : ▁▂▃▄▅▆▇█ (256 marches basses)\n",
    "```\n",
    "\n",
    "Plus de dimensions = plus de \"marches\" = variations moins intenses entre dimensions consécutives, mais même couverture globale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Propriétés importantes du PE sinusoïdal\n",
    "\n",
    "### 1. Valeurs bornées\n",
    "Toutes les valeurs sont dans [-1, 1] (propriété de sin/cos).\n",
    "\n",
    "### 2. Chaque position a un vecteur unique\n",
    "Grâce aux multiples fréquences combinées.\n",
    "\n",
    "### 3. Distance constante entre positions consécutives\n",
    "La distance entre PE(pos) et PE(pos+k) dépend principalement de **k**, pas de **pos**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification : distance entre positions consécutives\n",
    "pe = get_positional_encoding(1000, 64)\n",
    "\n",
    "print(\"Distance entre positions consécutives (PE[pos] vs PE[pos+1])\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for pos in [0, 10, 100, 500, 998]:\n",
    "    dist = torch.norm(pe[pos] - pe[pos + 1]).item()\n",
    "    print(f\"pos={pos:3d} vs pos={pos+1:3d} : distance = {dist:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualisation complète du PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap du PE\n",
    "pe_visu = get_positional_encoding(100, 64)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(pe_visu.T, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
    "plt.xlabel('Position dans la séquence')\n",
    "plt.ylabel('Dimension du vecteur')\n",
    "plt.title('Positional Encoding (100 positions × 64 dimensions)')\n",
    "plt.colorbar(label='Valeur')\n",
    "\n",
    "# Annotations\n",
    "plt.axhline(y=1.5, color='white', linestyle='--', alpha=0.5)\n",
    "plt.text(50, 0, 'Dimensions 0-1 : oscillation rapide', color='white', ha='center', fontsize=9)\n",
    "plt.axhline(y=62.5, color='white', linestyle='--', alpha=0.5)\n",
    "plt.text(50, 63, 'Dimensions 62-63 : oscillation lente', color='white', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Limites du PE sinusoïdal\n",
    "\n",
    "### 1. Position absolue, pas relative\n",
    "Le PE encode \"je suis en position 5\", pas \"je suis 2 positions après le verbe\". Le modèle doit apprendre les relations relatives à partir des positions absolues.\n",
    "\n",
    "### 2. Généralisation limitée\n",
    "Si le modèle est entraîné sur des séquences de 512 tokens, il peut avoir du mal avec des séquences de 2048 tokens (positions jamais vues).\n",
    "\n",
    "### 3. Pas de sémantique\n",
    "Le PE ne sait pas que la position du sujet est \"importante\". C'est le modèle qui doit l'apprendre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Alternatives modernes au PE sinusoïdal\n",
    "\n",
    "### 1. Learned Positional Embeddings (BERT, GPT-2)\n",
    "\n",
    "Au lieu de calculer le PE avec sin/cos, on **apprend** un vecteur pour chaque position.\n",
    "\n",
    "```python\n",
    "self.position_embeddings = nn.Embedding(max_position, d_model)\n",
    "# Matrice apprise de taille (max_position × d_model)\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- Le modèle apprend exactement ce qui est utile\n",
    "- Simple à implémenter\n",
    "\n",
    "**Inconvénients** :\n",
    "- Limité à `max_position` (pas de généralisation au-delà)\n",
    "- Plus de paramètres\n",
    "\n",
    "---\n",
    "\n",
    "### 2. RoPE - Rotary Position Embedding (LLaMA, Mistral)\n",
    "\n",
    "Au lieu d'**ajouter** le PE à l'embedding, on **tourne** les vecteurs Q et K dans l'espace.\n",
    "\n",
    "```\n",
    "Q_rotated = rotate(Q, position)\n",
    "K_rotated = rotate(K, position)\n",
    "```\n",
    "\n",
    "**Idée clé** : Le produit scalaire `Q · K` devient sensible à la **position relative** (pas absolue).\n",
    "\n",
    "**Avantages** :\n",
    "- Encode naturellement les positions relatives\n",
    "- Meilleure généralisation sur les longues séquences\n",
    "- Utilisé par les modèles les plus performants (LLaMA, Mistral, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ALiBi - Attention with Linear Biases (BLOOM)\n",
    "\n",
    "Pas de PE du tout ! On ajoute un **biais** directement aux scores d'attention.\n",
    "\n",
    "```\n",
    "Score[i, j] = Q[i] · K[j] - m × |i - j|\n",
    "```\n",
    "\n",
    "Plus deux tokens sont éloignés, plus le score est pénalisé.\n",
    "\n",
    "**Avantages** :\n",
    "- Très simple\n",
    "- Excellente généralisation (peut traiter des séquences plus longues que l'entraînement)\n",
    "- Pas de paramètres supplémentaires\n",
    "\n",
    "**Inconvénients** :\n",
    "- Biais vers les tokens proches (peut manquer des dépendances longues)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Relative Position Embeddings (T5, Transformer-XL)\n",
    "\n",
    "Apprend des embeddings pour les **distances relatives** plutôt que les positions absolues.\n",
    "\n",
    "```\n",
    "Score[i, j] = Q[i] · K[j] + position_bias[i - j]\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- Encode directement \"2 positions avant/après\"\n",
    "- Meilleure généralisation\n",
    "\n",
    "**Inconvénients** :\n",
    "- Plus complexe à implémenter\n",
    "- Nécessite de limiter la distance max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tableau récapitulatif\n",
    "\n",
    "| Méthode | Utilisé par | Position | Généralisation | Complexité |\n",
    "|---------|-------------|----------|----------------|------------|\n",
    "| **Sinusoïdal** | Transformer original | Absolue | Moyenne | Simple |\n",
    "| **Learned** | BERT, GPT-2 | Absolue | Limitée (max_pos) | Simple |\n",
    "| **RoPE** | LLaMA, Mistral | Relative | Bonne | Moyenne |\n",
    "| **ALiBi** | BLOOM | Relative | Excellente | Très simple |\n",
    "| **Relative PE** | T5 | Relative | Bonne | Complexe |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Le PE sinusoïdal est une solution élégante pour encoder la position :\n",
    "\n",
    "1. **Simple** : juste des sin/cos, pas de paramètres à apprendre\n",
    "2. **Unique** : chaque position a une signature distincte\n",
    "3. **Borné** : valeurs entre -1 et 1\n",
    "\n",
    "Mais les architectures modernes préfèrent des méthodes qui encodent les **positions relatives** (RoPE, ALiBi) car elles généralisent mieux sur les longues séquences.\n",
    "\n",
    "Le PE est ajouté une fois au début et \"disparaît\" dans les calculs suivants. C'est le modèle (via W_q, W_k, W_v) qui apprend à exploiter cette information de position."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
