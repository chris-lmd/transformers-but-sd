{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet B - Recherche d'Images par Texte avec CLIP\n",
    "\n",
    "**Module** : R√©seaux de Neurones Approfondissement  \n",
    "**Dur√©e** : 2h  \n",
    "**Objectif** : D√©couvrir le multimodal avec CLIP (OpenAI)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs du projet\n",
    "\n",
    "Dans ce projet, vous allez :\n",
    "1. Comprendre comment CLIP relie images et texte\n",
    "2. Faire de la classification zero-shot (sans entra√Ænement)\n",
    "3. Construire un moteur de recherche d'images par texte\n",
    "4. Explorer les capacit√©s et limites du mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision ftfy regex matplotlib numpy Pillow requests tqdm -q\n",
    "!pip install git+https://github.com/openai/CLIP.git -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Charger CLIP\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "print(f\"CLIP charg√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Comment fonctionne CLIP ?\n",
    "\n",
    "CLIP (Contrastive Language-Image Pre-training) apprend √† aligner images et textes dans un m√™me espace vectoriel.\n",
    "\n",
    "```\n",
    "Image ‚Üí Image Encoder ‚Üí Embedding image (512 dim)\n",
    "                                    ‚Üì\n",
    "                              Similarit√© cosinus\n",
    "                                    ‚Üë\n",
    "Texte ‚Üí Text Encoder ‚Üí Embedding texte (512 dim)\n",
    "```\n",
    "\n",
    "### Principe\n",
    "- Images et textes similaires ‚Üí Embeddings proches\n",
    "- Images et textes diff√©rents ‚Üí Embeddings √©loign√©s\n",
    "\n",
    "### Entra√Ænement\n",
    "CLIP a √©t√© entra√Æn√© sur 400 millions de paires (image, texte) du web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Premier exemple : Classification Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger une image depuis une URL\n",
    "def load_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "# Image exemple : un chat\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\"\n",
    "image = load_image_from_url(url)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Image √† classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification zero-shot\n",
    "# On compare l'image √† plusieurs descriptions textuelles\n",
    "\n",
    "# Pr√©parer l'image\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Labels candidats\n",
    "labels = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a bird\", \"a photo of a car\"]\n",
    "text_inputs = clip.tokenize(labels).to(device)\n",
    "\n",
    "# Encoder\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "# Normaliser\n",
    "image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Similarit√©\n",
    "similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Classification Zero-Shot:\")\n",
    "for label, prob in zip(labels, similarity[0]):\n",
    "    print(f\"  {label}: {prob.item():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Sans aucun entra√Ænement sp√©cifique, CLIP reconna√Æt correctement le chat !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Exercice : Classification personnalis√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 1 : Cr√©er votre propre classifieur\n",
    "# ============================================\n",
    "\n",
    "def zero_shot_classify(image, labels, model, preprocess, device):\n",
    "    \"\"\"\n",
    "    Classification zero-shot avec CLIP.\n",
    "    \n",
    "    Args:\n",
    "        image: Image PIL\n",
    "        labels: Liste de descriptions textuelles\n",
    "        model: Mod√®le CLIP\n",
    "        preprocess: Fonction de pr√©traitement\n",
    "        device: Device (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        dict avec labels et probabilit√©s\n",
    "    \"\"\"\n",
    "    # TODO: Impl√©menter\n",
    "    \n",
    "    # 1. Pr√©traiter l'image\n",
    "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 2. Tokenizer les labels\n",
    "    text_inputs = clip.tokenize(labels).to(device)\n",
    "    \n",
    "    # 3. Encoder image et texte\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    \n",
    "    # 4. Normaliser\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # 5. Calculer similarit√© + softmax\n",
    "    similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
    "    \n",
    "    # 6. Retourner r√©sultats\n",
    "    results = {}\n",
    "    for label, prob in zip(labels, similarity[0]):\n",
    "        results[label] = prob.item()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec diff√©rentes images\n",
    "urls = [\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/1200px-YellowLabradorLooking_new.jpg\",  # Chien\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a7/Camponotus_flavomarginatus_ant.jpg/1200px-Camponotus_flavomarginatus_ant.jpg\",  # Fourmi\n",
    "]\n",
    "\n",
    "labels_test = [\n",
    "    \"a photo of a dog\",\n",
    "    \"a photo of a cat\", \n",
    "    \"a photo of an insect\",\n",
    "    \"a photo of a person\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    img = load_image_from_url(url)\n",
    "    results = zero_shot_classify(img, labels_test, model, preprocess, device)\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "    best_label = max(results, key=results.get)\n",
    "    axes[idx].set_title(f\"{best_label}\\n({results[best_label]:.1%})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Moteur de Recherche d'Images par Texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une base d'images\n",
    "image_urls = {\n",
    "    \"chat_roux\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\",\n",
    "    \"chien_labrador\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/1200px-YellowLabradorLooking_new.jpg\",\n",
    "    \"tour_eiffel\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/85/Tour_Eiffel_Wikimedia_Commons_%28cropped%29.jpg/800px-Tour_Eiffel_Wikimedia_Commons_%28cropped%29.jpg\",\n",
    "    \"plage_tropicale\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/75/Maldives_beach.JPG/1200px-Maldives_beach.JPG\",\n",
    "    \"montagne_neige\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Everest_North_Face_toward_Base_Camp_Tibet_Luca_Galuzzi_2006.jpg/1200px-Everest_North_Face_toward_Base_Camp_Tibet_Luca_Galuzzi_2006.jpg\",\n",
    "    \"voiture_sport\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Ferrari_F40_in_Monterey.jpg/1200px-Ferrari_F40_in_Monterey.jpg\",\n",
    "}\n",
    "\n",
    "# Charger et afficher\n",
    "images = {}\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, url) in enumerate(image_urls.items()):\n",
    "    try:\n",
    "        img = load_image_from_url(url)\n",
    "        images[name] = img\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(name.replace('_', ' '))\n",
    "        axes[idx].axis('off')\n",
    "    except:\n",
    "        print(f\"Erreur chargement: {name}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"\\n{len(images)} images charg√©es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXERCICE 2 : Construire l'index d'embeddings\n",
    "# ============================================\n",
    "\n",
    "class ImageSearchEngine:\n",
    "    \"\"\"\n",
    "    Moteur de recherche d'images par texte avec CLIP.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, preprocess, device):\n",
    "        self.model = model\n",
    "        self.preprocess = preprocess\n",
    "        self.device = device\n",
    "        self.image_embeddings = None\n",
    "        self.image_names = []\n",
    "        self.images = {}\n",
    "    \n",
    "    def index_images(self, images_dict):\n",
    "        \"\"\"\n",
    "        Indexe une collection d'images.\n",
    "        \n",
    "        Args:\n",
    "            images_dict: dict {nom: image_PIL}\n",
    "        \"\"\"\n",
    "        self.images = images_dict\n",
    "        self.image_names = list(images_dict.keys())\n",
    "        \n",
    "        embeddings = []\n",
    "        \n",
    "        print(\"Indexation des images...\")\n",
    "        for name, img in tqdm(images_dict.items()):\n",
    "            # TODO: Encoder chaque image\n",
    "            img_input = self.preprocess(img).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                img_embedding = self.model.encode_image(img_input)\n",
    "            \n",
    "            # Normaliser\n",
    "            img_embedding = img_embedding / img_embedding.norm(dim=-1, keepdim=True)\n",
    "            embeddings.append(img_embedding)\n",
    "        \n",
    "        # Empiler tous les embeddings\n",
    "        self.image_embeddings = torch.cat(embeddings, dim=0)\n",
    "        print(f\"Index cr√©√©: {self.image_embeddings.shape}\")\n",
    "    \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        Recherche les images les plus similaires √† une requ√™te texte.\n",
    "        \n",
    "        Args:\n",
    "            query: Texte de recherche\n",
    "            top_k: Nombre de r√©sultats\n",
    "        \n",
    "        Returns:\n",
    "            Liste de (nom, score, image)\n",
    "        \"\"\"\n",
    "        # TODO: Impl√©menter la recherche\n",
    "        \n",
    "        # 1. Encoder la requ√™te\n",
    "        text_input = clip.tokenize([query]).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_embedding = self.model.encode_text(text_input)\n",
    "        \n",
    "        text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # 2. Calculer similarit√© avec toutes les images\n",
    "        similarities = (text_embedding @ self.image_embeddings.T).squeeze(0)\n",
    "        \n",
    "        # 3. Trier et retourner top_k\n",
    "        top_indices = similarities.argsort(descending=True)[:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            name = self.image_names[idx]\n",
    "            score = similarities[idx].item()\n",
    "            results.append((name, score, self.images[name]))\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le moteur de recherche\n",
    "search_engine = ImageSearchEngine(model, preprocess, device)\n",
    "search_engine.index_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de recherche\n",
    "queries = [\n",
    "    \"a cute animal\",\n",
    "    \"a famous monument in Paris\",\n",
    "    \"a tropical vacation destination\",\n",
    "    \"a fast red sports car\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nüîç Recherche: '{query}'\")\n",
    "    results = search_engine.search(query, top_k=2)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    for idx, (name, score, img) in enumerate(results):\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f\"{name}\\n(score: {score:.3f})\")\n",
    "        axes[idx].axis('off')\n",
    "    plt.suptitle(f\"Requ√™te: {query}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Recherche en Fran√ßais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP comprend un peu le fran√ßais (entra√Æn√© sur web multilingue)\n",
    "queries_fr = [\n",
    "    \"un chat mignon\",\n",
    "    \"la tour eiffel √† paris\",\n",
    "    \"une plage paradisiaque\",\n",
    "    \"une voiture de course\",\n",
    "]\n",
    "\n",
    "for query in queries_fr:\n",
    "    results = search_engine.search(query, top_k=1)\n",
    "    name, score, _ = results[0]\n",
    "    print(f\"'{query}' ‚Üí {name} (score: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour de meilleurs r√©sultats en fran√ßais, on peut utiliser un traducteur\n",
    "# comme dans le projet A\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_fr_to_en\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "\n",
    "def search_french(engine, query_fr, top_k=3):\n",
    "    \"\"\"Recherche avec traduction FR -> EN.\"\"\"\n",
    "    query_en = translator(query_fr, max_length=100)[0]['translation_text']\n",
    "    print(f\"FR: {query_fr}\")\n",
    "    print(f\"EN: {query_en}\")\n",
    "    return engine.search(query_en, top_k)\n",
    "\n",
    "# Test\n",
    "results = search_french(search_engine, \"un animal domestique adorable\")\n",
    "print(f\"\\nR√©sultat: {results[0][0]} (score: {results[0][1]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Analyse des embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisons la similarit√© entre toutes les paires d'images\n",
    "similarity_matrix = (search_engine.image_embeddings @ search_engine.image_embeddings.T).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(similarity_matrix, \n",
    "            xticklabels=[n.replace('_', '\\n') for n in search_engine.image_names],\n",
    "            yticklabels=[n.replace('_', '\\n') for n in search_engine.image_names],\n",
    "            annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title(\"Similarit√© entre images (embeddings CLIP)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation t-SNE des embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Ajouter quelques embeddings de texte\n",
    "texts = [\"a cat\", \"a dog\", \"a building\", \"a beach\", \"a mountain\", \"a car\"]\n",
    "text_inputs = clip.tokenize(texts).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = model.encode_text(text_inputs)\n",
    "    text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Combiner image et texte embeddings\n",
    "all_embeddings = torch.cat([search_engine.image_embeddings, text_embeddings], dim=0).cpu().numpy()\n",
    "all_labels = search_engine.image_names + texts\n",
    "all_types = ['image'] * len(search_engine.image_names) + ['text'] * len(texts)\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=3, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['blue' if t == 'image' else 'red' for t in all_types]\n",
    "markers = ['o' if t == 'image' else '^' for t in all_types]\n",
    "\n",
    "for i, (x, y) in enumerate(embeddings_2d):\n",
    "    plt.scatter(x, y, c=colors[i], marker=markers[i], s=100)\n",
    "    plt.annotate(all_labels[i].replace('_', ' '), (x, y), fontsize=9)\n",
    "\n",
    "plt.title(\"Espace des embeddings CLIP (t-SNE)\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend(['Images (bleu)', 'Textes (rouge)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** : Les images et textes similaires sont proches dans l'espace des embeddings !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Exercices\n",
    "\n",
    "### Exercice 3 : √âtendre la base d'images\n",
    "Ajoutez plus d'images et testez des requ√™tes plus complexes.\n",
    "\n",
    "### Exercice 4 : Cr√©er une d√©mo interactive\n",
    "Permettez √† l'utilisateur de saisir une requ√™te.\n",
    "\n",
    "### Exercice 5 : Analyser les limites\n",
    "Trouvez des cas o√π CLIP se trompe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espace pour vos exp√©rimentations\n",
    "\n",
    "# Exercice 3 : Ajouter des images\n",
    "# nouvelles_images = {\n",
    "#     \"nom\": load_image_from_url(\"url\"),\n",
    "# }\n",
    "\n",
    "# Exercice 4 : D√©mo interactive\n",
    "def demo_recherche():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RECHERCHE D'IMAGES - D√©mo Interactive\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Entrez une description textuelle\")\n",
    "    print(\"Tapez 'quit' pour quitter\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nRecherche: \")\n",
    "        if query.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        results = search_engine.search(query, top_k=3)\n",
    "        \n",
    "        print(f\"\\nR√©sultats pour '{query}':\")\n",
    "        for name, score, _ in results:\n",
    "            print(f\"  {name}: {score:.3f}\")\n",
    "\n",
    "# D√©commenter pour tester\n",
    "# demo_recherche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 5 : Cas limites\n",
    "# Testez ces requ√™tes ambigu√´s ou difficiles\n",
    "\n",
    "difficult_queries = [\n",
    "    \"something blue\",  # Ambigu\n",
    "    \"a happy scene\",   # Subjectif\n",
    "    \"danger\",          # Abstrait\n",
    "    \"the number 5\",    # Conceptuel\n",
    "]\n",
    "\n",
    "print(\"Tests de requ√™tes difficiles:\")\n",
    "for query in difficult_queries:\n",
    "    results = search_engine.search(query, top_k=1)\n",
    "    name, score, _ = results[0]\n",
    "    print(f\"  '{query}' ‚Üí {name} ({score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Conclusion\n",
    "\n",
    "### Ce que vous avez appris\n",
    "\n",
    "1. **CLIP** aligne images et textes dans un m√™me espace vectoriel\n",
    "2. **Zero-shot** : Classification sans entra√Ænement sp√©cifique\n",
    "3. **Recherche s√©mantique** : Trouver des images par description\n",
    "4. **Multimodal** : Combiner vision et langage\n",
    "\n",
    "### Lien avec l'actualit√©\n",
    "\n",
    "CLIP est √† la base de :\n",
    "- **DALL-E** : G√©n√©ration d'images √† partir de texte\n",
    "- **Midjourney** : Art g√©n√©ratif\n",
    "- **Stable Diffusion** : Images open-source\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Biais du dataset (web anglophone)\n",
    "- Difficult√© avec concepts abstraits\n",
    "- Ne comprend pas le contexte/l'humour\n",
    "\n",
    "### Pour aller plus loin\n",
    "\n",
    "- Explorer d'autres mod√®les CLIP (ViT-L/14, etc.)\n",
    "- Utiliser CLIP pour filtrer des images\n",
    "- Combiner avec g√©n√©ration d'images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
