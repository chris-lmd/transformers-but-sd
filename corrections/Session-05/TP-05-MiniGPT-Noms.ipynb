{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 5 - Bloc Transformer & Mini-GPT (Partie 1)\n",
    "\n",
    "**Module** : Réseaux de Neurones Approfondissement  \n",
    "**Durée** : 2h  \n",
    "**Objectif** : Assembler un bloc Transformer complet et commencer à générer des noms de Pokémon\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs pédagogiques\n",
    "\n",
    "À la fin de cette session, vous serez capable de :\n",
    "1. Assembler un **bloc Transformer** complet (MHA + FFN + résiduel)\n",
    "2. Comprendre l'architecture d'un **décodeur** (GPT)\n",
    "3. Créer un **tokenizer character-level**\n",
    "4. Entraîner un **Mini-GPT** pour générer des noms de Pokémon\n",
    "\n",
    "---\n",
    "\n",
    "## Rappel : Où en sommes-nous ?\n",
    "\n",
    "Sessions précédentes :\n",
    "- ✅ `scaled_dot_product_attention()` \n",
    "- ✅ `MultiHeadAttention` (avec split/concat heads)\n",
    "- ✅ `FeedForward` Network\n",
    "- ✅ LayerNorm + Résiduel\n",
    "- ✅ Masque causal (GPT vs BERT)\n",
    "\n",
    "Aujourd'hui, on **assemble tout** et on passe à la **génération** !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib numpy datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU disponible: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CPU uniquement\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Rappel : Nos briques de base\n",
    "\n",
    "Chargeons les composants des sessions précédentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"Scaled Dot-Product Attention.\"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = attention_weights @ V\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def concat_heads(self, x):\n",
    "        batch_size, _, seq_len, _ = x.shape\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        return x.view(batch_size, seq_len, self.embed_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        Q = self.split_heads(self.W_q(x))\n",
    "        K = self.split_heads(self.W_k(x))\n",
    "        V = self.split_heads(self.W_v(x))\n",
    "        \n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        concat_output = self.concat_heads(attn_output)\n",
    "        output = self.W_o(concat_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-Forward Network.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, ff_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        if ff_dim is None:\n",
    "            ff_dim = 4 * embed_dim\n",
    "        \n",
    "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"✅ Composants chargés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Exercice 1 : Assembler le bloc Transformer\n",
    "\n",
    "Un bloc Transformer combine :\n",
    "1. **Multi-Head Attention** + résiduel + LayerNorm\n",
    "2. **Feed-Forward** + résiduel + LayerNorm\n",
    "\n",
    "### Architecture (Pre-LN)\n",
    "\n",
    "```\n",
    "Input\n",
    "  │\n",
    "  ├───────────────────┐\n",
    "  ↓                   │\n",
    "LayerNorm             │\n",
    "  ↓                   │\n",
    "Multi-Head Attention  │\n",
    "  ↓                   │\n",
    "Dropout               │\n",
    "  ↓                   │\n",
    "  + ←─────────────────┘ (résiduel)\n",
    "  │\n",
    "  ├───────────────────┐\n",
    "  ↓                   │\n",
    "LayerNorm             │\n",
    "  ↓                   │\n",
    "Feed-Forward          │\n",
    "  ↓                   │\n",
    "Dropout               │\n",
    "  ↓                   │\n",
    "  + ←─────────────────┘ (résiduel)\n",
    "  ↓\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Un bloc Transformer complet.\n",
    "    \n",
    "    Architecture : Pre-LN (LayerNorm AVANT attention/FFN), plus stable \n",
    "    à l'entraînement que Post-LN (LayerNorm après).\n",
    "    \n",
    "    Le dropout désactive aléatoirement des neurones pendant l'entraînement\n",
    "    (régularisation pour éviter l'overfitting). Désactivé à l'inférence.\n",
    "    \n",
    "    Args:\n",
    "        embed_dim: Dimension des embeddings\n",
    "        num_heads: Nombre de têtes d'attention\n",
    "        ff_dim: Dimension du feed-forward (défaut: 4 × embed_dim)\n",
    "        dropout: Taux de dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        if ff_dim is None:\n",
    "            ff_dim = 4 * embed_dim\n",
    "        \n",
    "        # TODO 1: Créer les composants\n",
    "        # - self.attention : MultiHeadAttention(embed_dim, num_heads)\n",
    "        # - self.feed_forward : FeedForward(embed_dim, ff_dim, dropout)\n",
    "        # - self.norm1, self.norm2 : nn.LayerNorm(embed_dim)\n",
    "        # - self.dropout1, self.dropout2 : nn.Dropout(dropout)\n",
    "        \n",
    "        self.attention = ...\n",
    "        self.feed_forward = ...\n",
    "        self.norm1 = ...\n",
    "        self.norm2 = ...\n",
    "        self.dropout1 = ...\n",
    "        self.dropout2 = ...\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input, shape (batch, seq_len, embed_dim)\n",
    "            mask: Masque optionnel (causal pour GPT)\n",
    "        \n",
    "        Returns:\n",
    "            output: shape (batch, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        # TODO 2: Implémenter le forward (Pre-LN)\n",
    "        # Suivre le schéma ci-dessus :\n",
    "        # 1. Sauvegarder x dans residual\n",
    "        # 2. Appliquer norm1, puis attention (attention retourne output, weights)\n",
    "        # 3. Appliquer dropout1\n",
    "        # 4. Ajouter le résiduel\n",
    "        # 5. Répéter pour feed_forward (norm2, ff, dropout2, résiduel)\n",
    "        \n",
    "        # Bloc 1: Attention avec résiduel\n",
    "        residual = x\n",
    "        x = ...  # norm1\n",
    "        x, _ = ...  # attention (retourne output, weights)\n",
    "        x = ...  # dropout1\n",
    "        x = ...  # addition résiduelle\n",
    "        \n",
    "        # Bloc 2: Feed-Forward avec résiduel\n",
    "        residual = x\n",
    "        x = ...  # norm2\n",
    "        x = ...  # feed_forward\n",
    "        x = ...  # dropout2\n",
    "        x = ...  # addition résiduelle\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "block = TransformerBlock(embed_dim=64, num_heads=4, dropout=0.1)\n",
    "\n",
    "x = torch.randn(2, 10, 64)  # batch=2, seq=10, embed=64\n",
    "out = block(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")    # (2, 10, 64)\n",
    "print(f\"Output shape: {out.shape}\")  # Attendu: (2, 10, 64)\n",
    "\n",
    "if out is not None and out.shape == (2, 10, 64):\n",
    "    print(\"\\n✅ Correct !\")\n",
    "else:\n",
    "    print(\"\\n❌ Vérifiez votre implémentation\")\n",
    "\n",
    "# Paramètres\n",
    "n_params = sum(p.numel() for p in block.parameters())\n",
    "print(f\"\\nParamètres du bloc: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Vous savez construire un Transformer !\n",
    "\n",
    "### Récapitulatif\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                      TRANSFORMER                            │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│  Tokens → Embedding → Positional Encoding                   │\n",
    "│                          ↓                                  │\n",
    "│  ┌─────────────────────────────────────────┐                │\n",
    "│  │         TransformerBlock × N            │                │\n",
    "│  │  ┌─────────────────────────────────┐    │                │\n",
    "│  │  │  LayerNorm → MHA → Dropout → +  │    │                │\n",
    "│  │  │  LayerNorm → FFN → Dropout → +  │    │                │\n",
    "│  │  └─────────────────────────────────┘    │                │\n",
    "│  └─────────────────────────────────────────┘                │\n",
    "│                          ↓                                  │\n",
    "│                    LayerNorm final                          │\n",
    "│                          ↓                                  │\n",
    "│                   Couche de sortie                          │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Maintenant, utilisons-le pour générer du texte !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTIE 2 : Mini-GPT - Générer des noms de Pokémon\n",
    "\n",
    "## 4. Introduction au projet\n",
    "\n",
    "### Objectif\n",
    "\n",
    "Créer un **Mini-GPT** qui génère des **noms de Pokémon** inventés !\n",
    "\n",
    "```\n",
    "Input:  \"Pika\"    →  Output: \"Pikarion\"\n",
    "Input:  \"Dra\"     →  Output: \"Dracofeu\"\n",
    "Input:  \"Flam\"    →  Output: \"Flamirex\"\n",
    "```\n",
    "\n",
    "### Pourquoi character-level ?\n",
    "\n",
    "- **Simple** : Vocabulaire de ~50 caractères (vs 50k tokens pour GPT-2)\n",
    "- **Rapide** : Entraînement en quelques minutes\n",
    "- **Pédagogique** : On voit clairement la génération caractère par caractère\n",
    "- **Fun** : Les noms inventés sont souvent amusants !\n",
    "\n",
    "### Dataset\n",
    "\n",
    "**1211 noms de Pokémon** en français (Pikachu, Dracaufeu, Mewtwo...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Chargement du dataset des noms de Pokémon...\")\n",
    "dataset = load_dataset(\"chris-lmd/pokemon-names-fr\")\n",
    "\n",
    "# Extraire les noms\n",
    "pokemon_names = [item[\"name\"] for item in dataset[\"train\"]]\n",
    "\n",
    "print(f\"\\n✅ {len(pokemon_names)} noms chargés\")\n",
    "print(f\"\\nExemples :\")\n",
    "for name in pokemon_names[:10]:\n",
    "    print(f\"  - {name}\")\n",
    "print(f\"  ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice libre : Exploration du dataset\n",
    "\n",
    "Objectif : produire des statistiques et des visualisations sur les noms de Pokémon.\n",
    "\n",
    "Tâches proposées :\n",
    "- Calculer des statistiques de longueur de nom de pokemon : min, max, moyenne, médiane, quartiles.\n",
    "- Tracer l'histogramme des longueurs .\n",
    "- Analyser la distribution des premières et dernières lettres.\n",
    "- Quels sont les pokemons avec les noms le plus petit et le plus grand ?\n",
    "- Calculer et visualiser la fréquence des caractères (top K). Quelles sont les lettres les plus utilisées pour les noms de Pokemon ? \n",
    "- Extraire et analyser les bigrammes et trigrammes les plus fréquents.\n",
    "- Proposer une visualisation libre supplémentaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice : Statistiques de longueur des noms\n",
    "# Objectif : calculer et visualiser la distribution des longueurs\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) TODO : Construire la liste des longueurs (en caractères)\n",
    "# lengths = [len(name) for name in pokemon_names]\n",
    "\n",
    "# 2) TODO : Calculer et afficher des statistiques\n",
    "# - min, max, moyenne, médiane\n",
    "\n",
    "\n",
    "# 3) TODO : Tracer l'histogramme des longueurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice : Fréquence des caractères (top K)\n",
    "# Objectif : compter les caractères et visualiser leur fréquence\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO : Construire la liste de tous les caractères\n",
    "\n",
    "# TODO : Compter et choisir K\n",
    "\n",
    "\n",
    "# TODO : Préparer les données pour la visualisation\n",
    "\n",
    "# TODO : Tracer le bar chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice : Bigrammes et trigrammes les plus fréquents\n",
    "# Objectif : extraire, compter et visualiser les n-grammes\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bigramme def: https://fr.wikipedia.org/wiki/Bigramme\n",
    "\n",
    "# Helper pour générer des n-grammes\n",
    "def ngrams(s, n):\n",
    "    return [s[i:i+n] for i in range(len(s) - n + 1)]\n",
    "\n",
    "# TODO : Construire les listes de bigrammes et trigrammes\n",
    "\n",
    "\n",
    "# TODO : Compter les fréquences\n",
    "\n",
    "# TODO : Visualiser le top K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice : Distribution des premières et dernières lettres\n",
    "# Objectif : compter et visualiser la fréquence des premières/dernières lettres\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO : Extraire les premières et dernières lettres (ignorer les chaînes vides)\n",
    "\n",
    "\n",
    "# TODO : Compter les fréquences\n",
    "# TODO : Visualiser (bar charts côte à côte)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Tokenizer character-level\n",
    "\n",
    "### Principe\n",
    "\n",
    "Chaque **caractère** = un token\n",
    "\n",
    "```\n",
    "\"Pikachu\" → ['P', 'i', 'k', 'a', 'c', 'h', 'u'] → [23, 12, 14, 4, 6, 11, 24]\n",
    "```\n",
    "\n",
    "### Vocabulaire\n",
    "\n",
    "On ajoute des tokens spéciaux :\n",
    "- `<PAD>` : Padding (remplissage)\n",
    "- `<BOS>` : Begin Of Sequence (début)\n",
    "- `<EOS>` : End Of Sequence (fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer character-level simple.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts):\n",
    "        \"\"\"\n",
    "        Construit le vocabulaire à partir d'une liste de textes.\n",
    "        \"\"\"\n",
    "        # Tokens spéciaux\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.bos_token = \"<BOS>\"\n",
    "        self.eos_token = \"<EOS>\"\n",
    "        \n",
    "        # Collecter tous les caractères uniques\n",
    "        all_chars = set()\n",
    "        for text in texts:\n",
    "            all_chars.update(text)\n",
    "        \n",
    "        # Créer le vocabulaire\n",
    "        special_tokens = [self.pad_token, self.bos_token, self.eos_token]\n",
    "        chars = sorted(list(all_chars))\n",
    "        \n",
    "        self.vocab = special_tokens + chars\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        # Mappings\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.idx_to_char = {i: c for i, c in enumerate(self.vocab)}\n",
    "        \n",
    "        # IDs des tokens spéciaux\n",
    "        self.pad_id = self.char_to_idx[self.pad_token]\n",
    "        self.bos_id = self.char_to_idx[self.bos_token]\n",
    "        self.eos_id = self.char_to_idx[self.eos_token]\n",
    "    \n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        \"\"\"\n",
    "        Convertit un texte en liste d'indices.\n",
    "        \"\"\"\n",
    "        ids = [self.char_to_idx.get(c, self.pad_id) for c in text]\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            ids = [self.bos_id] + ids + [self.eos_id]\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids, skip_special_tokens=True):\n",
    "        \"\"\"\n",
    "        Convertit une liste d'indices en texte.\n",
    "        \"\"\"\n",
    "        chars = []\n",
    "        for idx in ids:\n",
    "            token = self.idx_to_char.get(idx, \"\")\n",
    "            \n",
    "            if skip_special_tokens and token in [self.pad_token, self.bos_token, self.eos_token]:\n",
    "                continue\n",
    "            \n",
    "            chars.append(token)\n",
    "        \n",
    "        return \"\".join(chars)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le tokenizer\n",
    "tokenizer = CharTokenizer(pokemon_names)\n",
    "\n",
    "print(f\"Taille du vocabulaire: {tokenizer.vocab_size}\")\n",
    "print(f\"\\nVocabulaire: {tokenizer.vocab}\")\n",
    "\n",
    "# Test\n",
    "test_name = \"Pikachu\"\n",
    "encoded = tokenizer.encode(test_name)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\nTest :\")\n",
    "print(f\"  Original: {test_name}\")\n",
    "print(f\"  Encodé:   {encoded}\")\n",
    "print(f\"  Décodé:   {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Exercice 2 : Dataset PyTorch\n",
    "\n",
    "Pour entraîner le modèle, on crée des paires **(input, target)**.\n",
    "\n",
    "### Principe : prédire le caractère suivant\n",
    "\n",
    "Le modèle GPT apprend à prédire le **prochain caractère** à chaque position.\n",
    "Grâce au **masque causal**, à chaque position il ne voit que ce qui précède :\n",
    "\n",
    "| Position | Le modèle voit | Doit prédire |\n",
    "|----------|----------------|--------------|\n",
    "| 0 | `[BOS]` | `P` |\n",
    "| 1 | `[BOS] P` | `i` |\n",
    "| 2 | `[BOS] P i` | `k` |\n",
    "| 3 | `[BOS] P i k` | `a` |\n",
    "| ... | ... | ... |\n",
    "| 7 | `[BOS] P i k a c h u` | `[EOS]` |\n",
    "\n",
    "### Construction des paires Input/Target\n",
    "\n",
    "Pour la séquence `[BOS] P i k a c h u [EOS]` :\n",
    "- **Input** : `[BOS] P i k a c h u` (tout sauf le dernier)\n",
    "- **Target** : `P i k a c h u [EOS]` (tout sauf le premier)\n",
    "\n",
    "À chaque position `i`, le modèle prédit `target[i]` en ne voyant que `input[0:i+1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PokemonNameDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset pour l'entraînement du générateur de noms.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, names, tokenizer, max_len=32):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Encoder tous les noms\n",
    "        self.data = []\n",
    "        for name in names:\n",
    "            ids = tokenizer.encode(name)\n",
    "            if len(ids) <= max_len:\n",
    "                self.data.append(ids)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ids = self.data[idx]\n",
    "        \n",
    "        # Padding\n",
    "        padding = [self.tokenizer.pad_id] * (self.max_len - len(ids))\n",
    "        ids_padded = ids + padding\n",
    "        \n",
    "        # TODO: Créer input (x) et target (y)\n",
    "        # - x = tout sauf le DERNIER token (ids_padded[:-1])\n",
    "        # - y = tout sauf le PREMIER token (ids_padded[1:])\n",
    "        # Convertir en tensors avec torch.tensor(..., dtype=torch.long)\n",
    "        \n",
    "        x = ...\n",
    "        y = ...\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le dataset\n",
    "dataset_pokemon = PokemonNameDataset(pokemon_names, tokenizer, max_len=32)\n",
    "\n",
    "print(f\"Nombre d'exemples: {len(dataset_pokemon)}\")\n",
    "\n",
    "# Voir un exemple\n",
    "x, y = dataset_pokemon[0]\n",
    "print(f\"\\nExemple :\")\n",
    "print(f\"  Input (x):  {x[:10].tolist()}...\")\n",
    "print(f\"  Target (y): {y[:10].tolist()}...\")\n",
    "print(f\"\\n  Décodé x: '{tokenizer.decode(x.tolist())}'\")\n",
    "print(f\"  Décodé y: '{tokenizer.decode(y.tolist())}'\")\n",
    "\n",
    "if x is not None and y is not None and x.shape == y.shape:\n",
    "    print(\"\\n✅ Correct !\")\n",
    "else:\n",
    "    print(\"\\n❌ Vérifiez votre implémentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Architecture Mini-GPT\n",
    "\n",
    "Notre Mini-GPT est un **décodeur** (comme GPT) avec :\n",
    "- Token embeddings\n",
    "- Position embeddings (apprenables)\n",
    "- N blocs Transformer avec **masque causal**\n",
    "- Tête de sortie (prédiction du prochain caractère)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Mini-GPT pour la génération de texte character-level.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=64, num_heads=4, num_layers=4, \n",
    "                 ff_dim=256, max_seq_len=32, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Blocs Transformer\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Sortie\n",
    "        self.ln_final = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "        # Masque causal pré-calculé : chaque position ne voit que les précédentes\n",
    "        # C'est ce qui distingue GPT (décodeur) de BERT (encodeur bidirectionnel)\n",
    "        # Cf. Session 4 pour l'explication détaillée du masque causal\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\",\n",
    "            torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool()\n",
    "        )\n",
    "        \n",
    "        # Compter les paramètres\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"Mini-GPT: {n_params:,} paramètres ({n_params/1e3:.1f}K)\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Token indices, shape (batch, seq_len)\n",
    "        Returns:\n",
    "            logits: shape (batch, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        positions = torch.arange(seq_len, device=x.device)\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(positions)\n",
    "        x = self.dropout(tok_emb + pos_emb)\n",
    "        \n",
    "        # Masque causal (tronqué à la longueur actuelle)\n",
    "        mask = self.causal_mask[:seq_len, :seq_len]\n",
    "        \n",
    "        # Blocs Transformer\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        # Sortie\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, prompt_ids, max_new_tokens=20, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Génère du texte à partir d'un prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt_ids: Liste d'indices du prompt\n",
    "            max_new_tokens: Nombre max de tokens à générer\n",
    "            temperature: Contrôle la \"créativité\" (1.0 = normal, <1 = conservateur, >1 = créatif)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        # Convertir en tensor\n",
    "        x = torch.tensor([prompt_ids], dtype=torch.long, device=next(self.parameters()).device)\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Tronquer si nécessaire\n",
    "            x_cond = x if x.size(1) <= self.max_seq_len else x[:, -self.max_seq_len:]\n",
    "            \n",
    "            # Forward\n",
    "            logits = self(x_cond)\n",
    "            logits = logits[:, -1, :] / temperature  # Dernier token\n",
    "            \n",
    "            # Sampling\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Ajouter le nouveau token\n",
    "            x = torch.cat([x, next_token], dim=1)\n",
    "            \n",
    "            # Stop si EOS\n",
    "            if next_token.item() == tokenizer.eos_id:\n",
    "                break\n",
    "        \n",
    "        return x[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le modèle\n",
    "model = MiniGPT(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_dim=64,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    "    ff_dim=256,\n",
    "    max_seq_len=32,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Test forward\n",
    "x_test = torch.randint(0, tokenizer.vocab_size, (2, 10)).to(device)\n",
    "logits = model(x_test)\n",
    "print(f\"\\nTest forward:\")\n",
    "print(f\"  Input:  {x_test.shape}\")\n",
    "print(f\"  Output: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Entraînement\n",
    "\n",
    "On entraîne le modèle à prédire le **caractère suivant** (cross-entropy loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset_pokemon, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Loss (ignore le padding)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Batches par epoch: {len(dataloader)}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'entraînement\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def generate_sample(model, tokenizer, prompt=\"\", temperature=0.8):\n",
    "    \"\"\"Génère un nom à partir d'un prompt.\"\"\"\n",
    "    if prompt:\n",
    "        prompt_ids = [tokenizer.bos_id] + [tokenizer.char_to_idx.get(c, tokenizer.pad_id) for c in prompt]\n",
    "    else:\n",
    "        prompt_ids = [tokenizer.bos_id]\n",
    "    \n",
    "    output_ids = model.generate(prompt_ids, max_new_tokens=20, temperature=temperature)\n",
    "    return tokenizer.decode(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement\n",
    "print(\"Début de l'entraînement...\\n\")\n",
    "\n",
    "losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss = train_epoch(model, dataloader, optimizer, criterion, device)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Afficher la progression\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{NUM_EPOCHS} | Loss: {loss:.4f}\")\n",
    "        \n",
    "        # Générer un exemple\n",
    "        sample = generate_sample(model, tokenizer, prompt=\"\")\n",
    "        print(f\"  Exemple: {sample}\")\n",
    "\n",
    "print(\"\\n✅ Entraînement terminé !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbe de loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Courbe d'entraînement Mini-GPT\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Génération de noms !\n",
    "\n",
    "Testons notre modèle avec différents prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer des noms avec différents prompts\n",
    "prompts = [\"Pika\", \"Dra\", \"Flam\", \"Aqua\", \"Méga\", \"\"]\n",
    "\n",
    "print(\"Noms de Pokémon générés :\\n\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for prompt in prompts:\n",
    "    names = [generate_sample(model, tokenizer, prompt, temperature=0.8) for _ in range(3)]\n",
    "    \n",
    "    if prompt:\n",
    "        print(f\"Prompt '{prompt}' :\")\n",
    "    else:\n",
    "        print(f\"Sans prompt :\")\n",
    "    \n",
    "    for name in names:\n",
    "        print(f\"  → {name}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effet de la température\n",
    "print(\"Effet de la température :\\n\")\n",
    "\n",
    "for temp in [0.5, 0.8, 1.0, 1.5]:\n",
    "    print(f\"Temperature = {temp} :\")\n",
    "    names = [generate_sample(model, tokenizer, \"Pika\", temperature=temp) for _ in range(5)]\n",
    "    print(f\"  {', '.join(names)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Récapitulatif\n",
    "\n",
    "### Ce que nous avons fait\n",
    "\n",
    "1. ✅ Assemblé un **bloc Transformer** complet\n",
    "2. ✅ Créé un **tokenizer character-level**\n",
    "3. ✅ Implémenté **Mini-GPT** (décodeur avec masque causal)\n",
    "4. ✅ Entraîné sur **1211 noms de Pokémon**\n",
    "5. ✅ Généré des **noms inventés** !\n",
    "\n",
    "### Prochaine session\n",
    "\n",
    "On va :\n",
    "1. **Fine-tuner GPT-2** français sur le corpus Pokémon complet\n",
    "2. Générer des **descriptions** de Pokémon\n",
    "3. Créer un **pipeline complet** : nom + description = nouveau Pokémon !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle (optionnel)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab': tokenizer.vocab,\n",
    "}, 'minigpt_pokemon_names.pt')\n",
    "\n",
    "print(\"✅ Modèle sauvegardé: minigpt_pokemon_names.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
