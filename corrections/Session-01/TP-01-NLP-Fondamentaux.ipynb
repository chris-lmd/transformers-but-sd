{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# TP 01 - Fondamentaux NLP pour les Transformers\n\n**Module** : R√©seaux de Neurones Approfondissement  \n**Dur√©e** : 2h  \n**Objectif** : Mieux comprendre comment une machine peut interpr√©ter du texte\n\n---\n\n## Comment faire interpr√©ter du texte par une machine ?\n\nSi vous avez d√©j√† travaill√© avec des **images** le probl√®me est de prime abord plus simple : une image peut naturellement se d√©couper en une grille de pixels, chaque pixel est un nombre (0-255). Le r√©seau peut directement les traiter.\n\nMais pour le **texte** suivant ?\n\n```\n\"L'apprentissage automatique r√©volutionne l'intelligence artificielle\"\n```\n\nCe n'est qu'une suite de caract√®res. Un r√©seau de neurones, par exemple, ne comprend que des **nombres**. Comment passer de l'un √† l'autre ?\n\n---\n\n## Les probl√®mes √† r√©soudre\n\nPour transformer du texte en repr√©sentation num√©rique exploitable, il faut r√©soudre **deux probl√®mes distincts** :\n\n### Probl√®me 1 : La tokenization\n\n**Comment d√©couper le texte suivant en morceaux ?**\n\n```\n\"L'apprentissage automatique\" ‚Üí ???\n```\n\nPlusieurs strat√©gies sont possibles :\n- Par mots : `[\"L'apprentissage\", \"automatique\"]`\n- Par caract√®res : `[\"L\", \"'\", \"a\", \"p\", \"p\", \"r\", ...]`\n- Par sous-mots : `[\"L'\", \"apprent\", \"issage\", \"auto\", \"matique\"]`\n\nChaque strat√©gie a ses avantages et inconv√©nients. Nous les explorerons dans ce TP.\n\n### Probl√®me 2 : L'embedding\n\n**Comment transformer ces morceaux en vecteurs qui ont du SENS ?**\n\nUne fois le texte d√©coup√©, on pourrait simplement num√©roter les tokens :\n```\n\"chat\" ‚Üí 42\n\"voiture\" ‚Üí 46\n\"chien\" ‚Üí 73\n```\n\nMais ces nombres sont **arbitraires**. Ils ne capturent pas que \"chat\" et \"chien\" sont des concepts proches (animaux domestiques), alors que \"voiture\" est compl√®tement diff√©rent.\n\n**Il faut trouver un moyen** de transformer chaque token en un **vecteur de plusieurs dimensions** o√π la **proximit√© g√©om√©trique** refl√®te la **proximit√© s√©mantique** :\n\n```\n\"chat\"    ‚Üí [0.2, -0.5, 0.8, ...]   ‚îê\n                                    ‚îú‚îÄ vecteurs proches !\n\"chien\"   ‚Üí [0.3, -0.4, 0.7, ...]   ‚îò\n\n\"voiture\" ‚Üí [-0.8, 0.2, -0.3, ...]  ‚Üê vecteur √©loign√©\n```\n\nPlusieurs approches existent pour construire ces vecteurs. Dans ce TP, nous explorerons **Word2Vec**, une m√©thode remarquable qui a r√©volutionn√© le NLP en 2013.\n\n---\n\n## Plan du TP\n\n| Section | Th√®me | Ce que vous apprendrez |\n|---------|-------|------------------------|\n| ¬ß2 | Tokenization | Les 3 strat√©gies (mots, caract√®res, BPE) |\n| ¬ß3 | Embeddings | Comment les vecteurs capturent le sens (Word2Vec) |\n| ¬ß4 | R√©capitulatif | Synth√®se des concepts |\n| ¬ß5 | Ressources | Pour aller plus loin |\n| ¬ß6 | Attention (teaser) | Aper√ßu du m√©canisme cl√© des Transformers |\n| ¬ß7 | Mini-projet | Pipeline NLP complet (optionnel) |\n\nCommen√ßons par le premier probl√®me : **comment d√©couper le texte ?**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports\n",
    "\n",
    "Ex√©cutez cette cellule pour installer les d√©pendances n√©cessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Installation des d√©pendances (Google Colab)\n!pip install torch matplotlib numpy gensim transformers scikit-learn datasets tokenizers -q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Tokenization : D√©couper le texte\n",
    "\n",
    "La **tokenization** consiste √† d√©couper le texte en unit√©s (tokens). Il existe plusieurs strat√©gies.\n",
    "\n",
    "### 2.1 Tokenization par mots (Word-level)\n",
    "\n",
    "La plus intuitive : on d√©coupe sur les espaces et la ponctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercice 0a : Impl√©menter la tokenization par mots\n",
    "\n",
    "Compl√©tez la fonction `tokenize_words` ci-dessous. Elle doit :\n",
    "- S√©parer le texte sur les espaces\n",
    "- Garder la ponctuation comme tokens s√©par√©s\n",
    "\n",
    "**Indice** : Utilisez `re.findall()` pour capturer les mots OU la ponctuation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tokenization simple par mots\ndef tokenize_words(text):\n    \"\"\"Tokenization basique par espaces et ponctuation.\"\"\"\n    import re\n    # S√©pare sur espaces et garde la ponctuation comme tokens\n    # CORRECTION: Capture les mots (\\w+) OU la ponctuation ([^\\w\\s])\n    return re.findall(r'\\w+|[^\\w\\s]', text)\n\ntexte = \"Le chat mange la souris. La souris court vite !\"\ntokens = tokenize_words(texte)\n\nprint(f\"Texte : {texte}\")\nprint(f\"Tokens : {tokens}\")\nprint(f\"Nombre de tokens : {len(tokens)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Probl√®me** : Le vocabulaire peut devenir √©norme !\n\nAjoutez les noms propres, les n√©ologismes, les mots √©trangers, les fautes de frappe... Le vocabulaire explose.\n\n√Ä un moment, il faut **fixer une taille de vocabulaire** (ex: 50 000 mots). Mais alors, que faire des mots inconnus ?\n\nImaginons un vocabulaire constitu√© au pr√©alable :\n```\n[\"le\", \"chat\", \"mange\", ...] (50 000 mots)\n```\nSi un mot trait√© n'appartient pas au vocabulaire, l'information est perdue\n```\nNouveau mot : \"transformers\" ‚Üí <UNK> ?\n```\n‚Üí Pas id√©al."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization par caract√®res (Character-level)\n",
    "\n",
    "Une solution : d√©couper caract√®re par caract√®re. Plus de mots inconnus !"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercice 0b : Impl√©menter la tokenization par caract√®res\n",
    "\n",
    "Compl√©tez la fonction `tokenize_chars` ci-dessous.\n",
    "\n",
    "**Indice** : En Python, une cha√Æne est d√©j√† it√©rable caract√®re par caract√®re..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def tokenize_chars(text):\n    \"\"\"Tokenization par caract√®res.\"\"\"\n    # CORRECTION: Une cha√Æne Python est it√©rable caract√®re par caract√®re\n    return list(text)\n\ntexte = \"Le chat dort.\"\ntokens = tokenize_chars(texte)\n\nprint(f\"Texte : {texte}\")\nprint(f\"Tokens : {tokens}\")\nprint(f\"Nombre de tokens : {len(tokens)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Probl√®me historique** : S√©quences tr√®s longues ! \"anticonstitutionnellement\" = 25 tokens.\n\nLe mod√®le doit \"r√©apprendre\" que `c-h-a-t` forme le concept de chat. Le co√ªt computationnel √©tait longtemps consid√©r√© comme prohibitif.\n\n---\n\n> üìö **Des travaux r√©cents montrent que la tokenization byte-level peut devenir comp√©titive**\n>\n> \n> **Bolmo** (Allen AI, 2025) op√®re directement sur les **bytes UTF-8** (256 tokens possibles) avec une architecture adapt√©e :\n> - Un encodeur local (mLSTM) traite les bytes\n> - Un \"boundary predictor\" regroupe les bytes en patches de taille variable\n> - Le Transformer traite ces patches (pas les bytes bruts)\n> \n> **Avantages** : pas de vocabulaire fig√©, robuste aux typos, meilleure compr√©hension caract√®re.\n\nPour ce TP, nous utiliserons **BPE** que nous allons voir tout de suite et qui reste le standard actuel, mais gardez en t√™te que le domaine √©volue rapidement !"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tokenization Subword : BPE (Byte Pair Encoding)\n",
    "\n",
    "Les deux approches pr√©c√©dentes ont des d√©fauts :\n",
    "- **Mots** : vocabulaire √©norme + mots inconnus\n",
    "- **Caract√®res** : s√©quences trop longues + perte de sens\n",
    "\n",
    "**BPE** (Byte Pair Encoding) est un **compromis intelligent** utilis√© par GPT, BERT, et une grande partie des LLMs modernes.\n",
    "\n",
    "---\n",
    "\n",
    "#### Le principe\n",
    "\n",
    "BPE construit son vocabulaire en analysant un grand corpus de texte :\n",
    "\n",
    "1. **D√©part** : vocabulaire = tous les caract√®res\n",
    "2. **R√©p√©ter** : trouver la paire de tokens adjacents la plus fr√©quente ‚Üí la fusionner en un nouveau token\n",
    "3. **Stop** : quand le vocabulaire atteint la taille voulue (ex: 50 000 tokens)\n",
    "\n",
    "---\n",
    "\n",
    "#### L'algorithme pas √† pas\n",
    "\n",
    "Pour comprendre, prenons un **corpus artificiel simplifi√©** :\n",
    "```\n",
    "\"smartphone smartphone smartphone smartwatch smartwatch phone phone\"\n",
    "```\n",
    "\n",
    "**√âtape 0 : Partir des caract√®res**\n",
    "```\n",
    "Vocabulaire : {s, m, a, r, t, p, h, o, n, e, w, c}\n",
    "```\n",
    "\n",
    "**√âtapes suivantes : Fusionner les paires les plus fr√©quentes**\n",
    "\n",
    "| √âtape | Paire la + fr√©quente | Nouveau token |\n",
    "|-------|---------------------|---------------|\n",
    "| 1 | (s, m) | \"sm\" |\n",
    "| 2 | (sm, a) | \"sma\" |\n",
    "| 3 | (sma, r) | \"smar\" |\n",
    "| 4 | (smar, t) | \"smart\" |\n",
    "| 5 | (p, h) | \"ph\" |\n",
    "| 6 | (ph, o) | \"pho\" |\n",
    "| 7 | (pho, n) | \"phon\" |\n",
    "| 8 | (phon, e) | \"phone\" |\n",
    "\n",
    "**R√©sultat :**\n",
    "```\n",
    "\"smartphone\" ‚Üí [\"smart\", \"phone\"]  ‚Üê 2 tokens r√©utilisables !\n",
    "\"smartwatch\" ‚Üí [\"smart\", \"watch\"]\n",
    "\"phone\"      ‚Üí [\"phone\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Pourquoi c'est malin ?\n",
    "\n",
    "Un mot **jamais vu** comme \"smartcar\" sera d√©coup√© en :\n",
    "```\n",
    "\"smartcar\" ‚Üí [\"smart\", \"car\"]\n",
    "```\n",
    "\n",
    "Le mod√®le conna√Æt d√©j√† \"smart\" ! Pas besoin de token `<UNK>`.\n",
    "\n",
    "**Bonus** : les sous-mots fr√©quents ont de **bons embeddings** (on verra pourquoi dans la section Word2Vec). Donc m√™me un mot rare peut b√©n√©ficier de repr√©sentations de qualit√© via ses composants.\n",
    "\n",
    "**Nous obtenons le meilleur des deux mondes** :\n",
    "- Mots fr√©quents ‚Üí tokens entiers (efficace)\n",
    "- Mots rares/nouveaux ‚Üí sous-mots connus (robuste)\n",
    "\n",
    "Bien que ce mod√®le soit tr√®s performant dans un grand nombre de cas, il faut rester conscient que certains mots tr√®s sp√©cifiques peuvent avoir une repr√©sentation de moindre qualit√©.\n",
    "\n",
    "**Ressource** : [Explication d√©taill√©e des tokenizers (FR)](https://lbourdois.github.io/blog/nlp/Les-tokenizers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n#### En pratique : GPT-2 vs CamemBERT\n\nLes tokenizers BPE sont entra√Æn√©s sur un corpus sp√©cifique. **GPT-2** a √©t√© entra√Æn√© principalement sur du texte anglais, tandis que **CamemBERT** est un mod√®le fran√ßais.\n\nCons√©quence : un m√™me texte sera d√©coup√© diff√©remment selon le tokenizer utilis√© !",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Chargement des tokenizers\nfrom transformers import GPT2Tokenizer, CamembertTokenizer\n\n# GPT-2 : tokenizer anglais\ngpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# CamemBERT : tokenizer fran√ßais  \ncamembert_tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n\nprint(\"Tokenizers charg√©s !\")\nprint(f\"  GPT-2 : {gpt2_tokenizer.vocab_size} tokens\")\nprint(f\"  CamemBERT : {camembert_tokenizer.vocab_size} tokens\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercice 0c : Comparer GPT-2 vs CamemBERT\n\nTestez les deux tokenizers sur des phrases en fran√ßais et en anglais. Observez les diff√©rences !\n\n**Questions √† explorer** :\n1. Comment le tokenizer influe-t-il sur le nombre de tokens ?\n2. Les tokens produits vous paraissent-ils avoir du sens ?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# EXERCICE 0c : Comparer GPT-2 vs CamemBERT\n# ============================================\n\n# Fonction utilitaire pour comparer\ndef compare_tokenizers(texte):\n    tokens_gpt2 = gpt2_tokenizer.tokenize(texte)\n    tokens_camembert = camembert_tokenizer.tokenize(texte)\n    \n    print(f\"Texte : '{texte}'\")\n    print(f\"  GPT-2     : {len(tokens_gpt2):2d} tokens ‚Üí {tokens_gpt2}\")\n    print(f\"  CamemBERT : {len(tokens_camembert):2d} tokens ‚Üí {tokens_camembert}\")\n    print()\n\n# Exemples fournis\nprint(\"=== Phrases en FRAN√áAIS ===\\n\")\ncompare_tokenizers(\"Le chat mange la souris.\")\ncompare_tokenizers(\"L'intelligence artificielle r√©volutionne le monde.\")\ncompare_tokenizers(\"anticonstitutionnellement\")\n\nprint(\"=== Phrases en ANGLAIS ===\\n\")\ncompare_tokenizers(\"The cat eats the mouse.\")\ncompare_tokenizers(\"Artificial intelligence revolutionizes the world.\")\ncompare_tokenizers(\"internationalization\")\n\n# TODO: Testez vos propres phrases !\n# compare_tokenizers(\"Votre phrase ici\")"
  },
  {
   "cell_type": "markdown",
   "source": "**Observations** :\n- CamemBERT d√©coupe mieux le fran√ßais (moins de tokens)\n- GPT-2 d√©coupe mieux l'anglais\n- Le caract√®re `ƒ†` (GPT-2) ou `‚ñÅ` (CamemBERT) indique un espace avant le token",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.4 Construction d'un vocabulaire\n\nUne fois la strat√©gie de tokenization choisie, on construit un **vocabulaire** : une table de correspondance token ‚Üî index.\n\n### Exercice 1 : Construire un vocabulaire\n\n√Ä partir d'un corpus, vous allez :\n1. Collecter tous les tokens uniques\n2. Cr√©er un dictionnaire `vocab` avec un token sp√©cial `<UNK>` (pour les mots inconnus)\n3. Impl√©menter les fonctions de conversion token ‚Üî ID\n4. Tester sur une phrase\n\n> üí° **Note** : En pratique, on ajoute souvent d'autres tokens sp√©ciaux comme `<PAD>` (pour aligner les s√©quences de longueurs diff√©rentes lors du batching). On les verra dans les prochaines sessions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# EXERCICE 1 : Construire un vocabulaire\n# ============================================\n\ncorpus = [\n    \"le chat mange\",\n    \"le chien dort\",\n    \"la souris court\"\n]\n\n# CORRECTION 1: Collecter tous les tokens uniques du corpus\n# Utilise un set() pour les tokens uniques et tokenize_words()\nall_tokens = set()\nfor phrase in corpus:\n    tokens = tokenize_words(phrase)\n    all_tokens.update(tokens)\n\n\n# CORRECTION 2: Cr√©er le vocabulaire avec le token sp√©cial <UNK>\n# Le vocabulaire commence par {\"<UNK>\": 0}\n# Puis ajoute chaque token avec un index unique\nvocab = {\"<UNK>\": 0}\nfor i, token in enumerate(sorted(all_tokens), start=1):\n    vocab[token] = i\n\n\n# CORRECTION 3: Cr√©er le vocabulaire inverse (id ‚Üí token)\nid_to_token = {idx: token for token, idx in vocab.items()}\n\n\n# CORRECTION 4: Impl√©menter la fonction tokens_to_ids\n# Retourne <UNK> (index 0) pour les mots inconnus\ndef tokens_to_ids(text, vocab):\n    \"\"\"Convertit un texte en liste d'indices.\"\"\"\n    tokens = tokenize_words(text)\n    return [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n\n\n# CORRECTION 5: Impl√©menter la fonction ids_to_tokens\ndef ids_to_tokens(ids, id_to_token):\n    \"\"\"Convertit une liste d'indices en tokens.\"\"\"\n    return [id_to_token[idx] for idx in ids]\n\n\n# === Tests ===\nprint(\"Vocabulaire :\", vocab)\nprint()\n\n# Test encodage\nphrase = \"le chat court\"\nids = tokens_to_ids(phrase, vocab)\nprint(f\"'{phrase}' ‚Üí {ids}\")\n\n# Test d√©codage\ntokens_back = ids_to_tokens(ids, id_to_token) if id_to_token else []\nprint(f\"{ids} ‚Üí {tokens_back}\")\n\n# Test avec mot inconnu\nphrase_inconnue = \"le hamster mange\"\nids_inconnu = tokens_to_ids(phrase_inconnue, vocab)\nphrase_inconnue_post_token = ids_to_tokens(ids_inconnu,id_to_token)\nprint(f\"\\n'{phrase_inconnue}' ‚Üí {ids_inconnu} ‚Üí {phrase_inconnue_post_token}\")\nprint(\"(hamster devrait √™tre remplac√© par l'index de <UNK>)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. Embeddings\n\n### 3.1 Le probl√®me des indices\n\nLes indices (0, 1, 2, ...) n'ont pas de **sens s√©mantique**. \n\n- `chat = 3` et `chien = 5` ‚Üí sont-ils proches ? (oui, ce sont des animaux et ils ont beaucoup de choses en commun)\n- `chat = 3` et `voiture = 4` ‚Üí sont-ils proches ? (non, beaucoup moins que chien et chat)\n\nCes indices ne permettent pas de mesurer cette proximit√© !\n\n### Approches classiques pour encoder des phrases : Bag-of-Words et TF-IDF\n\nHistoriquement, on repr√©sentait un texte par un vecteur de la taille du vocabulaire :\n- **Bag-of-Words** : compter les occurrences de chaque mot\n- **TF-IDF** : pond√©rer par la raret√© des mots dans le corpus\n\n```\nExemple simplifi√© (BOW vs TF-IDF) :\n\nCorpus : [\"le chat mange\", \"le chat dort\"]\nVocabulaire : [le, chat, mange, dort]\n\nBOW (comptage brut) :\n  \"le chat mange\" ‚Üí [1, 1, 1, 0]\n  \"le chat dort\"  ‚Üí [1, 1, 0, 1]\n\nTF-IDF (pond√©r√© par raret√©) :\n  \"le chat mange\" ‚Üí [0.3, 0.3, 0.7, 0]    ‚Üê \"mange\" p√®se plus (mot distinctif)\n  \"le chat dort\"  ‚Üí [0.3, 0.3, 0, 0.7]    ‚Üê \"dort\" p√®se plus (mot distinctif)\n                     ‚Üë    ‚Üë\n              mots communs ‚Üí poids r√©duit\n```\n\nEn pratique, le vocabulaire contient 50 000+ mots ‚Üí vecteurs **sparse** (majoritairement des z√©ros) :\n\n```\n\"le chat dort\" ‚Üí [0, 0, ..., 1, ..., 0, 1, ..., 0]  (50 000 dimensions)\n                              ‚Üë         ‚Üë\n                            chat      dort\n```\n\nPour TF-IDF et BOW, on obtient des vecteurs qui permettent de comparer des morceaux de texte entre eux (descriptions de produits sur un site marchand par exemple). Pour autant, ils ne permettent pas de capturer le sens des mots et leurs proximit√©s relatives. Une m√©thode apparue en 2013 (Word2Vec) a permis d'apporter cette compr√©hension plus profonde, de mani√®re automatis√©e et sans supervision."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---\n\n### 3.2 Word2Vec : Apprendre des embeddings qui ont du sens\n\nComme son nom l'indique, Word2Vec transforme des mots en vecteurs. Mais comment apprend-il des vecteurs o√π \"chat\" et \"chien\" sont vraiment proches ?\n\n---\n\n#### L'intuition fondamentale\n\n> **\"Tu connais un mot par les mots qui l'entourent\"** (hypoth√®se distributionnelle)\n\nObservez ces phrases :\n```\n\"Le chat mange sa p√¢t√©e\"\n\"Le chat dort sur le canap√©\"  \n\"Mon chat joue avec une balle\"\n\n\"Le chien mange sa p√¢t√©e\"\n\"Le chien dort sur le canap√©\"\n\"Mon chien joue avec une balle\"\n```\n\n\"Chat\" et \"chien\" apparaissent dans les **m√™mes contextes**. Word2Vec va leur attribuer des vecteurs similaires.\n\n---\n\n#### L'architecture de Word2Vec\n\nWord2Vec repose sur une architecture similaire √† un r√©seau de neurones **√©tonnamment simple** : une entr√©e, une couche cach√©e, une sortie. Pas d'activation (simple multiplication matricielle).\n\n##### Le r√©seau\n\n```\n      ENTR√âE                 COUCHE CACH√âE              SORTIE\n     (one-hot)               (embeddings)              (softmax)\n  \n   ‚îå‚îÄ‚îÄ‚îÄ‚îê                                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n   ‚îÇ 0 ‚îÇ  \"le\"                                        ‚îÇ 0.02  ‚îÇ \"le\"\n   ‚îú‚îÄ‚îÄ‚îÄ‚î§                                              ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n   ‚îÇ 0 ‚îÇ  \"chat\"             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ 0.41  ‚îÇ \"chat\"\n   ‚îú‚îÄ‚îÄ‚îÄ‚î§                     ‚îÇ           ‚îÇ            ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n   ‚îÇ 1 ‚îÇ  \"noir\"  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂  ‚îÇ  vecteur  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ 0.05  ‚îÇ \"noir\"\n   ‚îú‚îÄ‚îÄ‚îÄ‚î§            W        ‚îÇ  128 dim  ‚îÇ     W'     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n   ‚îÇ 0 ‚îÇ  \"dort\"             ‚îÇ           ‚îÇ            ‚îÇ 0.38  ‚îÇ \"dort\"\n   ‚îú‚îÄ‚îÄ‚îÄ‚î§                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n   ‚îÇ 0 ‚îÇ  \"sur\"                                       ‚îÇ 0.14  ‚îÇ \"sur\"\n   ‚îî‚îÄ‚îÄ‚îÄ‚îò                                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n  \n  V dimensions              D dimensions              V dimensions\n  (taille vocab)            (ex: 128)                 (probabilit√©s)\n```\n\n| Couche | Dimensions | Ce qu'elle contient |\n|--------|------------|---------------------|\n| Entr√©e | V (ex: 50 000) | Vecteur one-hot du mot |\n| Cach√©e | D (ex: 128) | **Le vecteur qu'on veut r√©cup√©rer** |\n| Sortie | V (ex: 50 000) | Probabilit√© de chaque mot |\n\n---\n\n##### Le but : r√©cup√©rer la couche cach√©e\n\nL'objectif de Word2Vec n'est **pas** de faire des pr√©dictions. C'est de construire de bons vecteurs.\n\nLa **matrice W** (entre l'entr√©e et la couche cach√©e) contient tous les embeddings :\n\n```\nMatrice W (V √ó D)\n                    D dimensions\n              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n   \"le\"     ‚Üí ‚îÇ 0.2  -0.1  0.5 ... ‚îÇ\n   \"chat\"   ‚Üí ‚îÇ 0.3  -0.4  0.7 ... ‚îÇ ‚Üê Ces deux lignes\n   \"chien\"  ‚Üí ‚îÇ 0.3  -0.3  0.6 ... ‚îÇ ‚Üê sont proches !\n   \"noir\"   ‚Üí ‚îÇ 0.1   0.5 -0.2 ... ‚îÇ\n   ...        ‚îÇ        ...         ‚îÇ\n              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Apr√®s l'entra√Ænement** :\n- On **garde** la matrice W ‚Üí c'est notre table d'embeddings\n- On **jette** tout le reste (matrice W', couche de sortie)\n\nLa sortie n'√©tait qu'un **pr√©texte** pour entra√Æner le r√©seau !\n\n---\n\n##### L'entra√Ænement : deux techniques\n\nPour que les vecteurs capturent le sens des mots, on entra√Æne le r√©seau √† pr√©dire les relations entre mots voisins dans un corpus.\n\n**Deux approches sym√©triques existent :**\n\n###### Skip-gram : mot central ‚Üí mots voisins\n\nOn donne un mot, le r√©seau pr√©dit les mots qui l'entourent.\n\n```\nPhrase : \"Le chat noir dort sur\"\n                  ‚Üë\n             mot central\n\nFen√™tre de contexte (¬±1 mot) :\n    Entr√©e  : \"noir\"\n    Cibles  : \"chat\", \"dort\" (trait√©s un par un)\n```\n\n```\n    \"noir\"                     \"chat\" ?\n       ‚îÇ                          ‚Üë\n       ‚ñº                          ‚îÇ\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    vecteur     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n   ‚îÇ   W   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ   W'    ‚îÇ\n   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   (128 dim)    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n###### CBOW : mots voisins ‚Üí mot central\n\nOn donne les mots du contexte, le r√©seau pr√©dit le mot du milieu.\n\n```\nPhrase : \"Le chat noir dort sur\"\n              ‚Üë         ‚Üë\n            contexte (¬±1)\n\nEntr√©e  : \"chat\" + \"dort\" (moyenn√©s)\nCible   : \"noir\"\n```\n\n```\n\"chat\" + \"dort\"                \"noir\" ?\n       ‚îÇ                          ‚Üë\n       ‚ñº                          ‚îÇ\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    vecteur     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n   ‚îÇ   W   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ   W'    ‚îÇ\n   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   (128 dim)    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n###### Comparaison\n\n| | Skip-gram | CBOW |\n|--|-----------|------|\n| Entr√©e | 1 mot | plusieurs mots |\n| Sortie | pr√©dire les voisins | pr√©dire le mot central |\n| Mots rares | ‚úÖ meilleur | moins bon |\n| Vitesse | plus lent | ‚úÖ plus rapide |\n\nEn pratique, **Skip-gram** est plus utilis√© car il donne de meilleurs r√©sultats sur les mots peu fr√©quents.\n\n---\n\n##### Cons√©quence : mots fr√©quents vs mots rares\n\nPlus un mot est **fr√©quent** dans le corpus, plus son embedding est ajust√© souvent ‚Üí meilleure qualit√©.\n\nUn mot vu 100 000 fois aura un excellent embedding. Un mot vu 3 fois restera proche de son initialisation al√©atoire.\n\n> C'est pour √ßa que BPE aide : un mot rare comme \"smartcar\" est d√©coup√© en \"smart\" + \"car\", deux sous-mots tr√®s fr√©quents avec d'excellents embeddings !\n\n---\n\n##### Limitation : l'ordre des mots est ignor√©\n\nWord2Vec traite chaque paire (mot, voisin) **ind√©pendamment**. Il ne sait pas quel mot vient avant ou apr√®s.\n\n```\n\"Le chat mange la souris\"\n\"La souris mange le chat\"\n\n‚Üí M√™mes paires d'entra√Ænement !\n‚Üí Word2Vec ne voit pas la diff√©rence\n```\n\nC'est le **m√©canisme d'attention combin√© au positional encoding** (section 6) qui permettra de capturer l'ordre et les relations entre positions.\n\n---\n\n> üí° **En pratique** : Les impl√©mentations r√©elles (gensim, FastText...) ajoutent des optimisations pour acc√©l√©rer l'entra√Ænement sur de gros vocabulaires. L'architecture de base reste la m√™me.\n\n> üìö **Pour aller plus loin** : [The Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/) ‚Äî Visualisations d√©taill√©es de l'architecture et de l'entra√Ænement.\n\n---\n\n#### Pourquoi les analogies marchent ?\n\nApr√®s entra√Ænement, les vecteurs encodent des **relations** :\n\n```\nvecteur(\"roi\") - vecteur(\"homme\") ‚âà vecteur(\"reine\") - vecteur(\"femme\")\n```\n\nAutrement dit, la \"direction\" homme‚Üífemme dans l'espace vectoriel est la m√™me que roi‚Üíreine :\n\n```\n        homme ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí femme\n          ‚Üë    (m√™me         ‚Üë\n          ‚îÇ   direction)     ‚îÇ\n         roi ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí reine\n```\n\nC'est pour √ßa que `roi - homme + femme ‚âà reine` fonctionne !\n\n---\n\n#### Mesurer la similarit√© : Similarit√© cosinus\n\nUn vecteur poss√®de **deux caract√©ristiques** :\n- **Une direction** (o√π il pointe)\n- **Une norme** (sa longueur)\n\nA priori, pour comparer deux vecteurs, on devrait s'int√©resser aux deux.\n\n##### Le cas de Word2Vec\n\nEn pratique, la norme des embeddings Word2Vec est **pollu√©e** par des effets qui n'ont rien √† voir avec le sens : fr√©quence des mots dans le corpus, d√©tails de l'entra√Ænement...\n\nOn utilise donc la **similarit√© cosinus**, qui compare uniquement les directions. Et √ßa fonctionne tr√®s bien !\n\n$$\\text{similarit√©}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n\n| Valeur | Interpr√©tation |\n|--------|----------------|\n| 1 | M√™me direction |\n| 0 | Aucune relation |\n| -1 | Directions oppos√©es |\n\n##### Les mod√®les r√©cents\n\nLes mod√®les modernes (OpenAI, Sentence-Transformers...) int√®grent la normalisation **directement dans l'entra√Ænement** et retournent des embeddings d√©j√† normalis√©s. R√©sultat : cosine similarity = dot product, plus d'ambigu√Øt√©.\n\n> üìö **Pour aller plus loin** :\n> - [Pinecone - Vector Similarity Explained](https://www.pinecone.io/learn/vector-similarity/)\n> - [ArXiv - Is Cosine-Similarity Really About Similarity?](https://arxiv.org/abs/2403.05440) ‚Äî Limites th√©oriques"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---\n\n### 3.3 Exploration avec GloVe\n\nNous allons utiliser **GloVe** (similaire √† Word2Vec), entra√Æn√© sur Wikipedia."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Charger un mod√®le pr√©-entra√Æn√© (GloVe, similaire √† Word2Vec)\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(\"Chargement du mod√®le GloVe (peut prendre 1-2 min)...\")\n",
    "model = api.load(\"glove-wiki-gigaword-100\")  # 100 dimensions, entra√Æn√© sur Wikipedia\n",
    "print(f\"Mod√®le charg√© ! Vocabulaire : {len(model)} mots\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Explorer les similarit√©s\n",
    "print(\"=== Mots similaires √† 'king' ===\")\n",
    "for word, score in model.most_similar(\"king\", topn=5):\n",
    "    print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n=== Mots similaires √† 'computer' ===\")\n",
    "for word, score in model.most_similar(\"computer\", topn=5):\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# EXERCICE 2 : Explorer les similarit√©s\n# ============================================\n\n# Partie A : Trouver les mots similaires √† \"france\", \"cat\", \"happy\"\n# TODO: Utilisez model.most_similar(mot, topn=5)\n\n\n# Partie B : Exploration libre\n# TODO: Trouvez une paire de mots avec similarit√© > 0.7\n# Indice : model.similarity(\"mot1\", \"mot2\") retourne un score entre -1 et 1\n\n\n# Partie C : Trouver l'intrus\n# TODO: Utilisez model.doesnt_match([\"mot1\", \"mot2\", \"mot3\", \"mot4\"])\n# Exemple : model.doesnt_match([\"breakfast\", \"lunch\", \"dinner\", \"car\"])\n# Testez avec vos propres listes !",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# La magie des analogies : king - man + woman = ?\nprint(\"=== Analogie : king - man + woman = ? ===\")\nresult = model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=3)\nfor word, score in result:\n    print(f\"  {word}: {score:.4f}\")\n\nprint(\"\\n=== Analogie : paris - france + italy = ? ===\")\nresult = model.most_similar(positive=[\"paris\", \"italy\"], negative=[\"france\"], topn=3)\nfor word, score in result:\n    print(f\"  {word}: {score:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# EXERCICE 3 : Ma√Ætriser les analogies\n# ============================================\n\n# Partie A : Tester ces analogies classiques\n# - \"berlin\" - \"germany\" + \"france\" = ?\n# - \"good\" - \"better\" + \"bad\" = ?\n# - \"walked\" - \"walk\" + \"swim\" = ?\n\n# TODO: Syntaxe : model.most_similar(positive=[\"A\", \"C\"], negative=[\"B\"], topn=3)\n\n\n# Partie B : Inventer une analogie qui fonctionne\n# TODO: Trouvez une analogie originale qui donne le r√©sultat attendu\n# Exemples de domaines : m√©tiers, pays/capitales, animaux, verbes...\n\n\n# Partie C : Trouver une analogie qui √©choue\n# TODO: Trouvez une analogie qui devrait marcher logiquement mais √©choue\n# Expliquez pourquoi dans un commentaire (indice : fr√©quence des mots, biais du corpus...)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualisation des embeddings en 2D\nfrom sklearn.decomposition import PCA\n\n# S√©lectionner quelques mots\nwords = [\"king\", \"queen\", \"man\", \"woman\", \"prince\", \"princess\",\n         \"cat\", \"dog\", \"lion\", \"tiger\",\n         \"car\", \"bus\", \"train\", \"plane\"]\n\n# R√©cup√©rer leurs vecteurs\nvectors = np.array([model[w] for w in words])\n\n# R√©duire √† 2D avec PCA\npca = PCA(n_components=2)\nvectors_2d = pca.fit_transform(vectors)\n\n# Visualiser\nplt.figure(figsize=(12, 8))\nplt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], c='blue', s=100)\n\nfor i, word in enumerate(words):\n    plt.annotate(word, (vectors_2d[i, 0] + 0.1, vectors_2d[i, 1] + 0.1), fontsize=12)\n\nplt.title(\"Embeddings GloVe projet√©s en 2D\")\nplt.xlabel(\"Composante 1\")\nplt.ylabel(\"Composante 2\")\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"Observation : Les mots de m√™me cat√©gorie sont regroup√©s !\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# EXERCICE 4 : Visualisation personnalis√©e\n# ============================================\n\n# TODO: Cr√©ez votre propre visualisation avec 15-20 mots de votre choix\n# Choisissez des mots de 3-4 cat√©gories diff√©rentes (ex: sports, √©motions, pays, m√©tiers)\n\n# Vos mots (modifiez cette liste) :\nmy_words = [\n    # Cat√©gorie 1 (ex: sports) : \n    \n    # Cat√©gorie 2 (ex: √©motions) : \n    \n    # Cat√©gorie 3 (ex: pays) : \n    \n    # Cat√©gorie 4 (ex: m√©tiers) : \n]\n\n# TODO: V√©rifiez que tous vos mots sont dans le vocabulaire\n# for word in my_words:\n#     if word not in model:\n#         print(f\"'{word}' n'est pas dans le vocabulaire !\")\n\n# TODO: Copiez et adaptez le code de visualisation de la cellule pr√©c√©dente\n# Remplacez 'words' par 'my_words'\n\n\n# QUESTION : Les mots de m√™me cat√©gorie sont-ils regroup√©s ? \n# Y a-t-il des surprises ?",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 4. R√©capitulatif\n\n### Le pipeline NLP\n\n| √âtape | Entr√©e | Sortie | R√¥le |\n|-------|--------|--------|------|\n| **Tokenization** | Texte brut | Liste de tokens | D√©couper le texte |\n| **Vocabulaire** | Tokens | Indices | Table token ‚Üî ID |\n| **Embedding** | Indices | Vecteurs denses | Sens s√©mantique |\n\n### Points cl√©s\n\n1. **Tokenization BPE** : meilleur compromis entre mots et caract√®res\n2. **Embeddings** : transforment les mots en vecteurs comparables\n3. **Word2Vec** : montre que les embeddings capturent le sens (analogies !)\n4. **Similarit√© cosinus** : mesure la proximit√© entre vecteurs\n\n### Prochaine session\n\nNous verrons le **m√©canisme d'attention** : comment les mots \"communiquent\" entre eux pour se comprendre mutuellement.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 5. Pour aller plus loin (optionnel)\n\n### Ressources\n\n- [Les tokenizers en NLP (FR)](https://lbourdois.github.io/blog/nlp/Les-tokenizers/) - Excellent article en fran√ßais\n- [The Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/) - Visualisations tr√®s claires\n- [Bolmo: Byte-level Language Models (Allen AI, 2025)](https://allenai.org/blog/bolmo) - Une alternative √† BPE ?\n\n### Exp√©rimentations sugg√©r√©es\n\n1. Tester d'autres analogies Word2Vec\n2. Visualiser les embeddings de votre choix\n3. Comparer diff√©rents tokenizers (BERT, GPT-2, etc.)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 6. Teaser : Le m√©canisme d'attention\n\nMaintenant que nous savons repr√©senter du texte (tokenization + embeddings), la prochaine √©tape est de permettre aux mots de **communiquer entre eux**.\n\nC'est le r√¥le du **m√©canisme d'attention**, que nous verrons en d√©tail au prochain TP.\n\n### L'id√©e\n\n> Pour comprendre un mot, il faut regarder les autres mots de la phrase.\n\nExemple : *\"Le chat qui dormait sur le canap√© a saut√©\"*\n- Pour comprendre **\"a saut√©\"** ‚Üí regarder **\"chat\"** (le sujet)\n- Pour comprendre **\"dormait\"** ‚Üí regarder **\"chat\"** et **\"canap√©\"**\n\n### Visualisation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Matrice d'attention simul√©e\nphrase = [\"Le\", \"chat\", \"mange\", \"la\", \"souris\"]\n\n# Chaque ligne = un mot qui \"regarde\" les autres\n# Valeurs = poids d'attention (somme = 1 par ligne)\nattention = torch.tensor([\n    [0.8, 0.1, 0.05, 0.03, 0.02],  # \"Le\" regarde surtout lui-m√™me\n    [0.1, 0.7, 0.1, 0.05, 0.05],   # \"chat\" regarde surtout lui-m√™me\n    [0.05, 0.4, 0.4, 0.05, 0.1],   # \"mange\" regarde \"chat\" et lui-m√™me\n    [0.02, 0.03, 0.05, 0.8, 0.1],  # \"la\" regarde surtout lui-m√™me\n    [0.02, 0.1, 0.2, 0.08, 0.6],   # \"souris\" regarde \"mange\" et elle-m√™me\n])\n\n# Visualisation\nplt.figure(figsize=(8, 6))\nplt.imshow(attention, cmap='Blues')\nplt.xticks(range(5), phrase)\nplt.yticks(range(5), phrase)\nplt.xlabel(\"Mots regard√©s\")\nplt.ylabel(\"Mots qui regardent\")\nplt.title(\"Qui regarde qui ? (Matrice d'attention)\")\nplt.colorbar(label=\"Poids d'attention\")\n\nfor i in range(5):\n    for j in range(5):\n        plt.text(j, i, f'{attention[i,j]:.2f}', \n                ha='center', va='center',\n                color='white' if attention[i,j] > 0.5 else 'black')\nplt.show()\n\nprint(\"Le verbe 'mange' regarde fortement 'chat' (son sujet) !\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Ce qu'on voit** :\n- Chaque mot peut \"regarder\" tous les autres mots\n- Les poids indiquent l'importance de chaque relation\n- Le mod√®le **apprend** ces poids pendant l'entra√Ænement\n\n**Au prochain TP**, nous verrons :\n- Comment calculer cette matrice d'attention\n- Les concepts Query, Key, Value\n- L'architecture compl√®te du Transformer"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 7. Mini-projet : Construire son propre pipeline NLP\n",
    "\n",
    "> **üè† BONUS √Ä FAIRE CHEZ SOI**\n",
    "> \n",
    "> Cette section est **optionnelle** et ne fait pas partie du TP en session.\n",
    "> Elle est propos√©e pour les √©tudiants qui souhaitent approfondir √† la maison.\n",
    "\n",
    "Dans ce projet, vous allez **tout construire √† partir de libs g√©n√©riques** : votre propre tokenizer BPE et vos propres embeddings Word2Vec, entra√Æn√©s sur un corpus th√©matique Pok√©mon !\n",
    "\n",
    "### Objectifs\n",
    "\n",
    "1. **Entra√Æner un tokenizer BPE** adapt√© au vocabulaire Pok√©mon\n",
    "2. **Entra√Æner Word2Vec** sur les tokens BPE\n",
    "3. **Explorer les similarit√©s** entre Pok√©mon, types, attaques...\n",
    "4. **(Avanc√©)** Comparer avec un mod√®le fran√ßais pr√©-entra√Æn√© fine-tun√©\n",
    "\n",
    "### Structure du projet\n",
    "\n",
    "| Partie | Contenu | Difficult√© |\n",
    "|--------|---------|------------|\n",
    "| **Partie 1** | BPE custom + Word2Vec from scratch | ‚≠ê‚≠ê |\n",
    "| **Partie 2** | Fine-tuning FastText fran√ßais | ‚≠ê‚≠ê‚≠ê |\n",
    "| **Partie 3** | Comparaison des deux approches | ‚≠ê |\n",
    "\n",
    "---\n",
    "\n",
    "### Partie 1 : Pipeline from scratch\n",
    "\n",
    "#### 1.1 Chargement du corpus Pok√©mon\n",
    "\n",
    "Nous utilisons un corpus extrait de **Pok√©pedia** (le wiki Pok√©mon francophone) contenant les descriptions de Pok√©mon, attaques, lieux, et bien plus."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# PARTIE 1.1 : Chargement du corpus Pok√©mon\n# ============================================\n\nfrom datasets import load_dataset\n\n# Charger le corpus Pok√©mon depuis Hugging Face\ndataset = load_dataset(\"chris-lmd/pokepedia-fr\")\n\nprint(f\"Nombre d'articles : {len(dataset['train'])}\")\n\n# Extraire le texte\ncorpus_texts = [article[\"content\"] for article in dataset[\"train\"]]\n\n# Aper√ßu\nprint(f\"\\n=== Exemple d'article ===\")\nprint(f\"Titre : {dataset['train'][0]['title']}\")\nprint(f\"Contenu (extrait) :\\n{corpus_texts[0][:500]}...\")\n\n# Statistiques\ntotal_chars = sum(len(t) for t in corpus_texts)\ntotal_words = sum(len(t.split()) for t in corpus_texts)\nprint(f\"\\n=== Statistiques du corpus ===\")\nprint(f\"Nombre d'articles : {len(corpus_texts)}\")\nprint(f\"Nombre total de mots : {total_words:,}\")\nprint(f\"Nombre total de caract√®res : {total_chars:,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### 1.2 Entra√Ænement du tokenizer BPE\n\nNous allons cr√©er notre propre tokenizer BPE adapt√© au vocabulaire Pok√©mon. La biblioth√®que `tokenizers` de Hugging Face permet d'entra√Æner un BPE en quelques lignes.\n\n**Rappel** : BPE fusionne it√©rativement les paires de caract√®res les plus fr√©quentes. Sur notre corpus, il apprendra des tokens comme :\n- `\"Pika\"` + `\"chu\"` ‚Üí fr√©quent ensemble\n- `\"Draco\"` + `\"feu\"` ‚Üí fusion possible\n- `\"√©volu\"` + `\"tion\"` ‚Üí motif r√©current",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# PARTIE 1.2 : Entra√Ænement du tokenizer BPE\n# ============================================\n\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom collections import Counter\n\n# === √âtape 0 : Analyser les mots uniques du corpus ===\nprint(\"Analyse du corpus...\")\nall_words = []\nfor text in corpus_texts:\n    # Tokenization simple par espaces (mots bruts)\n    words = text.split()\n    all_words.extend(words)\n\nword_counts = Counter(all_words)\nunique_words = set(all_words)\n\nprint(f\"  Nombre total de mots : {len(all_words):,}\")\nprint(f\"  Mots uniques : {len(unique_words):,}\")\nprint(f\"  Top 10 mots : {word_counts.most_common(10)}\")\n\n# === √âtape 1 : Cr√©er et entra√Æner le tokenizer BPE ===\ntokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntokenizer.pre_tokenizer = Whitespace()\n\n# Taille du vocabulaire - ajustable !\nVOCAB_SIZE = 8000\n\ntrainer = BpeTrainer(\n    vocab_size=VOCAB_SIZE,\n    min_frequency=2,\n    special_tokens=[\"[UNK]\", \"[PAD]\"],\n    show_progress=True\n)\n\nprint(f\"\\nEntra√Ænement du tokenizer BPE (vocab_size={VOCAB_SIZE})...\")\ntokenizer.train_from_iterator(corpus_texts, trainer=trainer)\nprint(f\"Tokenizer entra√Æn√© ! Vocabulaire : {tokenizer.get_vocab_size()} tokens\")\n\n# === √âtape 2 : Analyser la couverture ===\nprint(\"\\n\" + \"=\" * 50)\nprint(\"ANALYSE DE LA COUVERTURE\")\nprint(\"=\" * 50)\n\n# Compter combien de mots uniques = 1 seul token\nwords_as_single_token = []\nwords_split = []\n\nfor word in unique_words:\n    tokens = tokenizer.encode(word).tokens\n    if len(tokens) == 1 and tokens[0] != \"[UNK]\":\n        words_as_single_token.append(word)\n    else:\n        words_split.append((word, tokens))\n\ncoverage = len(words_as_single_token) / len(unique_words) * 100\n\nprint(f\"\\nMots uniques du corpus : {len(unique_words):,}\")\nprint(f\"Mots = 1 token (couverts) : {len(words_as_single_token):,} ({coverage:.1f}%)\")\nprint(f\"Mots d√©coup√©s en 2+ tokens : {len(words_split):,} ({100-coverage:.1f}%)\")\n\nprint(f\"\\nüìä Ratio vocab_size / mots_uniques : {VOCAB_SIZE / len(unique_words):.1%}\")\n\n# Exemples\nprint(\"\\n‚úÖ Exemples de mots couverts (1 token) :\")\nsample_covered = [w for w in words_as_single_token if len(w) > 4][:10]\nprint(f\"  {sample_covered}\")\n\nprint(\"\\n‚ùå Exemples de mots d√©coup√©s :\")\nsample_split = [(w, t) for w, t in words_split if len(w) > 5 and len(t) <= 4][:5]\nfor word, tokens in sample_split:\n    print(f\"  '{word}' ‚Üí {tokens}\")\n\n# === √âtape 3 : Test du tokenizer ===\nprint(\"\\n\" + \"=\" * 50)\nprint(\"TEST DU TOKENIZER\")\nprint(\"=\" * 50)\n\ntest_phrases = [\n    \"Pikachu utilise Tonnerre\",\n    \"Dracaufeu est un Pok√©mon de type Feu\",\n    \"√âvolution de Salam√®che en Reptincel\",\n    \"M√©ga-√âvolution disponible\"\n]\n\nfor phrase in test_phrases:\n    output = tokenizer.encode(phrase)\n    print(f\"'{phrase}'\")\n    print(f\"  ‚Üí {output.tokens}\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### Comment interpr√©ter ces r√©sultats ?\n\n> **Pourquoi seulement ~4% de couverture ?**\n> \n> Ce chiffre peut sembler faible, mais c'est **normal et attendu** :\n> \n> 1. **BPE optimise pour la fr√©quence** : Les mots tr√®s fr√©quents deviennent des tokens entiers, les mots rares sont d√©coup√©s en sous-mots connus\n> \n> 2. **107k mots uniques** inclut beaucoup de \"bruit\" : hapax (mots vus 1 seule fois), mots avec ponctuation coll√©e, fautes de frappe, mots √©trangers...\n> \n> 3. **L'important** : Les mots du domaine (Pikachu, Dracaufeu, √©volution, attaque...) sont bien des tokens entiers\n> \n> **R√®gle pratique** :\n> - Si les mots **importants** de votre domaine sont d√©coup√©s ‚Üí augmenter `vocab_size`\n> - Si beaucoup de tokens n'apparaissent que 1-2 fois ‚Üí r√©duire `vocab_size`\n> \n> **Ordres de grandeur** :\n> | Mod√®le | vocab_size | Corpus |\n> |--------|------------|--------|\n> | GPT-2 | 50k | ~40 Go |\n> | CamemBERT | 32k | ~138 Go |\n> | Notre corpus | 8k | ~10 Mo |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### 1.3 Tokenization du corpus\n\nMaintenant, appliquons notre tokenizer BPE √† tout le corpus pour pr√©parer l'entra√Ænement de Word2Vec.",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ============================================\n# PARTIE 1.3 : Tokenization du corpus complet\n# ============================================\n\nimport random\n\n# Tokenizer tout le corpus\nprint(\"Tokenization du corpus...\")\ncorpus_tokenized = []\n\nfor i, text in enumerate(corpus_texts):\n    # D√©couper le texte en phrases (approximatif)\n    sentences = text.replace('\\n', ' ').split('. ')\n    \n    for sentence in sentences:\n        if len(sentence.strip()) > 10:  # Ignorer les phrases trop courtes\n            output = tokenizer.encode(sentence)\n            tokens = output.tokens\n            if len(tokens) >= 3:  # Au moins 3 tokens pour Word2Vec\n                corpus_tokenized.append(tokens)\n    \n    if (i + 1) % 1000 == 0:\n        print(f\"  {i + 1}/{len(corpus_texts)} articles trait√©s...\")\n\nprint(f\"\\nCorpus tokenis√© !\")\nprint(f\"Nombre de phrases : {len(corpus_tokenized):,}\")\nprint(f\"Nombre total de tokens : {sum(len(s) for s in corpus_tokenized):,}\")\n\n# Exemples al√©atoires (phrases assez longues)\nprint(\"\\n=== Exemples de phrases tokenis√©es ===\")\nlong_phrases = [p for p in corpus_tokenized if len(p) >= 12]\nfor phrase in random.sample(long_phrases, min(5, len(long_phrases))):\n    print(f\"  {phrase[:12]}...\")"
  },
  {
   "cell_type": "markdown",
   "source": "#### 1.4 Entra√Ænement de Word2Vec\n\nNous utilisons **gensim** pour entra√Æner Word2Vec sur notre corpus tokenis√©.\n\n**Param√®tres importants** :\n- `vector_size` : dimension des embeddings (100-300 typique)\n- `window` : taille de la fen√™tre de contexte\n- `min_count` : ignorer les tokens trop rares\n- `sg=1` : utiliser Skip-gram (meilleur pour les mots rares)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# PARTIE 1.4 : Entra√Ænement de Word2Vec\n# ============================================\n\nfrom gensim.models import Word2Vec\n\nprint(\"Entra√Ænement de Word2Vec...\")\n\n# Entra√Æner le mod√®le\nmodel_w2v = Word2Vec(\n    sentences=corpus_tokenized,\n    vector_size=100,      # Dimension des embeddings\n    window=5,             # Fen√™tre de contexte (¬±5 mots)\n    min_count=3,          # Ignorer les tokens vus < 3 fois\n    sg=1,                 # Skip-gram (1) vs CBOW (0)\n    workers=4,            # Parall√©lisation\n    epochs=10             # Nombre de passes sur le corpus\n)\n\nprint(f\"Mod√®le entra√Æn√© !\")\nprint(f\"Vocabulaire Word2Vec : {len(model_w2v.wv)} tokens\")\nprint(f\"Dimension des vecteurs : {model_w2v.wv.vector_size}\")\n\n# Sauvegarder le mod√®le (optionnel)\n# model_w2v.save(\"pokemon_word2vec.model\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "#### 1.5 Exploration des embeddings Pok√©mon\n\nC'est le moment de tester si notre mod√®le a captur√© les relations s√©mantiques du monde Pok√©mon !",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# PARTIE 1.5 : Exploration des similarit√©s\n# ============================================\n\n# Fonction utilitaire pour chercher un token\ndef find_token(word, model):\n    \"\"\"Cherche un token dans le vocabulaire (insensible √† la casse).\"\"\"\n    word_lower = word.lower()\n    for token in model.wv.key_to_index:\n        if word_lower == token.lower():\n            return token\n    return None\n\n# Fonction pour afficher les similarit√©s\ndef show_similar(word, model, topn=10):\n    \"\"\"Affiche les mots les plus similaires.\"\"\"\n    token = find_token(word, model)\n    if token is None:\n        print(f\"'{word}' non trouv√© dans le vocabulaire\")\n        return\n    \n    print(f\"=== Mots similaires √† '{token}' ===\")\n    try:\n        for similar, score in model.wv.most_similar(token, topn=topn):\n            print(f\"  {similar}: {score:.4f}\")\n    except KeyError as e:\n        print(f\"Erreur: {e}\")\n\n# Tester avec des noms de Pok√©mon\nprint(\"=\" * 50)\nshow_similar(\"Pikachu\", model_w2v)\nprint()\nshow_similar(\"Dracaufeu\", model_w2v)\nprint()\nshow_similar(\"√âvolution\", model_w2v)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# EXERCICE 5 : Explorer votre mod√®le Pok√©mon\n# ============================================\n\n# TODO A : Testez les similarit√©s avec d'autres Pok√©mon\n# Exemples : Salam√®che, Carapuce, Bulbizarre, Mewtwo, Rondoudou...\n# show_similar(\"...\", model_w2v)\n\n\n# TODO B : Testez avec des types (Feu, Eau, Plante, √âlectrik...)\n# show_similar(\"Feu\", model_w2v)\n\n\n# TODO C : Testez avec des attaques (Tonnerre, Lance-Flammes, Surf...)\n# show_similar(\"Tonnerre\", model_w2v)\n\n\n# TODO D : Essayez des analogies Pok√©mon !\n# Exemple : Pikachu - √âlectrik + Feu = ?\n# model_w2v.wv.most_similar(positive=[\"Pikachu\", \"Feu\"], negative=[\"√âlectrik\"], topn=5)\n\n\n# QUESTIONS DE R√âFLEXION :\n# 1. Les Pok√©mon de m√™me type sont-ils proches ?\n# 2. Les √©volutions sont-elles proches (Salam√®che ‚Üî Dracaufeu) ?\n# 3. Quelles analogies fonctionnent ? Lesquelles √©chouent ?",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# PARTIE 1.6 : Visualisation des embeddings\n# ============================================\n\nfrom sklearn.decomposition import PCA\n\n# S√©lectionner des tokens int√©ressants (√† adapter selon votre vocabulaire)\n# V√©rifiez d'abord quels tokens existent :\nprint(\"=== Quelques tokens du vocabulaire ===\")\nvocab_sample = list(model_w2v.wv.key_to_index.keys())[:50]\nprint(vocab_sample)\n\n# TODO: S√©lectionnez des tokens pour la visualisation\n# Choisissez des Pok√©mon, types, attaques...\ntokens_to_plot = [\n    # Pok√©mon (v√©rifiez qu'ils existent dans vocab_sample ou cherchez-les)\n    # \"Pikachu\", \"Raichu\", ...\n    \n    # Types\n    # \"Feu\", \"Eau\", ...\n    \n    # Attaques\n    # \"Tonnerre\", ...\n]\n\n# Filtrer les tokens qui existent dans le vocabulaire\ntokens_valid = [t for t in tokens_to_plot if t in model_w2v.wv]\nprint(f\"\\nTokens valides : {len(tokens_valid)}/{len(tokens_to_plot)}\")\n\nif len(tokens_valid) >= 5:\n    # R√©cup√©rer les vecteurs\n    vectors = np.array([model_w2v.wv[t] for t in tokens_valid])\n    \n    # R√©duire √† 2D\n    pca = PCA(n_components=2)\n    vectors_2d = pca.fit_transform(vectors)\n    \n    # Visualiser\n    plt.figure(figsize=(14, 10))\n    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], c='red', s=100)\n    \n    for i, token in enumerate(tokens_valid):\n        plt.annotate(token, (vectors_2d[i, 0] + 0.02, vectors_2d[i, 1] + 0.02), fontsize=10)\n    \n    plt.title(\"Embeddings Pok√©mon (notre mod√®le) projet√©s en 2D\")\n    plt.xlabel(\"Composante 1\")\n    plt.ylabel(\"Composante 2\")\n    plt.grid(True, alpha=0.3)\n    plt.show()\nelse:\n    print(\"Pas assez de tokens valides pour la visualisation.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### Partie 2 : Fine-tuning FastText fran√ßais (Bonus ‚≠ê‚≠ê‚≠ê)\n\n> ‚ö†Ô∏è **Attention : Ressources requises**\n> \n> Cette partie n√©cessite **~8 Go de RAM** pour charger le mod√®le FastText fran√ßais.\n> - **Google Colab gratuit** (12.7 Go) : peut fonctionner mais risque de saturation m√©moire\n> - **Google Colab Pro** (25+ Go) : recommand√©\n> - **Machine locale** : 16 Go RAM minimum\n> \n> Si vous rencontrez une erreur m√©moire, passez directement √† la **Partie 3** qui compare votre mod√®le Word2Vec avec les concepts th√©oriques.\n\nDans cette partie, nous allons **fine-tuner** un mod√®le FastText pr√©-entra√Æn√© sur le fran√ßais, puis l'enrichir avec notre corpus Pok√©mon.\n\n**Diff√©rence avec la Partie 1** :\n- Partie 1 : On part de z√©ro ‚Üí le mod√®le ne conna√Æt QUE le monde Pok√©mon\n- Partie 2 : On part d'un mod√®le fran√ßais ‚Üí il conna√Æt d√©j√† la langue + on ajoute Pok√©mon\n\n**Avantage de FastText** : Il g√®re les **sous-mots**, donc m√™me un mot jamais vu comme \"M√©ga-Dracaufeu\" sera partiellement compris.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# PARTIE 2.1 : T√©l√©chargement du mod√®le FastText fran√ßais\n# ============================================\n\nimport os\nfrom gensim.models.fasttext import load_facebook_model\n\n# Le mod√®le FastText fran√ßais officiel (~1.2 Go compress√©, ~7 Go en m√©moire)\n# Source : https://fasttext.cc/docs/en/crawl-vectors.html\n\nMODEL_PATH = \"cc.fr.300.bin\"\nMODEL_GZ_PATH = \"cc.fr.300.bin.gz\"\n\n# T√©l√©charger le mod√®le fran√ßais (si pas d√©j√† fait)\nif not os.path.exists(MODEL_PATH):\n    if not os.path.exists(MODEL_GZ_PATH):\n        print(\"üì• T√©l√©chargement du mod√®le FastText fran√ßais (~1.2 Go)...\")\n        print(\"‚ö†Ô∏è Cela peut prendre 5-10 minutes selon votre connexion.\\n\")\n        !wget -q --show-progress https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.bin.gz\n    \n    print(\"\\nüì¶ D√©compression du mod√®le...\")\n    !gunzip -k cc.fr.300.bin.gz\n    print(\"‚úÖ D√©compression termin√©e !\")\nelse:\n    print(f\"‚úÖ Mod√®le d√©j√† pr√©sent : {MODEL_PATH}\")\n\n# Charger le mod√®le avec gensim (permet le fine-tuning !)\nprint(\"\\nüîÑ Chargement du mod√®le FastText fran√ßais...\")\nprint(\"(Cela peut prendre 1-2 minutes et utiliser ~7 Go de RAM)\")\n\nmodel_fasttext_fr = load_facebook_model(MODEL_PATH)\n\nprint(f\"\\n‚úÖ Mod√®le charg√© !\")\nprint(f\"   Vocabulaire : {len(model_fasttext_fr.wv):,} mots\")\nprint(f\"   Dimension : {model_fasttext_fr.wv.vector_size}\")\n\n# Test rapide du mod√®le fran√ßais\nprint(\"\\n=== Test du mod√®le fran√ßais ===\")\ntest_words_fr = [\"chat\", \"chien\", \"voiture\", \"ordinateur\", \"maison\"]\nfor word in test_words_fr:\n    try:\n        similar = model_fasttext_fr.wv.most_similar(word, topn=3)\n        print(f\"'{word}' ‚Üí {[w for w, s in similar]}\")\n    except KeyError:\n        print(f\"'{word}' ‚Üí non trouv√©\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# PARTIE 2.2 : Fine-tuning sur le corpus Pok√©mon\n# ============================================\n\n# Avec gensim, on peut continuer l'entra√Ænement d'un mod√®le FastText\n# en ajoutant de nouveaux mots au vocabulaire puis en r√©entra√Ænant\n\nprint(\"üìä Pr√©paration du corpus pour le fine-tuning...\")\n\n# Pr√©parer le corpus (liste de listes de mots)\ncorpus_for_finetuning = []\nfor text in corpus_texts:\n    sentences = text.replace('\\n', ' ').split('. ')\n    for sentence in sentences:\n        words = sentence.strip().split()\n        if len(words) >= 3:  # Au moins 3 mots par phrase\n            corpus_for_finetuning.append(words)\n\nprint(f\"   Phrases pr√©par√©es : {len(corpus_for_finetuning):,}\")\nprint(f\"   Tokens totaux : {sum(len(s) for s in corpus_for_finetuning):,}\")\n\n# Vocabulaire avant fine-tuning\nvocab_before = len(model_fasttext_fr.wv)\n\n# √âtendre le vocabulaire avec les mots Pok√©mon\nprint(\"\\nüîß Extension du vocabulaire avec les mots Pok√©mon...\")\nmodel_fasttext_fr.build_vocab(corpus_for_finetuning, update=True)\n\nvocab_after = len(model_fasttext_fr.wv)\nnew_words = vocab_after - vocab_before\nprint(f\"   Vocabulaire avant : {vocab_before:,} mots\")\nprint(f\"   Vocabulaire apr√®s : {vocab_after:,} mots\")\nprint(f\"   Nouveaux mots ajout√©s : {new_words:,}\")\n\n# Fine-tuning : continuer l'entra√Ænement sur le corpus Pok√©mon\nprint(\"\\nüöÄ Fine-tuning en cours (5 epochs)...\")\nprint(\"   Cela peut prendre quelques minutes...\")\n\nmodel_fasttext_fr.train(\n    corpus_for_finetuning,\n    total_examples=len(corpus_for_finetuning),\n    epochs=5\n)\n\nprint(\"\\n‚úÖ Fine-tuning termin√© !\")\n\n# Test avec des mots Pok√©mon\nprint(\"\\n=== Test apr√®s fine-tuning ===\")\ntest_pokemon = [\"Pikachu\", \"Dracaufeu\", \"√©volution\", \"attaque\", \"Pok√©mon\"]\nfor word in test_pokemon:\n    try:\n        similar = model_fasttext_fr.wv.most_similar(word, topn=5)\n        print(f\"'{word}' ‚Üí\")\n        for w, score in similar:\n            print(f\"    {w}: {score:.3f}\")\n        print()\n    except KeyError:\n        print(f\"'{word}' ‚Üí non trouv√©\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### Partie 3 : Comparaison des approches\n\nComparons maintenant les deux mod√®les (si vous avez fait la Partie 2) ou analysons les forces et faiblesses de notre mod√®le from scratch.\n\n| Crit√®re | BPE + Word2Vec (Partie 1) | FastText fine-tun√© (Partie 2) |\n|---------|---------------------------|------------------------------|\n| **Vocabulaire** | Tokens BPE Pok√©mon | Mots fran√ßais + Pok√©mon |\n| **Mots inconnus** | `[UNK]` | G√©r√© par sous-mots |\n| **Mots FR courants** | Qualit√© variable | Excellente (pr√©-entra√Æn√©) |\n| **Mots Pok√©mon rares** | D√©pend de la fr√©quence | Sous-mots aident |\n| **Taille mod√®le** | ~10-50 Mo | ~1.2 Go |\n| **Temps d'entra√Ænement** | Rapide | Plus long |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# PARTIE 3 : Tests comparatifs\n# ============================================\n\n# V√©rifier si le mod√®le FastText a √©t√© charg√© (Partie 2)\nfasttext_available = 'model_fasttext_fr' in dir() and model_fasttext_fr is not None\n\n# Tests √† effectuer\ntest_words = [\n    # Pok√©mon populaires\n    \"Pikachu\", \"Dracaufeu\", \"Mewtwo\",\n    # Types\n    \"Feu\", \"Eau\", \"√âlectrik\",\n    # Mots du domaine Pok√©mon\n    \"attaque\", \"combat\", \"√©volution\",\n    # Mots fran√ßais g√©n√©raux (test du pr√©-entra√Ænement)\n    \"ordinateur\", \"maison\", \"voiture\"\n]\n\nprint(\"=\" * 60)\nprint(\"COMPARAISON DES MOD√àLES\")\nprint(\"=\" * 60)\n\nprint(\"\\nüì¶ Mod√®le 1 : BPE + Word2Vec (from scratch)\")\nprint(\"-\" * 40)\nfor word in test_words:\n    token = find_token(word, model_w2v)\n    if token:\n        print(f\"  ‚úÖ {word} ‚Üí '{token}'\")\n    else:\n        print(f\"  ‚ùå {word} ‚Üí non trouv√©\")\n\nif fasttext_available:\n    print(\"\\nüì¶ Mod√®le 2 : FastText fran√ßais fine-tun√©\")\n    print(\"-\" * 40)\n    for word in test_words:\n        try:\n            vec = model_fasttext_fr.wv[word]\n            in_vocab = word in model_fasttext_fr.wv.key_to_index\n            status = \"‚úÖ\" if in_vocab else \"üî∂\"\n            print(f\"  {status} {word} ‚Üí vecteur dim {len(vec)}\" + \n                  (\" (inf√©r√©)\" if not in_vocab else \"\"))\n        except:\n            print(f\"  ‚ùå {word} ‚Üí erreur\")\n\n    # Comparaison des similarit√©s\n    print(\"\\n\" + \"=\" * 60)\n    print(\"COMPARAISON DES SIMILARIT√âS\")\n    print(\"=\" * 60)\n\n    comparison_words = [\"Pikachu\", \"√©volution\", \"maison\"]\n\n    for word in comparison_words:\n        print(f\"\\nüîç Mots similaires √† '{word}' :\")\n        \n        # Mod√®le 1\n        token = find_token(word, model_w2v)\n        if token:\n            try:\n                similar_w2v = model_w2v.wv.most_similar(token, topn=5)\n                print(f\"  Word2Vec: {[w for w, s in similar_w2v]}\")\n            except:\n                print(f\"  Word2Vec: erreur\")\n        else:\n            print(f\"  Word2Vec: non trouv√©\")\n        \n        # Mod√®le 2\n        try:\n            similar_ft = model_fasttext_fr.wv.most_similar(word, topn=5)\n            print(f\"  FastText: {[w for w, s in similar_ft]}\")\n        except:\n            print(f\"  FastText: erreur\")\nelse:\n    print(\"\\n‚ö†Ô∏è Mod√®le FastText non disponible (Partie 2 non ex√©cut√©e ou erreur m√©moire)\")\n    print(\"   Vous pouvez quand m√™me analyser les r√©sultats de votre mod√®le Word2Vec ci-dessus.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Questions de r√©flexion finale\n\nR√©pondez √† ces questions dans une cellule markdown ou dans un document s√©par√© :\n\n1. **Qualit√© des embeddings** : Votre mod√®le from scratch capture-t-il bien les relations Pok√©mon ? Donnez des exemples de similarit√©s qui fonctionnent et d'autres qui √©chouent.\n\n2. **Impact du tokenizer** : Comment le choix de la taille du vocabulaire BPE (8000 tokens) affecte-t-il les r√©sultats ? Que se passerait-il avec 2000 ou 20000 tokens ?\n\n3. **Limites du corpus** : Quels types de relations votre mod√®le ne peut PAS capturer ? (Indice : pensez aux informations absentes du corpus textuel)\n\n4. **Comparaison** (si Partie 2 faite) : Dans quel cas pr√©f√©reriez-vous le mod√®le from scratch ? Le mod√®le fine-tun√© ? Justifiez.\n\n5. **Applications** : Comment pourrait-on utiliser ces embeddings Pok√©mon dans une application r√©elle ? (ex: moteur de recherche, recommandation, chatbot...)\n\n---\n\n### F√©licitations ! üéâ\n\nVous avez construit un **pipeline de repr√©sentation de texte** :\n\n- ‚úÖ Tokenizer BPE entra√Æn√© sur un corpus sp√©cialis√©\n- ‚úÖ Embeddings Word2Vec capturant la s√©mantique Pok√©mon\n- ‚úÖ Exploration des similarit√©s et analogies\n- ‚úÖ (Bonus) Comparaison avec un mod√®le pr√©-entra√Æn√©\n\nCes techniques (tokenization subword + embeddings) sont les **briques de base** des mod√®les de langage modernes. GPT utilise BPE, et le concept d'embeddings appris est au c≈ìur de tous les Transformers. Dans les prochains TPs, nous verrons comment le m√©canisme d'**attention** permet de capturer le contexte !",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}