{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 02 - Le Mécanisme d'Attention - CORRECTION\n",
    "\n",
    "**Module** : Réseaux de Neurones Approfondissement  \n",
    "**Durée** : 2h  \n",
    "**Objectif** : Comprendre et implémenter le mécanisme d'attention\n",
    "\n",
    "---\n",
    "\n",
    "**VERSION ENSEIGNANT AVEC CORRECTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 : Calcul des scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 3\n",
    "d_k = 4\n",
    "\n",
    "Q = torch.randn(seq_len, d_k)\n",
    "K = torch.randn(seq_len, d_k)\n",
    "V = torch.randn(seq_len, d_k)\n",
    "\n",
    "# ============================================\n",
    "# CORRECTION EXERCICE 1\n",
    "# ============================================\n",
    "scores = Q @ K.T  # ou K.transpose(-2, -1) pour les batches\n",
    "\n",
    "print(\"Scores (Q @ K^T):\")\n",
    "print(scores)\n",
    "print(f\"Shape: {scores.shape}\")  # (3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 : Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CORRECTION EXERCICE 2\n",
    "# ============================================\n",
    "scaled_scores = scores / math.sqrt(d_k)\n",
    "\n",
    "print(\"Scaled scores:\")\n",
    "print(scaled_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 : Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CORRECTION EXERCICE 3\n",
    "# ============================================\n",
    "attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "\n",
    "print(\"Poids d'attention (après softmax):\")\n",
    "print(attention_weights)\n",
    "print(f\"\\nVérification - Somme par ligne: {attention_weights.sum(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 4 : Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CORRECTION EXERCICE 4\n",
    "# ============================================\n",
    "output = attention_weights @ V\n",
    "\n",
    "print(\"Output:\")\n",
    "print(output)\n",
    "print(f\"Shape: {output.shape}\")  # (3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5 : Fonction complète"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CORRECTION EXERCICE 5\n",
    "# ============================================\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Calcule le Scaled Dot-Product Attention.\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # 1. Calculer les scores\n",
    "    scores = Q @ K.transpose(-2, -1)\n",
    "    \n",
    "    # 2. Scaling\n",
    "    scaled_scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # 3. Softmax\n",
    "    attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "    \n",
    "    # 4. Output\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test\n",
    "Q_test = torch.randn(4, 8)\n",
    "K_test = torch.randn(4, 8)\n",
    "V_test = torch.randn(4, 8)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q_test, K_test, V_test)\n",
    "\n",
    "print(f\"Structure de sortie : {output.shape}\")  # (4, 8)\n",
    "print(f\"Structure des poids : {weights.shape}\")  # (4, 4)\n",
    "print(f\"Somme des poids par ligne : {weights.sum(dim=-1)}\")  # [1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 6 : Classe SelfAttention\n",
    "\n",
    "### Self-Attention vs Cross-Attention\n",
    "\n",
    "**Self-Attention** (ce qu'on fait ici) :\n",
    "- Q, K, V sont tous calculés à partir du **même** input `x`\n",
    "- Chaque mot regarde les autres mots **de la même phrase**\n",
    "\n",
    "**Cross-Attention** (on verra dans les projets) :\n",
    "- Q vient d'une source, K et V d'une **autre** source\n",
    "- Utilisé en traduction (décodeur interroge encodeur)\n",
    "\n",
    "### D'où viennent Q, K, V ?\n",
    "\n",
    "Dans les exercices 1-5 : tenseurs aléatoires pour comprendre le mécanisme.\n",
    "En pratique : **Q, K, V sont calculés à partir des embeddings**.\n",
    "\n",
    "| Rôle | Représentation | Question posée |\n",
    "|------|----------------|----------------|\n",
    "| **Query** | `Q = x @ W_q` | \"Qu'est-ce que je cherche ?\" |\n",
    "| **Key** | `K = x @ W_k` | \"Comment les autres me voient ?\" |\n",
    "| **Value** | `V = x @ W_v` | \"Quelle info je transmets ?\" |\n",
    "\n",
    "Les matrices W_q, W_k, W_v sont **apprises** : le modèle découvre quelles \"facettes\" sont utiles pour chaque rôle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CORRECTION EXERCICE 6\n",
    "# ============================================\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Module de Self-Attention.\n",
    "    \n",
    "    Projette l'input x vers Q, K, V puis applique l'attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Projections linéaires (3 matrices de poids distinctes)\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Projeter x vers Q, K, V\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Réutiliser la fonction de l'exercice 5\n",
    "        return scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# Test\n",
    "embed_dim = 32\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "attention_layer = SelfAttention(embed_dim)\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "output, weights = attention_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")  # (2, 5, 32)\n",
    "print(f\"Weights shape: {weights.shape}\")  # (2, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7 : Visualiser l'attention d'un vrai modèle\n",
    "\n",
    "Cette section utilise DistilBERT pour montrer des patterns d'attention réels.\n",
    "\n",
    "### Tokens spéciaux : [CLS] et [SEP]\n",
    "\n",
    "| Token | Signification | Rôle |\n",
    "|-------|---------------|------|\n",
    "| **[CLS]** | \"Classification\" | Ajouté au début. Son vecteur final représente toute la phrase. |\n",
    "| **[SEP]** | \"Separator\" | Ajouté à la fin. Sépare les phrases. |\n",
    "\n",
    "**Pourquoi [CLS] reçoit beaucoup d'attention ?** Il est entraîné pour \"résumer\" la phrase → normal qu'il attire l'attention.\n",
    "\n",
    "### Aperçu : Multi-Head Attention\n",
    "\n",
    "DistilBERT a **12 têtes par couche** × 6 couches = 72 patterns différents. Chaque tête capture des relations différentes (syntaxe, coréférence, positions...).\n",
    "\n",
    "> **On verra le Multi-Head en TP 03.** Ici, on visualise une tête qui capture bien \"it\" → \"cat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de transformers\n",
    "!pip install transformers -q\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Charger DistilBERT\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "model.eval()\n",
    "\n",
    "# Phrase de test\n",
    "phrase = \"The cat sat on the mat because it was tired\"\n",
    "inputs = tokenizer(phrase, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "attentions = outputs.attentions\n",
    "\n",
    "print(f\"Phrase: {phrase}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Visualisation - Couche 5, Tête 2 : capture bien \"it\" → \"cat\"\n",
    "layer = 4   # Couche 5 (0-indexed)\n",
    "head = 1    # Tête 2 (0-indexed)\n",
    "attention_matrix = attentions[layer][0, head].numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_matrix, cmap='Blues')\n",
    "plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "plt.yticks(range(len(tokens)), tokens)\n",
    "plt.title(f\"Attention réelle - Couche {layer+1}, Tête {head+1}\")\n",
    "plt.colorbar(label=\"Poids d'attention\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyse du pronom \"it\"\n",
    "it_index = tokens.index(\"it\")\n",
    "print(f\"\\nAttention de 'it' (Couche {layer+1}, Tête {head+1}) :\")\n",
    "print(\"-\" * 40)\n",
    "for token, weight in zip(tokens, attention_matrix[it_index]):\n",
    "    bar = \"█\" * int(weight * 30)\n",
    "    highlight = \" ← antécédent !\" if token == \"cat\" else \"\"\n",
    "    print(f\"  {token:10} {weight:.2f} {bar}{highlight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9 : Pour aller plus loin - GPT vs BERT\n",
    "\n",
    "### Masque causal (GPT)\n",
    "\n",
    "Le masque permet d'entraîner efficacement en un seul forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémentation du masque causal\n",
    "def scaled_dot_product_attention_with_mask(Q, K, V, mask=None):\n",
    "    \"\"\"Attention avec masking optionnel.\"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = attention_weights @ V\n",
    "    return output, attention_weights\n",
    "\n",
    "# Créer un masque causal (triangulaire supérieur)\n",
    "seq_len = 5\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "\n",
    "print(\"Masque causal (True = masqué):\")\n",
    "print(causal_mask.int())\n",
    "\n",
    "# Test\n",
    "Q = torch.randn(seq_len, 8)\n",
    "K = torch.randn(seq_len, 8)\n",
    "V = torch.randn(seq_len, 8)\n",
    "\n",
    "output, weights = scaled_dot_product_attention_with_mask(Q, K, V, causal_mask)\n",
    "\n",
    "print(\"\\nPoids d'attention avec masque:\")\n",
    "print(weights.round(decimals=2))\n",
    "print(\"\\n→ Chaque position ne voit que les positions précédentes (et elle-même)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
