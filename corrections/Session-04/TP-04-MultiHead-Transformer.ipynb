{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Session 4 - Multi-Head Attention & Fondations du Transformer (CORRECTION)\n\n**Module** : Réseaux de Neurones Approfondissement  \n**Durée** : 2h  \n**Objectif** : Compléter le Multi-Head Attention et comprendre les briques du Transformer\n\n---\n\n## Objectifs pédagogiques\n\nÀ la fin de cette session, vous serez capable de :\n1. Implémenter la classe `MultiHeadAttention` complète\n2. Comprendre le **Feed-Forward Network** et son rôle\n3. Expliquer **LayerNorm** et les **connexions résiduelles**\n4. Implémenter le **masque causal** (différence GPT vs BERT)\n5. Avoir une vision claire de l'architecture Transformer\n\n---\n\n## Rappel : Où en sommes-nous ?\n\nSession précédente :\n- ✅ Fonction `scaled_dot_product_attention()` complète\n- ✅ Classe `SelfAttention` avec W_q, W_k, W_v\n- ✅ Visualisation sur CamemBERT\n- ✅ Pourquoi Multi-Head + fonction `split_heads()`\n\nAujourd'hui, on **termine le Multi-Head** et on ajoute les **autres briques** du Transformer."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib numpy transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Rappel : split_heads et attention\n",
    "\n",
    "Reprenons les fonctions de la session précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        Q, K, V: shape (..., seq_len, d_k)\n",
    "        mask: Masque optionnel (True = position masquée)\n",
    "    \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "def split_heads(x, num_heads):\n",
    "    \"\"\"\n",
    "    Reshape: (batch, seq_len, embed_dim) -> (batch, num_heads, seq_len, d_k)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, embed_dim = x.shape\n",
    "    d_k = embed_dim // num_heads\n",
    "    x = x.view(batch_size, seq_len, num_heads, d_k)\n",
    "    return x.transpose(1, 2)\n",
    "\n",
    "\n",
    "print(\"✅ Fonctions chargées\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2. Exercice 1 : concat_heads\n\nAprès l'attention, on doit **recombiner les têtes**.\n\nC'est l'opération inverse de `split_heads` :\n- `(batch, num_heads, seq_len, d_k)` → `(batch, seq_len, embed_dim)`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_heads(x):\n",
    "    \"\"\"\n",
    "    Recombine les têtes d'attention.\n",
    "    \n",
    "    Reshape: (batch, num_heads, seq_len, d_k) -> (batch, seq_len, embed_dim)\n",
    "    \n",
    "    C'est l'inverse de split_heads.\n",
    "    \"\"\"\n",
    "    batch_size, num_heads, seq_len, d_k = x.shape\n",
    "    embed_dim = num_heads * d_k\n",
    "    \n",
    "    # CORRECTION:\n",
    "    # Étape 1: Remettre seq_len en position 1\n",
    "    x = x.transpose(1, 2)  # -> (batch, seq_len, num_heads, d_k)\n",
    "    # Étape 2: Fusionner num_heads et d_k\n",
    "    x = x.contiguous().view(batch_size, seq_len, embed_dim)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "x_test = torch.randn(2, 4, 6, 8)  # batch=2, heads=4, seq=6, d_k=8\n",
    "x_concat = concat_heads(x_test)\n",
    "\n",
    "print(f\"Avant concat: {x_test.shape}\")   # (2, 4, 6, 8)\n",
    "print(f\"Après concat: {x_concat.shape}\") # Attendu: (2, 6, 32)\n",
    "\n",
    "if x_concat is not None and x_concat.shape == (2, 6, 32):\n",
    "    print(\"\\n✅ Correct !\")\n",
    "else:\n",
    "    print(\"\\n❌ Vérifiez votre implémentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification : split puis concat doit donner le tenseur original\n",
    "x_original = torch.randn(2, 6, 32)\n",
    "x_split = split_heads(x_original, num_heads=4)\n",
    "x_back = concat_heads(x_split)\n",
    "\n",
    "print(f\"Original: {x_original.shape}\")\n",
    "print(f\"Split:    {x_split.shape}\")\n",
    "print(f\"Concat:   {x_back.shape}\")\n",
    "print(f\"\\nIdentique ? {torch.allclose(x_original, x_back)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. Exercice 2 : Classe MultiHeadAttention\n\nAssemblons tout dans une classe `MultiHeadAttention`.\n\n### Architecture\n\n```\nx ──► W_q ──► Q ──┐\nx ──► W_k ──► K ──┼──► split_heads ──► Attention ──► concat_heads ──► W_o ──► output\nx ──► W_v ──► V ──┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention.\n",
    "    \n",
    "    Args:\n",
    "        embed_dim: Dimension des embeddings\n",
    "        num_heads: Nombre de têtes d'attention\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim doit être divisible par num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads\n",
    "        \n",
    "        # CORRECTION 1: Créer les 4 projections linéaires\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)  # Projection de sortie\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input, shape (batch, seq_len, embed_dim)\n",
    "            mask: Masque optionnel\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, embed_dim)\n",
    "            attention_weights: (batch, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # CORRECTION 2: Projeter x vers Q, K, V\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # CORRECTION 3: Split en têtes\n",
    "        Q = split_heads(Q, self.num_heads)\n",
    "        K = split_heads(K, self.num_heads)\n",
    "        V = split_heads(V, self.num_heads)\n",
    "        \n",
    "        # CORRECTION 4: Appliquer l'attention\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # CORRECTION 5: Concat des têtes\n",
    "        concat_output = concat_heads(attn_output)\n",
    "        \n",
    "        # CORRECTION 6: Projection de sortie\n",
    "        output = self.W_o(concat_output)\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "mha = MultiHeadAttention(embed_dim=32, num_heads=4)\n",
    "\n",
    "x = torch.randn(2, 6, 32)  # batch=2, seq=6, embed=32\n",
    "output, weights = mha(x)\n",
    "\n",
    "print(f\"Input shape:   {x.shape}\")       # (2, 6, 32)\n",
    "print(f\"Output shape:  {output.shape}\")  # Attendu: (2, 6, 32)\n",
    "print(f\"Weights shape: {weights.shape}\") # Attendu: (2, 4, 6, 6)\n",
    "\n",
    "# Nombre de paramètres\n",
    "n_params = sum(p.numel() for p in mha.parameters())\n",
    "print(f\"\\nNombre de paramètres: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. Visualisation : 4 têtes sur CamemBERT\n\nVoyons ce que les différentes têtes capturent sur un modèle **réellement entraîné**."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import CamembertModel, CamembertTokenizer\n\n# Charger CamemBERT\ntokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\nmodel = CamembertModel.from_pretrained(\"camembert-base\", output_attentions=True)\nmodel.eval()\n\n# Phrase de test\nphrase = \"Le chat dort sur le canapé car il est fatigué\"\ninputs = tokenizer(phrase, return_tensors=\"pt\")\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nattentions = outputs.attentions\nprint(f\"Phrase: {phrase}\")\nprint(f\"Tokens: {tokens}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualiser 4 têtes de la couche 5\nlayer = 4\nheads_to_show = [0, 1, 5, 10]\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\nfor idx, head in enumerate(heads_to_show):\n    ax = axes[idx // 2, idx % 2]\n    w = attentions[layer][0, head].numpy()\n    \n    im = ax.imshow(w, cmap='Blues')\n    ax.set_xticks(range(len(tokens)))\n    ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=8)\n    ax.set_yticks(range(len(tokens)))\n    ax.set_yticklabels(tokens, fontsize=8)\n    ax.set_title(f\"Tête {head + 1}\", fontsize=11)\n    plt.colorbar(im, ax=ax)\n\nplt.suptitle(f\"Différentes têtes de la couche {layer+1} - Chaque tête capture des relations différentes\", fontsize=12)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Observation** : Chaque tête capture des **patterns différents** :\n- Certaines regardent les mots **proches**\n- D'autres capturent la **coréférence** (\"il\" → \"chat\")\n- Certaines se concentrent sur **\\<s\\>** ou **\\</s\\>**\n\nC'est la force du Multi-Head : **plusieurs experts** qui analysent sous différents angles."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Feed-Forward Network\n",
    "\n",
    "Après l'attention, chaque position passe par un **réseau feed-forward** identique.\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "### Pourquoi un FFN ?\n",
    "\n",
    "- L'attention capture les **relations entre tokens**\n",
    "- Le FFN **transforme** chaque token individuellement\n",
    "- Il ajoute de la **capacité non-linéaire** au modèle\n",
    "\n",
    "### Dimensions typiques\n",
    "\n",
    "La dimension cachée est généralement **4× embed_dim** :\n",
    "- Input: `embed_dim` (ex: 512)\n",
    "- Hidden: `4 × embed_dim` (ex: 2048)\n",
    "- Output: `embed_dim` (ex: 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-Forward Network (2 couches linéaires avec activation).\n",
    "    \n",
    "    Args:\n",
    "        embed_dim: Dimension d'entrée/sortie\n",
    "        ff_dim: Dimension cachée (défaut: 4 × embed_dim)\n",
    "        dropout: Taux de dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, ff_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        if ff_dim is None:\n",
    "            ff_dim = 4 * embed_dim\n",
    "        \n",
    "        # CORRECTION: Créer le réseau\n",
    "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()  # GELU est utilisé dans les Transformers modernes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (batch, seq_len, embed_dim)\n",
    "        Returns:\n",
    "            shape (batch, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        # CORRECTION: Implémenter le forward\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "ff = FeedForward(embed_dim=32, ff_dim=128)\n",
    "x = torch.randn(2, 6, 32)\n",
    "out = ff(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")    # (2, 6, 32)\n",
    "print(f\"Output shape: {out.shape}\")  # Attendu: (2, 6, 32)\n",
    "\n",
    "# Paramètres\n",
    "n_params = sum(p.numel() for p in ff.parameters())\n",
    "print(f\"\\nParamètres FFN: {n_params:,}\")\n",
    "print(f\"  - linear1: 32×128 + 128 = {32*128 + 128:,}\")\n",
    "print(f\"  - linear2: 128×32 + 32 = {128*32 + 32:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. LayerNorm et Connexions Résiduelles\n",
    "\n",
    "### Connexions résiduelles\n",
    "\n",
    "$$\\text{output} = x + \\text{SubLayer}(x)$$\n",
    "\n",
    "**Pourquoi ?** Permet aux gradients de mieux circuler (comme dans ResNet).\n",
    "\n",
    "### Layer Normalization\n",
    "\n",
    "Normalise sur la dimension des **features** (pas sur le batch comme BatchNorm).\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta$$\n",
    "\n",
    "**Pourquoi LayerNorm plutôt que BatchNorm ?**\n",
    "- Fonctionne avec des **séquences de longueur variable**\n",
    "- Indépendant de la **taille du batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démonstration LayerNorm\n",
    "x = torch.randn(2, 4, 8)  # (batch, seq, features)\n",
    "\n",
    "# LayerNorm normalise sur la dernière dimension (features)\n",
    "ln = nn.LayerNorm(8)\n",
    "x_norm = ln(x)\n",
    "\n",
    "print(\"Avant LayerNorm:\")\n",
    "print(f\"  Moyenne par position: {x[0, 0].mean():.4f}\")\n",
    "print(f\"  Écart-type par position: {x[0, 0].std():.4f}\")\n",
    "\n",
    "print(\"\\nAprès LayerNorm:\")\n",
    "print(f\"  Moyenne par position: {x_norm[0, 0].mean():.4f}\")\n",
    "print(f\"  Écart-type par position: {x_norm[0, 0].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture d'un bloc Transformer (Pre-LN)\n",
    "\n",
    "```\n",
    "Input\n",
    "  │\n",
    "  ├──────────────────┐\n",
    "  ↓                  │ (résiduel)\n",
    "LayerNorm            │\n",
    "  ↓                  │\n",
    "Multi-Head Attention │\n",
    "  ↓                  │\n",
    "Dropout              │\n",
    "  ↓                  │\n",
    "  + ←────────────────┘\n",
    "  │\n",
    "  ├──────────────────┐\n",
    "  ↓                  │ (résiduel)\n",
    "LayerNorm            │\n",
    "  ↓                  │\n",
    "Feed-Forward         │\n",
    "  ↓                  │\n",
    "Dropout              │\n",
    "  ↓                  │\n",
    "  + ←────────────────┘\n",
    "  ↓\n",
    "Output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Masque Causal : GPT vs BERT\n",
    "\n",
    "### Le problème\n",
    "\n",
    "Pour **générer du texte** (GPT), le modèle doit prédire le **mot suivant** sans voir le futur.\n",
    "\n",
    "```\n",
    "Phrase : \"Le chat dort\"\n",
    "\n",
    "BERT (bidirectionnel) :        GPT (causal) :\n",
    "- \"Le\" voit tout              - \"Le\" voit seulement \"Le\"\n",
    "- \"chat\" voit tout            - \"chat\" voit \"Le\", \"chat\"\n",
    "- \"dort\" voit tout            - \"dort\" voit \"Le\", \"chat\", \"dort\"\n",
    "```\n",
    "\n",
    "### La solution : Masque causal\n",
    "\n",
    "On **masque** les positions futures avec $-\\infty$ avant le softmax :\n",
    "\n",
    "$$\\text{softmax}(-\\infty) = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un masque causal\n",
    "seq_len = 4\n",
    "\n",
    "# torch.triu crée une matrice triangulaire supérieure\n",
    "# diagonal=1 : ne pas inclure la diagonale (un mot peut se voir lui-même)\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "\n",
    "print(\"Masque causal (True = position masquée) :\")\n",
    "print(causal_mask.int())\n",
    "print()\n",
    "print(\"Interprétation :\")\n",
    "print(\"  Ligne 0 (mot 1) : voit seulement position 0\")\n",
    "print(\"  Ligne 1 (mot 2) : voit positions 0, 1\")\n",
    "print(\"  Ligne 2 (mot 3) : voit positions 0, 1, 2\")\n",
    "print(\"  Ligne 3 (mot 4) : voit tout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation : attention avec et sans masque\n",
    "Q = torch.randn(1, 4, 8)\n",
    "K = torch.randn(1, 4, 8)\n",
    "V = torch.randn(1, 4, 8)\n",
    "\n",
    "# Sans masque (BERT-style)\n",
    "_, weights_bert = scaled_dot_product_attention(Q, K, V, mask=None)\n",
    "\n",
    "# Avec masque causal (GPT-style)\n",
    "_, weights_gpt = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "tokens_demo = [\"Le\", \"chat\", \"dort\", \"bien\"]\n",
    "\n",
    "for idx, (w, title) in enumerate([\n",
    "    (weights_bert[0], \"BERT (bidirectionnel)\\nChaque mot voit tout\"),\n",
    "    (weights_gpt[0], \"GPT (causal)\\nChaque mot voit seulement le passé\")\n",
    "]):\n",
    "    ax = axes[idx]\n",
    "    im = ax.imshow(w.numpy(), cmap='Blues')\n",
    "    ax.set_xticks(range(4))\n",
    "    ax.set_xticklabels(tokens_demo)\n",
    "    ax.set_yticks(range(4))\n",
    "    ax.set_yticklabels(tokens_demo)\n",
    "    ax.set_xlabel(\"Tokens regardés\")\n",
    "    ax.set_ylabel(\"Tokens qui regardent\")\n",
    "    ax.set_title(title)\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pourquoi c'est important ?\n",
    "\n",
    "| Modèle | Masque | Usage |\n",
    "|--------|--------|-------|\n",
    "| **BERT** | Pas de masque (bidirectionnel) | Classification, QA, NER |\n",
    "| **GPT** | Masque causal | Génération de texte |\n",
    "\n",
    "**Dans le Mini-GPT** (prochaines sessions), on utilisera le masque causal pour générer du texte **caractère par caractère**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Cross-Attention (mention)\n",
    "\n",
    "Jusqu'ici, on a fait de la **self-attention** : Q, K, V viennent de la **même source**.\n",
    "\n",
    "Il existe aussi la **cross-attention** :\n",
    "\n",
    "```\n",
    "Self-attention:     x ──► Q, K, V     (même source)\n",
    "\n",
    "Cross-attention:    x_decoder ──► Q       (une source)\n",
    "                    x_encoder ──► K, V    (autre source)\n",
    "```\n",
    "\n",
    "### Cas d'usage\n",
    "\n",
    "| Application | Rôle du cross-attention |\n",
    "|-------------|------------------------|\n",
    "| **Traduction** | Le décodeur (français) \"interroge\" l'encodeur (anglais) |\n",
    "| **RAG** | Le modèle \"interroge\" les documents récupérés |\n",
    "| **Image captioning** | Le texte \"interroge\" l'image |\n",
    "\n",
    "**On ne l'implémente pas ici**, mais c'est la même mécanique avec Q d'un côté et K, V de l'autre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Récapitulatif : Les briques du Transformer\n",
    "\n",
    "### Schéma complet d'un bloc Transformer Encoder\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                   TRANSFORMER BLOCK                     │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│   Input ──────────────────────────────┐                 │\n",
    "│     │                                 │                 │\n",
    "│     ↓                                 │                 │\n",
    "│   LayerNorm                           │                 │\n",
    "│     │                                 │                 │\n",
    "│     ↓                                 │                 │\n",
    "│   Multi-Head Attention                │                 │\n",
    "│   (+ masque causal si GPT)            │                 │\n",
    "│     │                                 │                 │\n",
    "│     ↓                                 │                 │\n",
    "│   Dropout                             │                 │\n",
    "│     │                                 │                 │\n",
    "│     + ←───────────────────────────────┘ (résiduel 1)    │\n",
    "│     │                                                   │\n",
    "│     ├──────────────────────────────┐                    │\n",
    "│     ↓                              │                    │\n",
    "│   LayerNorm                        │                    │\n",
    "│     │                              │                    │\n",
    "│     ↓                              │                    │\n",
    "│   Feed-Forward (expand 4×)         │                    │\n",
    "│     │                              │                    │\n",
    "│     ↓                              │                    │\n",
    "│   Dropout                          │                    │\n",
    "│     │                              │                    │\n",
    "│     + ←────────────────────────────┘ (résiduel 2)       │\n",
    "│     │                                                   │\n",
    "│     ↓                                                   │\n",
    "│   Output                                                │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Tableau récapitulatif\n",
    "\n",
    "| Composant | Rôle | Paramètres |\n",
    "|-----------|------|------------|\n",
    "| **Multi-Head Attention** | Capturer les relations entre tokens | W_q, W_k, W_v, W_o |\n",
    "| **Feed-Forward** | Transformation non-linéaire par position | W_1, b_1, W_2, b_2 |\n",
    "| **LayerNorm** | Stabiliser l'entraînement | γ, β |\n",
    "| **Résiduel** | Faciliter le flux des gradients | - |\n",
    "| **Masque causal** | Empêcher de voir le futur (GPT) | - |\n",
    "\n",
    "### Prochaine session\n",
    "\n",
    "On va :\n",
    "1. **Assembler** un bloc Transformer complet\n",
    "2. Commencer le **Mini-GPT** : générer des noms de Pokémon !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Pour aller plus loin (optionnel)\n",
    "\n",
    "### Comparaison avec nn.MultiheadAttention de PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch fournit une implémentation optimisée\n",
    "mha_pytorch = nn.MultiheadAttention(embed_dim=32, num_heads=4, batch_first=True)\n",
    "\n",
    "x = torch.randn(2, 6, 32)\n",
    "\n",
    "# Pour nn.MultiheadAttention, on passe query, key, value séparément\n",
    "# En self-attention, query = key = value = x\n",
    "output_pytorch, weights_pytorch = mha_pytorch(x, x, x)\n",
    "\n",
    "print(f\"Output shape (PyTorch): {output_pytorch.shape}\")\n",
    "print(f\"Weights shape (PyTorch): {weights_pytorch.shape}\")\n",
    "\n",
    "# Avec masque causal\n",
    "causal_mask_pytorch = nn.Transformer.generate_square_subsequent_mask(6)\n",
    "output_causal, _ = mha_pytorch(x, x, x, attn_mask=causal_mask_pytorch)\n",
    "print(f\"\\nAvec masque causal: {output_causal.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul du nombre de paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice : calculer les paramètres d'un bloc Transformer\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "ff_dim = 2048  # 4 × embed_dim\n",
    "\n",
    "# Multi-Head Attention : 4 matrices (W_q, W_k, W_v, W_o)\n",
    "params_mha = 4 * (embed_dim * embed_dim + embed_dim)  # weights + bias\n",
    "\n",
    "# Feed-Forward : 2 couches\n",
    "params_ff = (embed_dim * ff_dim + ff_dim) + (ff_dim * embed_dim + embed_dim)\n",
    "\n",
    "# LayerNorm : 2 × (gamma + beta)\n",
    "params_ln = 2 * (embed_dim + embed_dim)\n",
    "\n",
    "total = params_mha + params_ff + params_ln\n",
    "\n",
    "print(f\"Paramètres par bloc Transformer (embed_dim={embed_dim}):\")\n",
    "print(f\"  Multi-Head Attention: {params_mha:,}\")\n",
    "print(f\"  Feed-Forward:         {params_ff:,}\")\n",
    "print(f\"  LayerNorm (×2):       {params_ln:,}\")\n",
    "print(f\"  ─────────────────────────────\")\n",
    "print(f\"  Total:                {total:,}\")\n",
    "print(f\"\\nPour 12 blocs (BERT): {total * 12:,} ≈ {total * 12 / 1e6:.0f}M paramètres\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}